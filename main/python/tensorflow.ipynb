{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Solver import Particle, Perceptron, PerceptronModel, VicsekModel, NeuralNetwork, PerceptronMode, Mode, NeuralSwarmModel\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# Use tensorflow agents for reinforcement learning\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# :::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "# Simulation settings\n",
    "settings = {\n",
    "        #                  N,      L,      v,      noise,  r\n",
    "        \"XXsmall\": [       5,      4,      0.03,   0.1,    1],\n",
    "        \"Xsmall\": [        20,     6,      0.03,   0.1,    1],\n",
    "        \"small\": [         100,    30,     0.03,   0.1,    1],\n",
    "        \"a\": [             300,    7,      0.03,   2.0,    1],\n",
    "        \"b\": [             300,    25,     0.03,   0.5,    1],\n",
    "        \"d\": [             300,    5,      0.03,   0.1,    1],\n",
    "        \"plot1_N40\": [     40,     3.1,    0.03,   0.1,    1],\n",
    "        \"large\": [         2000,   60,     0.03,   0.3,    1]\n",
    "    }\n",
    "    \n",
    "# Choose between RADIUS, FIXED, FIXEDRADIUS\n",
    "mode = Mode.FIXEDRADIUS\n",
    "# Flags\n",
    "ZDimension = False     # 2D or 3D\n",
    "# Duration of simulation\n",
    "timesteps = 5000\n",
    "# Choose settings\n",
    "chosen_settings = settings[\"small\"]\n",
    "N       = chosen_settings[0]\n",
    "L       = chosen_settings[1]\n",
    "v       = chosen_settings[2]\n",
    "noise   = chosen_settings[3]\n",
    "r       = chosen_settings[4]\n",
    "k_neighbors = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Neural network model\n",
    "# # ::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# class ReinforcementModel(tf.keras.Model):\n",
    "#     def __init__(self):\n",
    "#         super(ReinforcementModel, self).__init__()\n",
    "#         self.input_layer = tf.keras.Input(shape=(10,))  # 5 pairs of (sin, cos) values\n",
    "#         # Input and output layer at the same time\n",
    "#         self.dense1 = tf.keras.layers.Dense(5, activation='linear')\n",
    "#         self.dense2 = tf.keras.layers.Dense(5, activation='linear')\n",
    "\n",
    "#     def call(self, inputsCos, inputsSin):\n",
    "#         x = self.dense1(inputsCos)\n",
    "#         y = self.dense2(inputsSin)\n",
    "#         return x, y\n",
    "\n",
    "#     def get_angle(self, out_sin, out_cos):\n",
    "#         # Calculate angle from sin and cos output of the model (in radians) and return it in the range [0, 2pi]\n",
    "#         angle = np.arctan2(out_sin, out_cos)\n",
    "#         angle = (angle + 2 * np.pi) % (2 * np.pi)\n",
    "#         return angle\n",
    "\n",
    "# # ::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "# NN = ReinforcementModel()\n",
    "\n",
    "## Create simulation model\n",
    "#model = NeuralSwarmModel(N, L, v, noise, r, mode, k_neighbors, ZDimension, seed=True)\n",
    "\n",
    "# # Contains all neighbor lists for all particles\n",
    "# AllNeighborsParticles = model.get_all_neighbors()\n",
    "\n",
    "# print(len(AllNeighborsParticles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom environment\n",
    "class SimulationEnvironment(py_environment.PyEnvironment):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # THOUGHTS: This should be correct. A particle can only choose an angle, which is a float (scalar)\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.float32, minimum=0, maximum=2*np.pi, name='action')\n",
    "        # THOUGHTS: This should be correct. The observation is a vector of length k_neighbors + 1, where each entry is an angle. There is no information about the position of the particles.\n",
    "        # k_neighbors + 1 because the particle itself is also included\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(k_neighbors + 1,), dtype=np.float32, minimum=0, maximum=2*np.pi, name='observation')\n",
    "        # THOUGHTS: What is _state? Is it the current state of the environment? This should be different from observation, because the observation is what the agent sees, while the state is the actual state of the environment.\n",
    "        # So the state should be the order parameter to be maximized.\n",
    "        self._state = 0\n",
    "        self._episode_ended = False\n",
    "        self.simulation = NeuralSwarmModel(N, L, v, noise, r, mode, k_neighbors, ZDimension, seed=True)\n",
    "        # Only for testing: We observe and control the first particle\n",
    "        self.index = 0\n",
    "\n",
    "    def observation_spec(self):\n",
    "        \"\"\"Return observation_spec.\"\"\"\n",
    "        # DONE\n",
    "        return self._observation_spec\n",
    "\n",
    "    def action_spec(self):\n",
    "        \"\"\"Return action_spec.\"\"\"\n",
    "        # DONE\n",
    "        return self._action_spec\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Return initial_time_step.\"\"\"\n",
    "        # DONE\n",
    "        self._current_time_step = self._reset()\n",
    "        return self._current_time_step\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Apply action and return new time_step.\"\"\"\n",
    "        # DONE\n",
    "        if self._current_time_step is None:\n",
    "            return self.reset()\n",
    "        self._current_time_step = self._step(action)\n",
    "        return self._current_time_step\n",
    "\n",
    "    def current_time_step(self):\n",
    "        # DONE\n",
    "        return self._current_time_step\n",
    "\n",
    "    def time_step_spec(self):\n",
    "        \"\"\"Return time_step_spec.\"\"\"\n",
    "        # DONE\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Return initial_time_step.\"\"\"\n",
    "        # Reset simulation\n",
    "        self.simulation = NeuralSwarmModel(N, L, v, noise, r, mode, k_neighbors, ZDimension, seed=True)\n",
    "        self._state = 0\n",
    "        self._episode_ended = False\n",
    "        return ts.restart(np.zeros(shape=(k_neighbors + 1,), dtype=np.float32))\n",
    "\n",
    "    def _step(self, action):\n",
    "        \"\"\"Apply action and return new time_step\"\"\"\n",
    "\n",
    "        if self._episode_ended:\n",
    "            # The last action ended the episode. Ignore the current action and start\n",
    "            # a new episode.\n",
    "            return self.reset()\n",
    "        \n",
    "        oldState = self._state\n",
    "\n",
    "        # Make sure episodes don't go on forever.\n",
    "        # Define a stopping action: action == -1\n",
    "        if action < 0.:\n",
    "            self._episode_ended = True\n",
    "        elif action >= 0.:\n",
    "            # Do one step in the simulation\n",
    "            self.simulation.update_angle(self.index, action)\n",
    "            self.simulation.update()\n",
    "            self._state = self.simulation.get_local_order_parameter(self.index)\n",
    "        else:\n",
    "            raise ValueError('What did you do?')\n",
    "\n",
    "        if self._episode_ended:\n",
    "            # The reward is the difference between the new state and the old state.\n",
    "            # An increase in the order parameter is rewarded, a decrease is punished.\n",
    "            reward = self._state - oldState\n",
    "            observation = self.simulation.get_angles(self.index)\n",
    "            # The observation (first argument of ts.termination) is the angles of the neighbors of the particle\n",
    "            return ts.termination(np.array(observation, dtype=np.float32), reward)\n",
    "        else:\n",
    "            return ts.transition(np.array(observation, dtype=np.float32), reward=0.0, discount=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.dense1 = layers.Dense(64, activation='relu')\n",
    "        self.dense2 = layers.Dense(64, activation='relu')\n",
    "        self.out = layers.Dense(action_dim, activation='tanh')\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.dense1(state)\n",
    "        x = self.dense2(x)\n",
    "        action = self.out(x)\n",
    "        return action\n",
    "\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.dense1 = layers.Dense(64, activation='relu')\n",
    "        self.dense2 = layers.Dense(64, activation='relu')\n",
    "        self.out = layers.Dense(1)\n",
    "\n",
    "    def call(self, state, action):\n",
    "        x = tf.concat([state, action], axis=-1)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        value = self.out(x)\n",
    "        return value\n",
    "\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.actor = Actor(action_dim)\n",
    "        self.critic = Critic()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "        action = self.actor(state)\n",
    "        return action[0]\n",
    "\n",
    "    # Andere Methoden für Training, Aktualisieren der Zielnetzwerke, usw.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
