{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 12:30:09.599524: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-01 12:30:09.625545: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-01 12:30:10.073290: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    }
   ],
   "source": [
    "from Solver import Particle, Perceptron, PerceptronModel, VicsekModel, NeuralNetwork, PerceptronMode, Mode, NeuralSwarmModel\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation settings\n",
    "settings = {\n",
    "        #                  N,      L,      v,      noise,  r\n",
    "        \"XXsmall\": [       5,      4,      0.03,   0.1,    1],\n",
    "        \"Xsmall\": [        20,     6,      0.03,   0.1,    1],\n",
    "        \"small\": [         100,    30,     0.03,   0.1,    1],\n",
    "        \"a\": [             300,    7,      0.03,   2.0,    1],\n",
    "        \"b\": [             300,    25,     0.03,   0.5,    1],\n",
    "        \"d\": [             300,    5,      0.03,   0.1,    1],\n",
    "        \"plot1_N40\": [     40,     3.1,    0.03,   0.1,    1],\n",
    "        \"large\": [         2000,   60,     0.03,   0.3,    1]\n",
    "    }\n",
    "    \n",
    "# Choose between RADIUS, FIXED, FIXEDRADIUS\n",
    "mode = Mode.FIXEDRADIUS\n",
    "# Flags\n",
    "ZDimension = False     # 2D or 3D\n",
    "# Duration of simulation\n",
    "timesteps = 5000\n",
    "# Choose settings\n",
    "chosen_settings = settings[\"small\"]\n",
    "N       = chosen_settings[0]\n",
    "L       = chosen_settings[1]\n",
    "v       = chosen_settings[2]\n",
    "noise   = chosen_settings[3]\n",
    "r       = chosen_settings[4]\n",
    "k_neighbors = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use \"num_iterations = 1e6\" for better results (2 hrs)\n",
    "# 1e5 is just so this doesn't take too long (1 hr)\n",
    "num_iterations = 100 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 10 # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 100000 # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 2 # @param {type:\"integer\"}\n",
    "\n",
    "critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "target_update_tau = 0.005 # @param {type:\"number\"}\n",
    "target_update_period = 1 # @param {type:\"number\"}\n",
    "gamma = 0.99 # @param {type:\"number\"}\n",
    "reward_scale_factor = 10.0 # @param {type:\"number\"}\n",
    "\n",
    "actor_fc_layer_params = (8, 8)\n",
    "critic_joint_fc_layer_params = (256, 256)\n",
    "\n",
    "log_interval = 10 # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10 # @param {type:\"integer\"}\n",
    "eval_interval = 10 # @param {type:\"integer\"}\n",
    "\n",
    "policy_save_interval = 100 # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import py_environment\n",
    "\n",
    "\n",
    "# Create a custom environment\n",
    "class SimulationEnvironment(py_environment.PyEnvironment):\n",
    "    \"\"\"Interface for a swarm simulation environment.\n",
    "    \n",
    "    Can be converted into a TensorFlow environment.\n",
    "    \n",
    "    Provides uniform access to the simulation and hosts the reward function.\n",
    "    \"\"\"\n",
    "    # [ ] Create a hyperparameter by changing an episode to a certain number of timesteps\n",
    "    # [ ] Check if this change conflicts with the experience trajectories\n",
    "    minimum = 0.0\n",
    "    maximum = 2*np.pi\n",
    "    logging_flag = True\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # The action is the angle of the particle\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.float32, minimum=self.minimum, maximum=self.maximum, name='action')\n",
    "        \n",
    "        \n",
    "        # k_neighbors + 1 because the particle itself is also included\n",
    "        # [ ] Change the observation to a relative angle (this reduces the dimensionality of the observation space by 1)\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(k_neighbors + 1,), dtype=np.float32, minimum=self.minimum, maximum=self.maximum, name='observation')\n",
    "        \n",
    "        # Flags and variables\n",
    "        self._episode_ended = False\n",
    "        self.simulation = NeuralSwarmModel(N, L, v, noise, r, mode, k_neighbors, ZDimension, seed=False)\n",
    "        self._state = self.simulation.mean_direction2D()\n",
    "        # One \"episode\" and its corresponding reward consists of the iteration over all N particles\n",
    "        self.index = 0\n",
    "        # To change all angles at once, we need to store the new angles in a list\n",
    "        self.new_angles = np.zeros(shape=(N,), dtype=np.float32)\n",
    "        observation = self.simulation.get_angles(self.index)\n",
    "        self._current_time_step = ts.restart(np.array(observation, dtype=np.float32))\n",
    "        \n",
    "\n",
    "    def observation_spec(self):\n",
    "        \"\"\"Return observation_spec.\"\"\"\n",
    "        # DONE\n",
    "        return self._observation_spec\n",
    "\n",
    "    def action_spec(self):\n",
    "        \"\"\"Return action_spec.\"\"\"\n",
    "        # DONE\n",
    "        return self._action_spec\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Return initial_time_step and reset the simulation.\n",
    "        \n",
    "        Note that this is a hard reset and not a reset for the current epoch.\"\"\"\n",
    "        \n",
    "        # DONE\n",
    "        self._current_time_step = self._reset()\n",
    "        return self._current_time_step\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Apply action and return new time_step.\"\"\"\n",
    "        # DONE\n",
    "        if self._current_time_step is None:\n",
    "            return self.reset()\n",
    "        self._current_time_step = self._step(action)\n",
    "        return self._current_time_step\n",
    "\n",
    "    def current_time_step(self):\n",
    "        # DONE\n",
    "        return self._current_time_step\n",
    "\n",
    "    # def time_step_spec(self):\n",
    "        \"\"\"Return time_step_spec.\"\"\"\n",
    "        # DONE\n",
    "        # return ts.time_step_spec(self.observation_spec())\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Return initial_time_step and reset the simulation.\n",
    "        \n",
    "        Note that this is a hard reset and not a reset for the current epoch.\"\"\"\n",
    "        # THOUGHTS: In this case, a differentiation has to be made between an episode and an epoch.\n",
    "        # The episode ends when all particles have been updated. An epoch ends when the simulation is reset.\n",
    "        \n",
    "        # Reset simulation\n",
    "        self.simulation = NeuralSwarmModel(N, L, v, noise, r, mode, k_neighbors, ZDimension, seed=False)\n",
    "        self._state = self.simulation.mean_direction2D()\n",
    "        self._episode_ended = False\n",
    "        self.index = 0\n",
    "        observation = self.simulation.get_angles(self.index)\n",
    "        if self.logging_flag: print('Environment _reset.')\n",
    "        return ts.restart(np.array(observation, dtype=np.float32))\n",
    "\n",
    "    def _step(self, action):\n",
    "        \"\"\"Apply action and return new time_step.\n",
    "        This method hosts the reward function.\"\"\"\n",
    "        \n",
    "        \n",
    "        observation = self.simulation.get_angles(self.index)\n",
    "        observation = np.array(observation, dtype=np.float32)\n",
    "        \n",
    "        if action < self.minimum: action = self.minimum\n",
    "\n",
    "        if action >= self.minimum and self._episode_ended is False:\n",
    "            # Update angle of the particle, but don't update the simulation yet\n",
    "            self.new_angles[self.index] = action\n",
    "            if self.logging_flag: print(f'Action applied with action = {action}.')\n",
    "        elif self._episode_ended is False:\n",
    "            raise ValueError(f'What did you do? This should be a finite float value. Action = {action}.')\n",
    "        # Check whether nan is in the observation\n",
    "        elif np.isnan(observation).any():\n",
    "            raise ValueError(f'The observation contains nan. Observation = {observation}.')\n",
    "        # Check whether observation is in the correct range (observation is a list)\n",
    "        elif np.any(observation < self.minimum) or np.any(observation > self.maximum):\n",
    "            raise ValueError(f'The observation is not in the correct range. Observation = {observation}.')\n",
    "        \n",
    "        # Properly handle the case when the episode ends:\n",
    "        # [x] The episode ends when all particles have been updated\n",
    "        # [x] The driver needs to work with the updated simulation. So the simulation needs to go on instead of being reset (after self._episode_ended = True)\n",
    "\n",
    "        if self._episode_ended:\n",
    "            # Update all angles at once\n",
    "            self.simulation.update_angles(self.new_angles)\n",
    "            \n",
    "            oldState = self._state\n",
    "            \n",
    "            self.simulation.update()\n",
    "            self._state = self.simulation.mean_direction2D()\n",
    "            \n",
    "            # The reward is the difference between the new state and the old state.\n",
    "            # An increase in the order parameter is rewarded, a decrease is punished.\n",
    "            reward = self._state - oldState\n",
    "            # The observation (first argument of ts.termination) is the angles of the neighbors of the particle\n",
    "            self.index = 0\n",
    "            if self.logging_flag: print(f'Episode ended with reward = {reward}, state = {self._state}, observation = {observation}, action = {self.new_angles}.')\n",
    "            return ts.termination(observation, reward)\n",
    "        else:\n",
    "            self.index += 1\n",
    "            if self.index >= N:\n",
    "                self._episode_ended = True\n",
    "            return ts.transition(observation, reward=0.0, discount=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies\n",
    "\n",
    "With this option, the training can be scaled to multiple GPUs or TPUs, or multiple machines.\n",
    "\n",
    "For this to work, two steps are necessary:\n",
    "\n",
    "- The strategy has to be initialised.\n",
    "- The model must be defined in a strategy scope.\n",
    "\n",
    "In this project this option is not used, but it is included for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from tf_agents.train.utils import strategy_utils\n",
    "\n",
    "# Distribution strategy\n",
    "# For now, don't use GPU or TPU\n",
    "\n",
    "strategy = strategy_utils.get_strategy(tpu=False, use_gpu=False)\n",
    "\n",
    "# All variables and Agents need to be created under strategy.scope()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment wrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.networks import network\n",
    "from tf_agents.train.utils import spec_utils\n",
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "# There are two networks and two environments: one for training and one for evaluation.\n",
    "collect_env = SimulationEnvironment()\n",
    "eval_env = SimulationEnvironment()\n",
    "\n",
    "# Wrap the environment in a TF environment.\n",
    "tf_collect_env = tf_py_environment.TFPyEnvironment(collect_env)\n",
    "tf_eval_env = tf_py_environment.TFPyEnvironment(eval_env)\n",
    "\n",
    "# For the network to work with the environment, the specs have to be known.\n",
    "observation_spec, action_spec, time_step_spec = (spec_utils.get_tensor_specs(tf_collect_env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the python environments\n",
      "Testing the collect environment\n",
      "Environment _reset.\n",
      "Action applied with action = 5.994934558868408.\n",
      "Action applied with action = 2.6533150672912598.\n",
      "Action applied with action = 0.8836466073989868.\n",
      "Action applied with action = 1.7107394933700562.\n",
      "Action applied with action = 0.022165048867464066.\n",
      "Action applied with action = 1.0893616676330566.\n",
      "Action applied with action = 5.075008392333984.\n",
      "Action applied with action = 2.7597925662994385.\n",
      "Action applied with action = 3.504944324493408.\n",
      "Action applied with action = 5.4051833152771.\n",
      "Action applied with action = 0.7129935622215271.\n",
      "Action applied with action = 1.5518869161605835.\n",
      "Action applied with action = 2.339186906814575.\n",
      "Action applied with action = 4.996392726898193.\n",
      "Action applied with action = 2.927671432495117.\n",
      "Action applied with action = 0.5019637942314148.\n",
      "Action applied with action = 4.619950294494629.\n",
      "Action applied with action = 1.0580670833587646.\n",
      "Action applied with action = 1.2777822017669678.\n",
      "Action applied with action = 0.9060662388801575.\n",
      "Action applied with action = 4.045670032501221.\n",
      "Action applied with action = 2.944049596786499.\n",
      "Action applied with action = 1.4601645469665527.\n",
      "Action applied with action = 0.8095372915267944.\n",
      "Action applied with action = 3.1790552139282227.\n",
      "Action applied with action = 6.155867576599121.\n",
      "Action applied with action = 0.40360817313194275.\n",
      "Action applied with action = 1.6030287742614746.\n",
      "Action applied with action = 1.4198006391525269.\n",
      "Action applied with action = 3.005976676940918.\n",
      "Action applied with action = 4.102303981781006.\n",
      "Action applied with action = 1.2028858661651611.\n",
      "Action applied with action = 5.977814674377441.\n",
      "Action applied with action = 2.507888078689575.\n",
      "Action applied with action = 4.751445770263672.\n",
      "Action applied with action = 3.695321798324585.\n",
      "Action applied with action = 2.2359619140625.\n",
      "Action applied with action = 3.212467670440674.\n",
      "Action applied with action = 2.4660451412200928.\n",
      "Action applied with action = 5.771877288818359.\n",
      "Action applied with action = 0.2563757598400116.\n",
      "Action applied with action = 0.07914508879184723.\n",
      "Action applied with action = 1.0515397787094116.\n",
      "Action applied with action = 6.058427810668945.\n",
      "Action applied with action = 3.72237229347229.\n",
      "Action applied with action = 4.059017658233643.\n",
      "Action applied with action = 5.447654724121094.\n",
      "Action applied with action = 2.111222743988037.\n",
      "Action applied with action = 0.5441259145736694.\n",
      "Action applied with action = 4.079965114593506.\n",
      "Action applied with action = 5.765966892242432.\n",
      "Action applied with action = 5.816959381103516.\n",
      "Action applied with action = 1.39507257938385.\n",
      "Action applied with action = 3.2582335472106934.\n",
      "Action applied with action = 2.477086067199707.\n",
      "Action applied with action = 1.3324856758117676.\n",
      "Action applied with action = 5.1342573165893555.\n",
      "Action applied with action = 3.525477886199951.\n",
      "Action applied with action = 2.240497589111328.\n",
      "Action applied with action = 6.208390235900879.\n",
      "Action applied with action = 2.77374529838562.\n",
      "Action applied with action = 1.349682331085205.\n",
      "Action applied with action = 4.484363555908203.\n",
      "Action applied with action = 0.5238469839096069.\n",
      "Action applied with action = 2.9606330394744873.\n",
      "Action applied with action = 4.946735382080078.\n",
      "Action applied with action = 3.1140055656433105.\n",
      "Action applied with action = 0.34221428632736206.\n",
      "Action applied with action = 0.8643064498901367.\n",
      "Action applied with action = 1.6914708614349365.\n",
      "Action applied with action = 2.3425071239471436.\n",
      "Action applied with action = 6.253377914428711.\n",
      "Action applied with action = 0.08161938935518265.\n",
      "Action applied with action = 1.1802750825881958.\n",
      "Action applied with action = 0.9780796766281128.\n",
      "Action applied with action = 6.210364818572998.\n",
      "Action applied with action = 4.212048053741455.\n",
      "Action applied with action = 0.48858678340911865.\n",
      "Action applied with action = 4.749549865722656.\n",
      "Action applied with action = 6.010534286499023.\n",
      "Action applied with action = 4.117041110992432.\n",
      "Action applied with action = 5.906637668609619.\n",
      "Action applied with action = 5.497284889221191.\n",
      "Action applied with action = 1.7839339971542358.\n",
      "Action applied with action = 5.079196453094482.\n",
      "Action applied with action = 2.206303358078003.\n",
      "Action applied with action = 0.041045114398002625.\n",
      "Action applied with action = 5.582646369934082.\n",
      "Action applied with action = 3.7160680294036865.\n",
      "Action applied with action = 3.6197633743286133.\n",
      "Action applied with action = 5.35536527633667.\n",
      "Action applied with action = 6.229983329772949.\n",
      "Action applied with action = 1.1538957357406616.\n",
      "Action applied with action = 4.713738441467285.\n",
      "Action applied with action = 1.2148171663284302.\n",
      "Action applied with action = 1.168607234954834.\n",
      "Action applied with action = 4.5131516456604.\n",
      "Action applied with action = 1.53676438331604.\n",
      "Action applied with action = 0.04817945882678032.\n",
      "Action applied with action = 5.562385559082031.\n",
      "Episode ended with reward = 0.07555317410728572, state = 0.14692808225842166, observation = [0. 0. 0. 0. 0. 0.], action = [5.9949346  2.653315   0.8836466  1.7107395  0.02216505 1.0893617\n",
      " 5.0750084  2.7597926  3.5049443  5.4051833  0.71299356 1.5518869\n",
      " 2.339187   4.9963927  2.9276714  0.5019638  4.6199503  1.0580671\n",
      " 1.2777822  0.90606624 4.04567    2.9440496  1.4601645  0.8095373\n",
      " 3.1790552  6.1558676  0.40360817 1.6030288  1.4198006  3.0059767\n",
      " 4.102304   1.2028859  5.9778147  2.507888   4.751446   3.6953218\n",
      " 2.235962   3.2124677  2.4660451  5.7718773  0.25637576 0.07914509\n",
      " 1.0515398  6.058428   3.7223723  4.0590177  5.4476547  2.1112227\n",
      " 0.5441259  4.079965   5.765967   5.8169594  1.3950726  3.2582335\n",
      " 2.477086   1.3324857  5.1342573  3.525478   2.2404976  6.20839\n",
      " 2.7737453  1.3496823  4.4843636  0.523847   2.960633   4.9467354\n",
      " 3.1140056  0.3422143  0.86430645 1.6914709  2.3425071  6.253378\n",
      " 0.08161939 1.1802751  0.9780797  6.210365   4.212048   0.48858678\n",
      " 4.74955    6.0105343  4.117041   5.9066377  5.497285   1.783934\n",
      " 5.0791965  2.2063034  0.04104511 5.5826464  3.716068   3.6197634\n",
      " 5.3553653  6.2299833  1.1538957  4.7137384  1.2148172  1.1686072\n",
      " 4.5131516  1.5367644  0.04817946 5.5623856 ].\n",
      "Testing the eval environment\n",
      "Environment _reset.\n",
      "Action applied with action = 5.67449951171875.\n",
      "Action applied with action = 5.802371978759766.\n",
      "Action applied with action = 4.472878456115723.\n",
      "Action applied with action = 3.0971031188964844.\n",
      "Action applied with action = 3.10532546043396.\n",
      "Action applied with action = 1.1537156105041504.\n",
      "Action applied with action = 1.3300725221633911.\n",
      "Action applied with action = 3.3926045894622803.\n",
      "Action applied with action = 3.9607956409454346.\n",
      "Action applied with action = 0.3448154032230377.\n",
      "Action applied with action = 1.9110978841781616.\n",
      "Action applied with action = 3.742155075073242.\n",
      "Action applied with action = 4.9184064865112305.\n",
      "Action applied with action = 4.25288200378418.\n",
      "Action applied with action = 1.6408978700637817.\n",
      "Action applied with action = 1.2964470386505127.\n",
      "Action applied with action = 0.5967528223991394.\n",
      "Action applied with action = 4.522526264190674.\n",
      "Action applied with action = 1.749482274055481.\n",
      "Action applied with action = 1.4065897464752197.\n",
      "Action applied with action = 2.180150032043457.\n",
      "Action applied with action = 3.180706024169922.\n",
      "Action applied with action = 2.5845391750335693.\n",
      "Action applied with action = 5.111279487609863.\n",
      "Action applied with action = 3.8050875663757324.\n",
      "Action applied with action = 1.9321308135986328.\n",
      "Action applied with action = 2.8770668506622314.\n",
      "Action applied with action = 5.547569274902344.\n",
      "Action applied with action = 2.2703964710235596.\n",
      "Action applied with action = 3.656221866607666.\n",
      "Action applied with action = 1.6102104187011719.\n",
      "Action applied with action = 4.552435874938965.\n",
      "Action applied with action = 0.3318735361099243.\n",
      "Action applied with action = 5.105541706085205.\n",
      "Action applied with action = 5.761671543121338.\n",
      "Action applied with action = 5.905841827392578.\n",
      "Action applied with action = 2.0671746730804443.\n",
      "Action applied with action = 4.080644130706787.\n",
      "Action applied with action = 3.380544900894165.\n",
      "Action applied with action = 4.07704496383667.\n",
      "Action applied with action = 2.476367712020874.\n",
      "Action applied with action = 6.178980350494385.\n",
      "Action applied with action = 4.789997100830078.\n",
      "Action applied with action = 4.768835544586182.\n",
      "Action applied with action = 3.607156276702881.\n",
      "Action applied with action = 1.9529062509536743.\n",
      "Action applied with action = 4.418564319610596.\n",
      "Action applied with action = 3.220615863800049.\n",
      "Action applied with action = 2.8212788105010986.\n",
      "Action applied with action = 2.5549185276031494.\n",
      "Action applied with action = 2.0714590549468994.\n",
      "Action applied with action = 3.3148274421691895.\n",
      "Action applied with action = 2.985469341278076.\n",
      "Action applied with action = 5.249032020568848.\n",
      "Action applied with action = 3.5774881839752197.\n",
      "Action applied with action = 2.6500637531280518.\n",
      "Action applied with action = 4.167153358459473.\n",
      "Action applied with action = 2.4754884243011475.\n",
      "Action applied with action = 0.5427348017692566.\n",
      "Action applied with action = 5.148072242736816.\n",
      "Action applied with action = 4.817659854888916.\n",
      "Action applied with action = 4.582912921905518.\n",
      "Action applied with action = 1.257770299911499.\n",
      "Action applied with action = 5.920029640197754.\n",
      "Action applied with action = 3.9508020877838135.\n",
      "Action applied with action = 0.7438424825668335.\n",
      "Action applied with action = 4.759007453918457.\n",
      "Action applied with action = 1.3524967432022095.\n",
      "Action applied with action = 0.6582854986190796.\n",
      "Action applied with action = 5.067188262939453.\n",
      "Action applied with action = 0.4853665232658386.\n",
      "Action applied with action = 0.1233702227473259.\n",
      "Action applied with action = 2.936650514602661.\n",
      "Action applied with action = 0.8596600294113159.\n",
      "Action applied with action = 4.109921455383301.\n",
      "Action applied with action = 2.3870956897735596.\n",
      "Action applied with action = 2.532120704650879.\n",
      "Action applied with action = 0.8095437288284302.\n",
      "Action applied with action = 0.15042664110660553.\n",
      "Action applied with action = 1.0995296239852905.\n",
      "Action applied with action = 3.714874505996704.\n",
      "Action applied with action = 1.0297958850860596.\n",
      "Action applied with action = 1.3440929651260376.\n",
      "Action applied with action = 3.5092217922210693.\n",
      "Action applied with action = 2.2904181480407715.\n",
      "Action applied with action = 5.059183120727539.\n",
      "Action applied with action = 0.3883035480976105.\n",
      "Action applied with action = 4.534508228302002.\n",
      "Action applied with action = 2.765899896621704.\n",
      "Action applied with action = 1.5829854011535645.\n",
      "Action applied with action = 2.668163537979126.\n",
      "Action applied with action = 5.336402416229248.\n",
      "Action applied with action = 1.0343576669692993.\n",
      "Action applied with action = 5.782184600830078.\n",
      "Action applied with action = 5.902431964874268.\n",
      "Action applied with action = 2.092886447906494.\n",
      "Action applied with action = 1.179948091506958.\n",
      "Action applied with action = 0.6834290027618408.\n",
      "Action applied with action = 6.254748821258545.\n",
      "Action applied with action = 6.1284050941467285.\n",
      "Episode ended with reward = -0.09672706672812638, state = 0.06920460473134642, observation = [0. 0. 0. 0. 0. 0.], action = [5.6744995  5.802372   4.4728785  3.097103   3.1053255  1.1537156\n",
      " 1.3300725  3.3926046  3.9607956  0.3448154  1.9110979  3.742155\n",
      " 4.9184065  4.252882   1.6408979  1.296447   0.5967528  4.5225263\n",
      " 1.7494823  1.4065897  2.18015    3.180706   2.5845392  5.1112795\n",
      " 3.8050876  1.9321308  2.8770669  5.5475693  2.2703965  3.6562219\n",
      " 1.6102104  4.552436   0.33187354 5.1055417  5.7616715  5.905842\n",
      " 2.0671747  4.080644   3.380545   4.077045   2.4763677  6.1789804\n",
      " 4.789997   4.7688355  3.6071563  1.9529063  4.4185643  3.2206159\n",
      " 2.8212788  2.5549185  2.071459   3.3148274  2.9854693  5.249032\n",
      " 3.5774882  2.6500638  4.1671534  2.4754884  0.5427348  5.1480722\n",
      " 4.81766    4.582913   1.2577703  5.9200296  3.950802   0.7438425\n",
      " 4.7590075  1.3524967  0.6582855  5.0671883  0.48536652 0.12337022\n",
      " 2.9366505  0.85966    4.1099215  2.3870957  2.5321207  0.8095437\n",
      " 0.15042664 1.0995296  3.7148745  1.0297959  1.344093   3.5092218\n",
      " 2.2904181  5.059183   0.38830355 4.534508   2.7659     1.5829854\n",
      " 2.6681635  5.3364024  1.0343577  5.7821846  5.902432   2.0928864\n",
      " 1.1799481  0.683429   6.254749   6.128405  ].\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.environments import utils\n",
    "\n",
    "# Test the python environments\n",
    "\n",
    "print(\"Testing the python environments\")\n",
    "print(\"Testing the collect environment\")\n",
    "utils.validate_py_environment(collect_env, episodes=1)\n",
    "print(\"Testing the eval environment\")\n",
    "utils.validate_py_environment(eval_env, episodes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YES! IT WORKS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.agents.ddpg import critic_network\n",
    "\n",
    "# Define a network that can learn to predict the action given an observation.\n",
    "# This is a simple (on demand fully connected) network that takes in an observation and outputs an action.\n",
    "\n",
    "class ActorNet(network.Network):\n",
    "\n",
    "  def __init__(self, input_tensor_spec, output_tensor_spec, name='ActorNet'):\n",
    "    super(ActorNet, self).__init__(\n",
    "        input_tensor_spec=input_tensor_spec,\n",
    "        state_spec=())\n",
    "    self._output_tensor_spec = output_tensor_spec\n",
    "    # THOUGHTS: For now, only one layer is used. This can be changed later.\n",
    "    self._sub_layers = [\n",
    "        tf.keras.layers.Dense(\n",
    "            action_spec.shape.num_elements(), activation=\"linear\"),\n",
    "    ]\n",
    "\n",
    "  def call(self, observations, step_type=(), network_state=()):\n",
    "    del step_type  # unused.\n",
    "    \n",
    "    output = tf.cast(observations, dtype=tf.float32)\n",
    "    for layer in self._sub_layers:\n",
    "      output = layer(output)\n",
    "    actions = tf.reshape(output, [-1] + self._output_tensor_spec.shape.as_list())\n",
    "\n",
    "    # Scale and shift actions to the correct range if necessary.\n",
    "    return actions, network_state\n",
    "\n",
    "\n",
    "# Create the Actor Network\n",
    "actor_net = ActorNet(\n",
    "    input_tensor_spec=observation_spec,\n",
    "    output_tensor_spec=action_spec)\n",
    "\n",
    "\n",
    "# Critic Network\n",
    "critic_net = critic_network.CriticNetwork(\n",
    "      (observation_spec, action_spec),\n",
    "      observation_fc_layer_params=None,\n",
    "      action_fc_layer_params=None,\n",
    "      joint_fc_layer_params=None,\n",
    "      activation_fn=None,\n",
    "      kernel_initializer='glorot_uniform',\n",
    "      last_kernel_initializer='glorot_uniform')\n",
    "\n",
    "# The critic network is a Q network that takes in an observation and an action and outputs a Q value.\n",
    "# The observation and the action are frist passed through their own fully connected layers, before being concatenated and passed through a final fully connected layer.\n",
    "# action_fc_layer_params, joint_fc_layer_params, and activation_fn are the parameters for the fully connected layers and determine the depth and width of the networks.\n",
    "# The kernel_initializer and last_kernel_initializer are the initializers for the fully connected layers and the final layer respectively.\n",
    "# They initialize the weights of the network. The default is glorot_uniform, which is a good default for most networks.\n",
    "# The glorot_uniform initializer draws the weights from a uniform distribution with a range that depends on the number of input and output units.\n",
    "\n",
    "# DONT USE AN ACTOR DISTRIBUTION NETWORK FOR DDPG AGENTS\n",
    "# They need a deterministic action output. The actor distribution network is for stochastic policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 12:30:10.955284: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 12:30:10.978958: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 12:30:10.979011: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 12:30:10.981806: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 12:30:10.981865: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 12:30:10.981897: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 12:30:11.383689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 12:30:11.383736: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 12:30:11.383740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-08-01 12:30:11.383755: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 12:30:11.383776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9330 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.train.utils import train_utils\n",
    "\n",
    "# Initialize the agent\n",
    "\n",
    "train_step = train_utils.create_train_step()\n",
    "\n",
    "tf_agent = ddpg_agent.DdpgAgent(\n",
    "    time_step_spec,\n",
    "    action_spec,\n",
    "    actor_network=actor_net,\n",
    "    critic_network=critic_net,\n",
    "    actor_optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=actor_learning_rate),\n",
    "    critic_optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=critic_learning_rate),\n",
    "    target_update_tau=target_update_tau,\n",
    "    target_update_period=target_update_period,\n",
    "    td_errors_loss_fn=tf.math.squared_difference,\n",
    "    gamma=gamma,\n",
    "    reward_scale_factor=reward_scale_factor,\n",
    "    train_step_counter=train_step)\n",
    "\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/platform/tfrecord_checkpointer.cc:162]  Initializing TFRecordCheckpointer in /tmp/tmp_2m329dv.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:567] Loading latest checkpoint from /tmp/tmp_2m329dv\n",
      "[reverb/cc/platform/default/server.cc:71] Started replay server on port 34499\n"
     ]
    }
   ],
   "source": [
    "import reverb\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "\n",
    "# Use Reverb, a framework for experience replay developed by DeepMind, to store and sample experience tuples for training.\n",
    "# Using a samples_per_insert somewhere between 2 and 1000. This is a trade-off between the number of samples that can be drawn from the replay buffer and the number of times the replay buffer needs to be updated.\n",
    "rate_limiter=reverb.rate_limiters.SampleToInsertRatio(samples_per_insert=3.0, min_size_to_sample=3, error_buffer=3.0)\n",
    "\n",
    "table_name = 'uniform_table'\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=replay_buffer_capacity,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1))\n",
    "\n",
    "reverb_server = reverb.Server([table])\n",
    "\n",
    "# Since the agent needs N steps of experience to make an update, the dataset will need to sample batches of N steps + 1 to allow the agent to learn from a complete transition.\n",
    "\n",
    "reverb_replay = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    tf_agent.collect_data_spec,\n",
    "    sequence_length= 2,\n",
    "    table_name=table_name,\n",
    "    local_server=reverb_server)\n",
    "\n",
    "# A dataset is created from the replay buffer to be fed to the agent for training. \n",
    "dataset = reverb_replay.as_dataset(\n",
    "      sample_batch_size=batch_size, num_steps=2).prefetch(50)\n",
    "experience_dataset_fn = lambda: dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "\n",
    "# Policies\n",
    "# Create policies from the agent\n",
    "\n",
    "tf_eval_policy = tf_agent.policy\n",
    "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "  tf_eval_policy, use_tf_function=True)\n",
    "\n",
    "tf_collect_policy = tf_agent.collect_policy\n",
    "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "  tf_collect_policy, use_tf_function=True)\n",
    "\n",
    "# Random policy to sample from the environment\n",
    "random_policy = random_py_policy.RandomPyPolicy(\n",
    "  collect_env.time_step_spec(), collect_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.train import actor\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.train import learner\n",
    "\n",
    "import tempfile\n",
    "\n",
    "tempdir = tempfile.gettempdir()\n",
    "\n",
    "# As the Actors run data collection steps, they pass trajectories of (state, action, reward) to the observer, which caches and writes them to the Reverb replay system.\n",
    "\n",
    "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
    "  reverb_replay.py_client,\n",
    "  table_name,\n",
    "  sequence_length= 2,\n",
    "  stride_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment _reset.\n",
      "Action applied with action = 5.578791618347168.\n",
      "Action applied with action = 1.2024202346801758.\n",
      "Action applied with action = 5.136950492858887.\n",
      "Action applied with action = 5.2129316329956055.\n",
      "Action applied with action = 2.7218916416168213.\n",
      "Action applied with action = 3.079664468765259.\n",
      "Action applied with action = 4.729248523712158.\n",
      "Action applied with action = 4.072814464569092.\n",
      "Action applied with action = 0.51148921251297.\n",
      "Action applied with action = 3.3180508613586426.\n"
     ]
    }
   ],
   "source": [
    "# We create an Actor with the random policy and collect experiences to seed the replay buffer with.\n",
    "\n",
    "initial_collect_actor = actor.Actor(\n",
    "  collect_env,\n",
    "  random_policy,\n",
    "  train_step,\n",
    "  steps_per_run=initial_collect_steps,\n",
    "  observers=[rb_observer])\n",
    "\n",
    "initial_collect_actor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment _reset.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate an Actor with the collect policy to gather more experiences during training.\n",
    "\n",
    "env_step_metric = py_metrics.EnvironmentSteps()\n",
    "\n",
    "collect_actor = actor.Actor(\n",
    "  collect_env,\n",
    "  collect_policy,\n",
    "  train_step,\n",
    "  steps_per_run= 1,\n",
    "  metrics=actor.collect_metrics(10),\n",
    "  summary_dir=os.path.join(tempdir, learner.TRAIN_DIR),\n",
    "  observers=[rb_observer, env_step_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment _reset.\n"
     ]
    }
   ],
   "source": [
    "# Create an Actor which will be used to evaluate the policy during training.\n",
    "# actor.eval_metrics(num_eval_episodes) to log metrics later.\n",
    "\n",
    "eval_actor = actor.Actor(\n",
    "  eval_env,\n",
    "  eval_policy,\n",
    "  train_step,\n",
    "  episodes_per_run=num_eval_episodes,\n",
    "  metrics=actor.eval_metrics(num_eval_episodes),\n",
    "  summary_dir=os.path.join(tempdir, 'eval'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.ou_noise_policy.OUNoisePolicy object at 0x7f9d6fc8ed10>\". Calling saved_model.distribution() will raise the following assertion error: Distributions are not implemented yet.\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.train import triggers\n",
    "\n",
    "# Learners\n",
    "# The Learner component contains the agent and performs gradient step updates to the policy variables using experience data from the replay buffer.\n",
    "# After one or more training steps, the Learner can push a new set of variable values to the variable container.\n",
    "\n",
    "saved_model_dir = os.path.join(tempdir, learner.POLICY_SAVED_MODEL_DIR)\n",
    "\n",
    "# Triggers to save the agent's policy checkpoints.\n",
    "learning_triggers = [\n",
    "    triggers.PolicySavedModelTrigger(\n",
    "        saved_model_dir,\n",
    "        tf_agent,\n",
    "        train_step,\n",
    "        interval=policy_save_interval),\n",
    "    triggers.StepPerSecondLogTrigger(train_step, interval=1000),\n",
    "]\n",
    "\n",
    "# A strategy can be used here. The predefined strategy would be passed to the learner.\n",
    "agent_learner = learner.Learner(\n",
    "  tempdir,\n",
    "  train_step,\n",
    "  tf_agent,\n",
    "  experience_dataset_fn,\n",
    "  triggers=learning_triggers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action applied with action = 0.5201982855796814.\n",
      "Action applied with action = 0.5201982855796814.\n",
      "Action applied with action = 0.45424455404281616.\n",
      "Action applied with action = 0.297562837600708.\n",
      "Action applied with action = 0.009948015213012695.\n",
      "Action applied with action = 0.26990699768066406.\n",
      "Action applied with action = 0.5022057294845581.\n",
      "Action applied with action = 0.43795421719551086.\n",
      "Action applied with action = 0.3305937945842743.\n",
      "Action applied with action = 0.053896788507699966.\n",
      "Action applied with action = 0.04739867150783539.\n",
      "Action applied with action = 0.4482254087924957.\n",
      "Action applied with action = 0.3933533728122711.\n",
      "Action applied with action = 0.3142130970954895.\n",
      "Action applied with action = 0.10513055324554443.\n",
      "Action applied with action = 0.30889779329299927.\n",
      "Action applied with action = 0.07018222659826279.\n",
      "Action applied with action = 0.1966119408607483.\n",
      "Action applied with action = 0.3367120921611786.\n",
      "Action applied with action = 0.37686651945114136.\n",
      "Action applied with action = 0.2425023913383484.\n",
      "Action applied with action = 0.43501996994018555.\n",
      "Action applied with action = 0.1945350021123886.\n",
      "Action applied with action = 0.35543161630630493.\n",
      "Action applied with action = 0.2628052830696106.\n",
      "Action applied with action = 0.2852860987186432.\n",
      "Action applied with action = 0.09948042035102844.\n",
      "Action applied with action = 0.42401087284088135.\n",
      "Action applied with action = 0.17448663711547852.\n",
      "Action applied with action = 0.3701779842376709.\n",
      "Action applied with action = 0.05051342397928238.\n",
      "Action applied with action = 0.1388961672782898.\n",
      "Action applied with action = 0.17665022611618042.\n",
      "Action applied with action = 0.42395520210266113.\n",
      "Action applied with action = 0.17027567327022552.\n",
      "Action applied with action = 0.2690183222293854.\n",
      "Action applied with action = 0.06535373628139496.\n",
      "Action applied with action = 0.448952317237854.\n",
      "Action applied with action = 0.3533311188220978.\n",
      "Action applied with action = 0.058417826890945435.\n",
      "Action applied with action = 0.1296796053647995.\n",
      "Action applied with action = 0.17273423075675964.\n",
      "Action applied with action = 0.524764358997345.\n",
      "Action applied with action = 0.25999531149864197.\n",
      "Action applied with action = 0.47210851311683655.\n",
      "Action applied with action = 0.1967783123254776.\n",
      "Action applied with action = 0.4666735529899597.\n",
      "Action applied with action = 0.07856009155511856.\n",
      "Action applied with action = 0.09179402142763138.\n",
      "Action applied with action = 0.4291541874408722.\n",
      "Action applied with action = 0.44461193680763245.\n",
      "Action applied with action = 0.22216255962848663.\n",
      "Action applied with action = 0.13413256406784058.\n",
      "Action applied with action = 0.29212865233421326.\n",
      "Action applied with action = 0.29448017477989197.\n",
      "Action applied with action = 0.42880213260650635.\n",
      "Action applied with action = 0.05793681740760803.\n",
      "Action applied with action = 0.12657137215137482.\n",
      "Action applied with action = 0.2220422774553299.\n",
      "Action applied with action = 0.31478336453437805.\n",
      "Action applied with action = 0.06397048383951187.\n",
      "Action applied with action = 0.49401000142097473.\n",
      "Action applied with action = 0.0206659696996212.\n",
      "Action applied with action = 0.040675222873687744.\n",
      "Action applied with action = 0.46813151240348816.\n",
      "Action applied with action = 0.013009542599320412.\n",
      "Action applied with action = 0.03623422980308533.\n",
      "Action applied with action = 0.51850426197052.\n",
      "Action applied with action = 0.2083585262298584.\n",
      "Action applied with action = 0.2217986285686493.\n",
      "Action applied with action = 0.3319339156150818.\n",
      "Action applied with action = 0.10297908633947372.\n",
      "Action applied with action = 0.07631757855415344.\n",
      "Action applied with action = 0.40147966146469116.\n",
      "Action applied with action = 0.3091835379600525.\n",
      "Action applied with action = 0.5020145177841187.\n",
      "Action applied with action = 0.42687737941741943.\n",
      "Action applied with action = 0.3772841691970825.\n",
      "Action applied with action = 0.2604871094226837.\n",
      "Action applied with action = 0.20307552814483643.\n",
      "Action applied with action = 0.16409142315387726.\n",
      "Action applied with action = 0.03909704089164734.\n",
      "Action applied with action = 0.024692516773939133.\n",
      "Action applied with action = 0.30442872643470764.\n",
      "Action applied with action = 0.3955756425857544.\n",
      "Action applied with action = 0.5009607672691345.\n",
      "Action applied with action = 0.43000343441963196.\n",
      "Action applied with action = 0.2531689703464508.\n",
      "Action applied with action = 0.4638032019138336.\n",
      "Action applied with action = 0.05212126672267914.\n",
      "Action applied with action = 0.4156724512577057.\n",
      "Action applied with action = 0.028194986283779144.\n",
      "Action applied with action = 0.4408462643623352.\n",
      "Action applied with action = 0.08710964024066925.\n",
      "Action applied with action = 0.4480602741241455.\n",
      "Action applied with action = 0.4382529854774475.\n",
      "Action applied with action = 0.28343549370765686.\n",
      "Action applied with action = 0.40670472383499146.\n",
      "Action applied with action = 0.10613612830638885.\n",
      "Action applied with action = 0.32905304431915283.\n",
      "Episode ended with reward = 0.9046016215225039, state = 0.9875914115790219, observation = [0. 0. 0. 0. 0. 0.], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "step = 0: AverageReturn = 0.904602, AverageEpisodeLength = 0.000000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We instantiated the eval Actor with actor.eval_metrics above, which creates most commonly used metrics during policy evaluation:\n",
    "\n",
    "- Average return. The return is the sum of rewards obtained while running a policy in an environment for an episode, and we usually average this over a few episodes.\n",
    "- Average episode length.\n",
    "\n",
    "We run the Actor to generate these metrics.\n",
    "\"\"\"\n",
    "\n",
    "def get_eval_metrics():\n",
    "  eval_actor.run()\n",
    "  results = {}\n",
    "  for metric in eval_actor.metrics:\n",
    "    results[metric.name] = metric.result()\n",
    "  return results\n",
    "\n",
    "metrics = get_eval_metrics()\n",
    "\n",
    "def log_eval_metrics(step, metrics):\n",
    "  eval_results = (', ').join(\n",
    "      '{} = {:.6f}'.format(name, result) for name, result in metrics.items())\n",
    "  print('step = {0}: {1}'.format(step, eval_results))\n",
    "\n",
    "log_eval_metrics(0, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Action applied with action = 0.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (10736) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (10736) so Table uniform_table is accessed directly without gRPC.\n",
      "2023-08-01 12:30:13.417288: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f9b9402a7a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-01 12:30:13.417309: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Ti, Compute Capability 8.9\n",
      "2023-08-01 12:30:13.420773: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-08-01 12:30:13.537831: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-08-01 12:30:13.603962: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2023-08-01 12:30:13.605151: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action applied with action = 0.307146817445755.\n",
      "Action applied with action = 0.580174446105957.\n",
      "Action applied with action = 0.2431471347808838.\n",
      "Action applied with action = 1.6337933540344238.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 1.4397543668746948.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 0.4764465093612671.\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "step = 10: AverageReturn = 0.904602, AverageEpisodeLength = 0.000000\n",
      "step = 10: loss = 1.8816044330596924\n",
      "Action applied with action = 0.46933653950691223.\n",
      "Action applied with action = 2.259485960006714.\n",
      "Action applied with action = 0.5657114386558533.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 1.2318120002746582.\n",
      "Action applied with action = 0.19325010478496552.\n",
      "Action applied with action = 1.8222267627716064.\n",
      "Action applied with action = 1.3946130275726318.\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "step = 20: AverageReturn = 0.904602, AverageEpisodeLength = 0.000000\n",
      "step = 20: loss = 1.8403115272521973\n",
      "Action applied with action = 0.972040057182312.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 0.3486158549785614.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 1.0705642700195312.\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "step = 30: AverageReturn = 0.904602, AverageEpisodeLength = 0.000000\n",
      "step = 30: loss = 0.619933009147644\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 1.1636666059494019.\n",
      "Action applied with action = 0.3438771963119507.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 1.4440466165542603.\n",
      "Action applied with action = 2.267308235168457.\n",
      "Action applied with action = 0.32578253746032715.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 1.1533286571502686.\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "step = 40: AverageReturn = 0.904602, AverageEpisodeLength = 0.000000\n",
      "step = 40: loss = 2.2131779193878174\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 1.7208945751190186.\n",
      "Action applied with action = 0.30360883474349976.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 0.14108312129974365.\n",
      "Action applied with action = 0.09268873184919357.\n",
      "Action applied with action = 2.7156383991241455.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 1.0489802360534668.\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "step = 50: AverageReturn = 0.904602, AverageEpisodeLength = 0.000000\n",
      "step = 50: loss = 1.6965075731277466\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 0.7041967511177063.\n",
      "Action applied with action = 0.8628774881362915.\n",
      "Action applied with action = 0.1980086863040924.\n",
      "Action applied with action = 1.6454403400421143.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 0.7283099889755249.\n",
      "Action applied with action = 2.645467758178711.\n",
      "Action applied with action = 0.3277599811553955.\n",
      "Action applied with action = 0.3082166910171509.\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "step = 60: AverageReturn = 0.904602, AverageEpisodeLength = 0.000000\n",
      "step = 60: loss = 0.7381061911582947\n",
      "Action applied with action = 0.33473843336105347.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 0.5364042520523071.\n",
      "Action applied with action = 0.3293364346027374.\n",
      "Action applied with action = 0.146158367395401.\n",
      "Action applied with action = 2.0271196365356445.\n",
      "Action applied with action = 1.222643494606018.\n",
      "Action applied with action = 0.1592971384525299.\n",
      "Action applied with action = 0.0.\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "step = 70: AverageReturn = 0.904602, AverageEpisodeLength = 0.000000\n",
      "step = 70: loss = 0.6096959114074707\n",
      "Action applied with action = 1.0564578771591187.\n",
      "Action applied with action = 0.43345701694488525.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 0.6560502052307129.\n",
      "Action applied with action = 0.985790491104126.\n",
      "Action applied with action = 0.3499543070793152.\n",
      "Action applied with action = 2.1649909019470215.\n",
      "Action applied with action = 1.212456226348877.\n",
      "Action applied with action = 0.474873811006546.\n",
      "Action applied with action = 0.04164844751358032.\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "step = 80: AverageReturn = 0.904602, AverageEpisodeLength = 0.000000\n",
      "step = 80: loss = 0.1595643311738968\n",
      "Action applied with action = 1.0995252132415771.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 0.1625279188156128.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 1.8237922191619873.\n",
      "Action applied with action = 0.0834227129817009.\n",
      "Action applied with action = 0.8885166645050049.\n",
      "Action applied with action = 0.32887256145477295.\n",
      "Action applied with action = 0.9960196018218994.\n",
      "Action applied with action = 0.44507288932800293.\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "step = 90: AverageReturn = 0.904602, AverageEpisodeLength = 0.000000\n",
      "step = 90: loss = 0.2034912407398224\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 0.09248007833957672.\n",
      "Action applied with action = 0.4970077872276306.\n",
      "Action applied with action = 2.1820573806762695.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 1.6941447257995605.\n",
      "Action applied with action = 0.7553046941757202.\n",
      "Action applied with action = 0.0.\n",
      "Action applied with action = 0.4856851100921631.\n",
      "Action applied with action = 0.004095032811164856.\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "Episode ended with reward = 0.0, state = 0.9875914115790219, observation = [0.5201983 0.        0.        0.        0.        0.       ], action = [0.5201983  0.5201983  0.45424455 0.29756284 0.00994802 0.269907\n",
      " 0.5022057  0.43795422 0.3305938  0.05389679 0.04739867 0.4482254\n",
      " 0.39335337 0.3142131  0.10513055 0.3088978  0.07018223 0.19661194\n",
      " 0.3367121  0.37686652 0.24250239 0.43501997 0.194535   0.35543162\n",
      " 0.26280528 0.2852861  0.09948042 0.42401087 0.17448664 0.37017798\n",
      " 0.05051342 0.13889617 0.17665023 0.4239552  0.17027567 0.26901832\n",
      " 0.06535374 0.44895232 0.35333112 0.05841783 0.1296796  0.17273423\n",
      " 0.52476436 0.2599953  0.4721085  0.19677831 0.46667355 0.07856009\n",
      " 0.09179402 0.4291542  0.44461194 0.22216256 0.13413256 0.29212865\n",
      " 0.29448017 0.42880213 0.05793682 0.12657137 0.22204228 0.31478336\n",
      " 0.06397048 0.49401    0.02066597 0.04067522 0.4681315  0.01300954\n",
      " 0.03623423 0.51850426 0.20835853 0.22179863 0.33193392 0.10297909\n",
      " 0.07631758 0.40147966 0.30918354 0.5020145  0.42687738 0.37728417\n",
      " 0.2604871  0.20307553 0.16409142 0.03909704 0.02469252 0.30442873\n",
      " 0.39557564 0.50096077 0.43000343 0.25316897 0.4638032  0.05212127\n",
      " 0.41567245 0.02819499 0.44084626 0.08710964 0.44806027 0.438253\n",
      " 0.2834355  0.40670472 0.10613613 0.32905304].\n",
      "step = 100: AverageReturn = 0.904602, AverageEpisodeLength = 0.000000\n",
      "step = 100: loss = 1.1875879764556885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/platform/default/server.cc:84] Shutting down replay server\n"
     ]
    }
   ],
   "source": [
    "# The training loop involves both collecting data from the environment and optimizing the agent's networks.\n",
    "# Along the way, we will occasionally evaluate the agent's policy to see how we are doing.\n",
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = get_eval_metrics()[\"AverageReturn\"]\n",
    "returns = [avg_return]\n",
    "losses = [0]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "  # Training.\n",
    "  collect_actor.run()\n",
    "  loss_info = agent_learner.run(iterations=1)\n",
    "\n",
    "  # Evaluating.\n",
    "  step = agent_learner.train_step_numpy\n",
    "\n",
    "  if eval_interval and step % eval_interval == 0:\n",
    "    metrics = get_eval_metrics()\n",
    "    log_eval_metrics(step, metrics)\n",
    "    returns.append(metrics[\"AverageReturn\"])\n",
    "    losses.append(loss_info.loss.numpy())\n",
    "    \n",
    "\n",
    "  if log_interval and step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, loss_info.loss.numpy()))\n",
    "\n",
    "rb_observer.close()\n",
    "reverb_server.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8548486000299453, 0.9543547862768174)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.grid()\n",
    "plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.11065889596939088, 2.3238368153572084)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.grid()\n",
    "plt.ylim()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
