{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 00:01:14.794663: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-01 00:01:14.822443: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-01 00:01:15.302108: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    }
   ],
   "source": [
    "from Solver import Particle, Perceptron, PerceptronModel, VicsekModel, NeuralNetwork, PerceptronMode, Mode, NeuralSwarmModel\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation settings\n",
    "settings = {\n",
    "        #                  N,      L,      v,      noise,  r\n",
    "        \"XXsmall\": [       5,      4,      0.03,   0.1,    1],\n",
    "        \"Xsmall\": [        20,     6,      0.03,   0.1,    1],\n",
    "        \"small\": [         100,    30,     0.03,   0.1,    1],\n",
    "        \"a\": [             300,    7,      0.03,   2.0,    1],\n",
    "        \"b\": [             300,    25,     0.03,   0.5,    1],\n",
    "        \"d\": [             300,    5,      0.03,   0.1,    1],\n",
    "        \"plot1_N40\": [     40,     3.1,    0.03,   0.1,    1],\n",
    "        \"large\": [         2000,   60,     0.03,   0.3,    1]\n",
    "    }\n",
    "    \n",
    "# Choose between RADIUS, FIXED, FIXEDRADIUS\n",
    "mode = Mode.FIXEDRADIUS\n",
    "# Flags\n",
    "ZDimension = False     # 2D or 3D\n",
    "# Duration of simulation\n",
    "timesteps = 5000\n",
    "# Choose settings\n",
    "chosen_settings = settings[\"small\"]\n",
    "N       = chosen_settings[0]\n",
    "L       = chosen_settings[1]\n",
    "v       = chosen_settings[2]\n",
    "noise   = chosen_settings[3]\n",
    "r       = chosen_settings[4]\n",
    "k_neighbors = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use \"num_iterations = 1e6\" for better results (2 hrs)\n",
    "# 1e5 is just so this doesn't take too long (1 hr)\n",
    "num_iterations = 10000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000 # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 1000 # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 32 # @param {type:\"integer\"}\n",
    "\n",
    "critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "alpha_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "target_update_tau = 0.005 # @param {type:\"number\"}\n",
    "target_update_period = 1 # @param {type:\"number\"}\n",
    "gamma = 0.99 # @param {type:\"number\"}\n",
    "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
    "\n",
    "actor_fc_layer_params = (256, 256)\n",
    "critic_joint_fc_layer_params = (256, 256)\n",
    "\n",
    "log_interval = 500 # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 20 # @param {type:\"integer\"}\n",
    "eval_interval = 1000 # @param {type:\"integer\"}\n",
    "\n",
    "policy_save_interval = 500 # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import py_environment\n",
    "\n",
    "\n",
    "# Create a custom environment\n",
    "class SimulationEnvironment(py_environment.PyEnvironment):\n",
    "    \"\"\"Interface for a swarm simulation environment.\n",
    "    \n",
    "    Can be converted into a TensorFlow environment.\n",
    "    \n",
    "    Provides uniform access to the simulation and hosts the reward function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # THOUGHTS: This should be correct. A particle can only choose an angle, which is a float (scalar)\n",
    "        # Allow for small negative values to allow for numerical errors\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.float32, minimum=-1e-6, maximum=2*np.pi, name='action')\n",
    "        \n",
    "        # THOUGHTS: This should be correct. The observation is a vector of length k_neighbors + 1, where each entry is an angle. There is no information about the position of the particles.\n",
    "        \n",
    "        # k_neighbors + 1 because the particle itself is also included\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(k_neighbors + 1,), dtype=np.float32, minimum=0, maximum=2*np.pi, name='observation')\n",
    "        # THOUGHTS: What is _state? Is it the current state of the environment?\n",
    "        # This should be different from observation, because the observation is what the agent sees, while the state is the actual state of the environment.\n",
    "        # So the state should be the order parameter which is to be maximized.\n",
    "        self._episode_ended = False\n",
    "        self.simulation = NeuralSwarmModel(N, L, v, noise, r, mode, k_neighbors, ZDimension, seed=True)\n",
    "        self._state = self.simulation.mean_direction2D()\n",
    "        # One \"episode\" and its corresponding reward consists of the iteration over all N particles\n",
    "        self.index = 0\n",
    "        # To change all angles at once, we need to store the new angles in a list\n",
    "        self.new_angles = np.zeros(shape=(N,), dtype=np.float32)\n",
    "        observation = self.simulation.get_angles(self.index)\n",
    "        self._current_time_step = ts.restart(np.array(observation, dtype=np.float32))\n",
    "\n",
    "    def observation_spec(self):\n",
    "        \"\"\"Return observation_spec.\"\"\"\n",
    "        # DONE\n",
    "        return self._observation_spec\n",
    "\n",
    "    def action_spec(self):\n",
    "        \"\"\"Return action_spec.\"\"\"\n",
    "        # DONE\n",
    "        return self._action_spec\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Return initial_time_step and reset the simulation.\n",
    "        \n",
    "        Note that this is a hard reset and not a reset for the current epoch.\"\"\"\n",
    "        \n",
    "        # DONE\n",
    "        self._current_time_step = self._reset()\n",
    "        return self._current_time_step\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Apply action and return new time_step.\"\"\"\n",
    "        # DONE\n",
    "        if self._current_time_step is None:\n",
    "            return self.reset()\n",
    "        self._current_time_step = self._step(action)\n",
    "        return self._current_time_step\n",
    "\n",
    "    def current_time_step(self):\n",
    "        # DONE\n",
    "        return self._current_time_step\n",
    "\n",
    "    # def time_step_spec(self):\n",
    "        \"\"\"Return time_step_spec.\"\"\"\n",
    "        # DONE\n",
    "        # return ts.time_step_spec(self.observation_spec())\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Return initial_time_step and reset the simulation.\n",
    "        \n",
    "        Note that this is a hard reset and not a reset for the current epoch.\"\"\"\n",
    "        # THOUGHTS: In this case, a differentiation has to be made between an episode and an epoch.\n",
    "        # The episode ends when all particles have been updated. An epoch ends when the simulation is reset.\n",
    "        \n",
    "        # Reset simulation\n",
    "        self.simulation = NeuralSwarmModel(N, L, v, noise, r, mode, k_neighbors, ZDimension, seed=True)\n",
    "        self._state = self.simulation.mean_direction2D()\n",
    "        self._episode_ended = False\n",
    "        self.index = 0\n",
    "        observation = self.simulation.get_angles(self.index)\n",
    "        return ts.restart(np.array(observation, dtype=np.float32))\n",
    "\n",
    "    def _step(self, action):\n",
    "        \"\"\"Apply action and return new time_step.\n",
    "        This method hosts the reward function.\"\"\"\n",
    "\n",
    "        # if self._episode_ended:\n",
    "        #     # The last action ended the episode. Ignore the current action and start a new episode.\n",
    "        #     return self.reset()\n",
    "\n",
    "        # TODO: Make sure episodes don't go on forever. Define a stopping action.\n",
    "        if action >= 0. and self._episode_ended is False:\n",
    "            # Update angle of the particle, but don't update the simulation yet\n",
    "            self.new_angles[self.index] = action    \n",
    "        elif self._episode_ended is False:\n",
    "            raise ValueError('What did you do? This should be a finite float value.')\n",
    "        \n",
    "        # Properly handle the case when the episode ends:\n",
    "        # [x] The episode ends when all particles have been updated\n",
    "        # [x] The driver needs to work with the updated simulation. So the simulation needs to go on instead of being reset (after self._episode_ended = True)\n",
    "\n",
    "        if self._episode_ended:\n",
    "            # Update all angles at once\n",
    "            self.simulation.update_angles(self.new_angles)\n",
    "            \n",
    "            oldState = self._state\n",
    "            \n",
    "            self.simulation.update()\n",
    "            self._state = self.simulation.mean_direction2D()\n",
    "            \n",
    "            # The reward is the difference between the new state and the old state.\n",
    "            # An increase in the order parameter is rewarded, a decrease is punished.\n",
    "            reward = self._state - oldState\n",
    "            observation = self.simulation.get_angles(self.index)\n",
    "            # The observation (first argument of ts.termination) is the angles of the neighbors of the particle\n",
    "            self.index = 0\n",
    "            return ts.termination(np.array(observation, dtype=np.float32), reward)\n",
    "        else:\n",
    "            observation = self.simulation.get_angles(self.index)\n",
    "            self.index += 1\n",
    "            if self.index >= N:\n",
    "                self._episode_ended = True\n",
    "            return ts.transition(np.array(observation, dtype=np.float32), reward=0.0, discount=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.train.utils import strategy_utils\n",
    "\n",
    "# Distribution strategy\n",
    "# For now, don't use GPU or TPU\n",
    "\n",
    "strategy = strategy_utils.get_strategy(tpu=False, use_gpu=False)\n",
    "\n",
    "# All variables and Agents need to be created under strategy.scope()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment wrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/tf_agents/specs/array_spec.py:335: RuntimeWarning: invalid value encountered in cast\n",
      "  self._minimum[self._minimum == -np.inf] = low\n",
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/tf_agents/specs/array_spec.py:336: RuntimeWarning: invalid value encountered in cast\n",
      "  self._minimum[self._minimum == np.inf] = high\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.networks import network\n",
    "from tf_agents.train.utils import spec_utils\n",
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "# There are two networks and two environments: one for training and one for evaluation.\n",
    "collect_env = SimulationEnvironment()\n",
    "eval_env = SimulationEnvironment()\n",
    "\n",
    "# Wrap the environment in a TF environment.\n",
    "tf_collect_env = tf_py_environment.TFPyEnvironment(collect_env)\n",
    "tf_eval_env = tf_py_environment.TFPyEnvironment(eval_env)\n",
    "\n",
    "# For the network to work with the environment, the specs have to be known.\n",
    "observation_spec, action_spec, time_step_spec = (spec_utils.get_tensor_specs(tf_collect_env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.environments import utils\n",
    "\n",
    "# Test the python environments\n",
    "\n",
    "utils.validate_py_environment(collect_env, episodes=N)\n",
    "utils.validate_py_environment(eval_env, episodes=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YES! IT WORKS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.agents.sac import tanh_normal_projection_network\n",
    "\n",
    "# Define a network that can learn to predict the action given an observation.\n",
    "# This is a simple (on demand fully connected) network that takes in an observation and outputs an action.\n",
    "\n",
    "class ActorNet(network.Network):\n",
    "\n",
    "  def __init__(self, input_tensor_spec, output_tensor_spec):\n",
    "    super(ActorNet, self).__init__(\n",
    "        input_tensor_spec=input_tensor_spec,\n",
    "        state_spec=(),\n",
    "        name='ActorNet')\n",
    "    self._output_tensor_spec = output_tensor_spec\n",
    "    # THOUGHTS: For now, only one layer is used. This can be changed later.\n",
    "    self._sub_layers = [\n",
    "        tf.keras.layers.Dense(\n",
    "            action_spec.shape.num_elements(), activation=\"linear\"),\n",
    "    ]\n",
    "\n",
    "  def call(self, observations, step_type, network_state):\n",
    "    del step_type\n",
    "\n",
    "    output = tf.cast(observations, dtype=tf.float32)\n",
    "    for layer in self._sub_layers:\n",
    "      output = layer(output)\n",
    "    actions = tf.reshape(output, [-1] + self._output_tensor_spec.shape.as_list())\n",
    "\n",
    "    # Scale and shift actions to the correct range if necessary.\n",
    "    return actions, network_state\n",
    "\n",
    "\n",
    "# Create the Actor Network\n",
    "actor = ActorNet(\n",
    "    input_tensor_spec=observation_spec,\n",
    "    output_tensor_spec=action_spec)\n",
    "\n",
    "\n",
    "# Critic Network\n",
    "with strategy.scope():\n",
    "  critic_net = critic_network.CriticNetwork(\n",
    "        (observation_spec, action_spec),\n",
    "        observation_fc_layer_params=None,\n",
    "        action_fc_layer_params=None,\n",
    "        joint_fc_layer_params=critic_joint_fc_layer_params,\n",
    "        activation_fn=None,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        last_kernel_initializer='glorot_uniform')\n",
    "  \n",
    "# Actor Distribution Network\n",
    "# This is a distribution over the actor nerwork\n",
    "# TODO: It is not clear what continuous_projection_net does. tanh might not be the best choice.\n",
    "with strategy.scope():\n",
    "  actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "      observation_spec,\n",
    "      action_spec,\n",
    "      activation_fn=None,\n",
    "      fc_layer_params=actor_fc_layer_params,\n",
    "      continuous_projection_net=tanh_normal_projection_network.TanhNormalProjectionNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 00:01:16.329117: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 00:01:16.358761: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 00:01:16.358793: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 00:01:16.361854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 00:01:16.361893: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 00:01:16.361907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 00:01:16.814535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 00:01:16.814577: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 00:01:16.814583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-08-01 00:01:16.814599: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-01 00:01:16.814623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9330 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2023-08-01 00:01:18.416705: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.train.utils import train_utils\n",
    "\n",
    "# Initialize the agent\n",
    "\n",
    "with strategy.scope():\n",
    "  train_step = train_utils.create_train_step()\n",
    "\n",
    "  tf_agent = ddpg_agent.DdpgAgent(\n",
    "        time_step_spec,\n",
    "        action_spec,\n",
    "        actor_network=actor_net,\n",
    "        critic_network=critic_net,\n",
    "        actor_optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=actor_learning_rate),\n",
    "        critic_optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=critic_learning_rate),\n",
    "        target_update_tau=target_update_tau,\n",
    "        target_update_period=target_update_period,\n",
    "        td_errors_loss_fn=tf.math.squared_difference,\n",
    "        gamma=gamma,\n",
    "        reward_scale_factor=reward_scale_factor,\n",
    "        train_step_counter=train_step)\n",
    "\n",
    "  tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/platform/tfrecord_checkpointer.cc:162]  Initializing TFRecordCheckpointer in /tmp/tmpykzuvzxb.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:567] Loading latest checkpoint from /tmp/tmpykzuvzxb\n",
      "[reverb/cc/platform/default/server.cc:71] Started replay server on port 34627\n"
     ]
    }
   ],
   "source": [
    "import reverb\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "\n",
    "# Use Reverb, a framework for experience replay developed by DeepMind, to store and sample experience tuples for training.\n",
    "# Using a samples_per_insert somewhere between 2 and 1000. This is a trade-off between the number of samples that can be drawn from the replay buffer and the number of times the replay buffer needs to be updated.\n",
    "rate_limiter=reverb.rate_limiters.SampleToInsertRatio(samples_per_insert=3.0, min_size_to_sample=3, error_buffer=3.0)\n",
    "\n",
    "table_name = 'uniform_table'\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=replay_buffer_capacity,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1))\n",
    "\n",
    "reverb_server = reverb.Server([table])\n",
    "\n",
    "# Since the agent needs N steps of experience to make an update, the dataset will need to sample batches of N steps + 1 to allow the agent to learn from a complete transition.\n",
    "\n",
    "reverb_replay = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    tf_agent.collect_data_spec,\n",
    "    sequence_length= 2,\n",
    "    table_name=table_name,\n",
    "    local_server=reverb_server)\n",
    "\n",
    "# A dataset is created from the replay buffer to be fed to the agent for training. \n",
    "dataset = reverb_replay.as_dataset(\n",
    "      sample_batch_size=batch_size, num_steps=2).prefetch(50)\n",
    "experience_dataset_fn = lambda: dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "\n",
    "# Policies\n",
    "# Create policies from the agent\n",
    "\n",
    "tf_eval_policy = tf_agent.policy\n",
    "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "  tf_eval_policy, use_tf_function=True)\n",
    "\n",
    "tf_collect_policy = tf_agent.collect_policy\n",
    "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "  tf_collect_policy, use_tf_function=True)\n",
    "\n",
    "# Random policy to sample from the environment\n",
    "random_policy = random_py_policy.RandomPyPolicy(\n",
    "  collect_env.time_step_spec(), collect_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.train import actor\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.train import learner\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "tempdir = tempfile.gettempdir()\n",
    "\n",
    "# As the Actors run data collection steps, they pass trajectories of (state, action, reward) to the observer, which caches and writes them to the Reverb replay system.\n",
    "\n",
    "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
    "  reverb_replay.py_client,\n",
    "  table_name,\n",
    "  sequence_length= 2,\n",
    "  stride_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an Actor with the random policy and collect experiences to seed the replay buffer with.\n",
    "\n",
    "initial_collect_actor = actor.Actor(\n",
    "  collect_env,\n",
    "  random_policy,\n",
    "  train_step,\n",
    "  steps_per_run=initial_collect_steps,\n",
    "  observers=[rb_observer])\n",
    "\n",
    "initial_collect_actor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an Actor with the collect policy to gather more experiences during training.\n",
    "\n",
    "env_step_metric = py_metrics.EnvironmentSteps()\n",
    "\n",
    "collect_actor = actor.Actor(\n",
    "  collect_env,\n",
    "  collect_policy,\n",
    "  train_step,\n",
    "  steps_per_run= N + 1,\n",
    "  metrics=actor.collect_metrics(10),\n",
    "  summary_dir=os.path.join(tempdir, learner.TRAIN_DIR),\n",
    "  observers=[rb_observer, env_step_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Actor which will be used to evaluate the policy during training.\n",
    "# actor.eval_metrics(num_eval_episodes) to log metrics later.\n",
    "\n",
    "eval_actor = actor.Actor(\n",
    "  eval_env,\n",
    "  eval_policy,\n",
    "  train_step,\n",
    "  episodes_per_run=num_eval_episodes,\n",
    "  metrics=actor.eval_metrics(num_eval_episodes),\n",
    "  summary_dir=os.path.join(tempdir, 'eval'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.train import triggers\n",
    "\n",
    "# Learners\n",
    "# The Learner component contains the agent and performs gradient step updates to the policy variables using experience data from the replay buffer.\n",
    "# After one or more training steps, the Learner can push a new set of variable values to the variable container.\n",
    "\n",
    "saved_model_dir = os.path.join(tempdir, learner.POLICY_SAVED_MODEL_DIR)\n",
    "\n",
    "# Triggers to save the agent's policy checkpoints.\n",
    "learning_triggers = [\n",
    "    triggers.PolicySavedModelTrigger(\n",
    "        saved_model_dir,\n",
    "        tf_agent,\n",
    "        train_step,\n",
    "        interval=policy_save_interval),\n",
    "    triggers.StepPerSecondLogTrigger(train_step, interval=1000),\n",
    "]\n",
    "\n",
    "agent_learner = learner.Learner(\n",
    "  tempdir,\n",
    "  train_step,\n",
    "  tf_agent,\n",
    "  experience_dataset_fn,\n",
    "  triggers=learning_triggers,\n",
    "  strategy=strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We instantiated the eval Actor with actor.eval_metrics above, which creates most commonly used metrics during policy evaluation:\n",
    "\n",
    "- Average return. The return is the sum of rewards obtained while running a policy in an environment for an episode, and we usually average this over a few episodes.\n",
    "- Average episode length.\n",
    "\n",
    "We run the Actor to generate these metrics.\n",
    "\"\"\"\n",
    "\n",
    "def get_eval_metrics():\n",
    "  eval_actor.run()\n",
    "  results = {}\n",
    "  for metric in eval_actor.metrics:\n",
    "    results[metric.name] = metric.result()\n",
    "  return results\n",
    "\n",
    "metrics = get_eval_metrics()\n",
    "\n",
    "def log_eval_metrics(step, metrics):\n",
    "  eval_results = (', ').join(\n",
    "      '{} = {:.6f}'.format(name, result) for name, result in metrics.items())\n",
    "  print('step = {0}: {1}'.format(step, eval_results))\n",
    "\n",
    "log_eval_metrics(0, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training loop involves both collecting data from the environment and optimizing the agent's networks.\n",
    "# Along the way, we will occasionally evaluate the agent's policy to see how we are doing.\n",
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = get_eval_metrics()[\"AverageReturn\"]\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "  # Training.\n",
    "  collect_actor.run()\n",
    "  loss_info = agent_learner.run(iterations=1)\n",
    "\n",
    "  # Evaluating.\n",
    "  step = agent_learner.train_step_numpy\n",
    "\n",
    "  if eval_interval and step % eval_interval == 0:\n",
    "    metrics = get_eval_metrics()\n",
    "    log_eval_metrics(step, metrics)\n",
    "    returns.append(metrics[\"AverageReturn\"])\n",
    "\n",
    "  if log_interval and step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, loss_info.loss.numpy()))\n",
    "\n",
    "rb_observer.close()\n",
    "reverb_server.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two types of tf drivers: dynamic_episode_driver and dynamic_step_driver\n",
    "# dynamic_episode_driver: terminates after a given number of episodes\n",
    "# dynamic_step_driver: terminates after a given number of (valid) environment steps\n",
    "\n",
    "# Here we will use dynamic_episode_driver\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.metrics import tf_metrics\n",
    "\n",
    "num_episodes = tf_metrics.NumberOfEpisodes()\n",
    "env_steps = tf_metrics.EnvironmentSteps()\n",
    "observers = [num_episodes, env_steps]\n",
    "driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_env, tf_policy, observers, num_episodes=2)\n",
    "\n",
    "# Initial driver.run will reset the environment and initialize the policy.\n",
    "final_time_step, policy_state = driver.run()\n",
    "\n",
    "print('final_time_step', final_time_step)\n",
    "print('Number of Steps: ', env_steps.result().numpy())\n",
    "print('Number of Episodes: ', num_episodes.result().numpy())\n",
    "\n",
    "# Continue running from previous state\n",
    "final_time_step, _ = driver.run(final_time_step, policy_state)\n",
    "\n",
    "print('final_time_step', final_time_step)\n",
    "print('Number of Steps: ', env_steps.result().numpy())\n",
    "print('Number of Episodes: ', num_episodes.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for a DDPG agent in TensorFlow 2.0\n",
    "\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.dense1 = layers.Dense(64, activation='relu')\n",
    "        self.dense2 = layers.Dense(64, activation='relu')\n",
    "        self.out = layers.Dense(action_dim, activation='tanh')\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.dense1(state)\n",
    "        x = self.dense2(x)\n",
    "        action = self.out(x)\n",
    "        return action\n",
    "\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.dense1 = layers.Dense(64, activation='relu')\n",
    "        self.dense2 = layers.Dense(64, activation='relu')\n",
    "        self.out = layers.Dense(1)\n",
    "\n",
    "    def call(self, state, action):\n",
    "        x = tf.concat([state, action], axis=-1)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        value = self.out(x)\n",
    "        return value\n",
    "\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.actor = Actor(action_dim)\n",
    "        self.critic = Critic()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "        action = self.actor(state)\n",
    "        return action[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
