{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "2023-08-12 13:05:38,788\tWARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "2023-08-12 13:05:38,833\tWARNING deprecation.py:50 -- DeprecationWarning: `build_tf_policy` has been deprecated. This will raise an error in the future!\n",
      "2023-08-12 13:05:38,841\tWARNING deprecation.py:50 -- DeprecationWarning: `build_policy_class` has been deprecated. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "# Basics\n",
    "from Solver import Particle, Perceptron, PerceptronModel, VicsekModel, NeuralNetwork, PerceptronMode, Mode, NeuralSwarmModel\n",
    "\n",
    "import numpy        as np\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.utils import check_env\n",
    "from ray.rllib.algorithms.maddpg import MADDPGConfig\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.algorithms.maddpg import maddpg_tf_policy\n",
    "\n",
    "from gymnasium.spaces import Box\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation settings\n",
    "settings = {\n",
    "        #                  N,      L,      v,      noise,  r\n",
    "        \"testing\": [       1,      10,     0.03,   0.1,    1],\n",
    "        \"small\": [         100,    10,     0.03,   0.1,    1],\n",
    "        \"medium\": [        1000,   10,     0.03,   0.1,    1],\n",
    "    }\n",
    "    \n",
    "# Choose between RADIUS, FIXED, FIXEDRADIUS (don't use RADIUS)\n",
    "mode = Mode.FIXEDRADIUS\n",
    "# Flags\n",
    "ZDimension = False     # 2D or 3D\n",
    "seed = False           # Random seed\n",
    "# Choose settings\n",
    "chosen_settings = settings[\"small\"]\n",
    "N       = chosen_settings[0]\n",
    "L       = chosen_settings[1]\n",
    "v       = chosen_settings[2]\n",
    "noise   = chosen_settings[3]\n",
    "r       = chosen_settings[4]\n",
    "\n",
    "k_neighbors = 5\n",
    "# Timesteps in an episode\n",
    "T = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Multi-Agent Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentSimulationEnv(MultiAgentEnv):\n",
    "    minimum = 0.0\n",
    "    maximum = 2 * np.pi\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_agents = N\n",
    "        self._spaces_in_preferred_format = True\n",
    "        self._agent_ids = set(range(100))\n",
    "        \n",
    "        # We asume the same action space for all agents\n",
    "        self.action_space = Box(low=self.minimum, high=self.maximum, shape=(), dtype=np.float64)\n",
    "        \n",
    "        # We assume the same observation space for all agents\n",
    "        self.observation_space = Box(low=self.minimum, high=self.maximum, shape=(k_neighbors + 1,), dtype=np.float64)\n",
    "        \n",
    "        self.simulation = NeuralSwarmModel(N, L, v, noise, r, mode, k_neighbors, ZDimension, seed=seed)\n",
    "        self.new_angles = np.zeros(shape=(N,), dtype=np.float64)\n",
    "        self.index = 0\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # Reset the state of the environment to an initial state\n",
    "        observations = {}\n",
    "        infos = {}\n",
    "        self.simulation = NeuralSwarmModel(N, L, v, noise, r, mode, k_neighbors, ZDimension, seed=False)\n",
    "        self.index = 0\n",
    "        self.new_angles = np.zeros(shape=(N,), dtype=np.float64)\n",
    "        for agent_id in range(self.num_agents):\n",
    "            observations[agent_id] = self.simulation.get_angles(agent_id)\n",
    "        return observations, infos\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        # Actions for all agents are provided in a dictionary\n",
    "        \n",
    "        # Rewards for all agents are provided in a dictionary {agent_id: reward}\n",
    "        rewards = {}\n",
    "        # Observations for all agents are provided in a dictionary {agent_id: observation}\n",
    "        new_obs = {}\n",
    "        # Dones for all agents are provided in a dictionary {agent_id: done (boolean))}\n",
    "        dones = {}\n",
    "        # Truncated for all agents are provided in a dictionary {agent_id: truncated (boolean))}\n",
    "        # Truncated is used to indicate that the episode was ended early\n",
    "        truncated = {}\n",
    "        # Infos for all agents are provided in a dictionary {agent_id: info}\n",
    "        # Infos can be used to provide extra information about an agent's state or action\n",
    "        infos = {}\n",
    "        \n",
    "        # Collect all actions and set dones\n",
    "        for agent_id, action in action_dict.items():\n",
    "            action = np.clip(action, self.minimum, self.maximum)\n",
    "            self.new_angles[agent_id] = action\n",
    "            dones[agent_id] = True if self.index >= T else False\n",
    "            \n",
    "        # Update the simulation\n",
    "        self.simulation.update_angles(self.new_angles)\n",
    "        self.simulation.update()\n",
    "        self.index += 1\n",
    "        reward = self.simulation.mean_direction2D()\n",
    "        \n",
    "        # Collect observations and rewards\n",
    "        for agent_id in range(self.num_agents):\n",
    "            new_obs[agent_id] = self.simulation.get_angles(agent_id)\n",
    "            rewards[agent_id] = reward\n",
    "\n",
    "        dones['__all__'] = all(dones.values())  # Ends the episode if all agents are done\n",
    "        \n",
    "        return new_obs, rewards, dones, truncated, infos\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # Optional: For visualization\n",
    "        # Draw particles with matplotlib\n",
    "        # Particles are stored in self.simulation.particles . Positions are stored in particles[i].x and particles[i].y\n",
    "        # NOT YET FUNCTIONAL\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        ax.set_xlim(0, L)\n",
    "        ax.set_ylim(0, L)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title('Simulation')\n",
    "        \n",
    "        for particle in self.simulation.particles:\n",
    "            ax.plot(particle.x, particle.y, 'o', color='black', markersize=10)\n",
    "            \n",
    "        plt.show()\n",
    "\n",
    "    def close(self):\n",
    "        # Optional: Clean up. Called at the end of an episode.\n",
    "        pass\n",
    "    \n",
    "    # Optional methods\n",
    "    def observation_space_contains(self, observation):\n",
    "        # Check if the observation is a valid observation\n",
    "        # Obervation is a dictionary {agent_id: observation}\n",
    "        observations = observation.values()\n",
    "        return all([self.observation_space.contains(obs) for obs in observations])\n",
    "    \n",
    "    def action_space_contains(self, action):\n",
    "        # Check if the action is a valid action\n",
    "        # Action is a dictionary {agent_id: action}\n",
    "        actions = action.values()\n",
    "        return all([self.action_space.contains(act) for act in actions])\n",
    "    \n",
    "    def observation_space_sample(self):\n",
    "        return {agent_id: self.observation_space.sample() for agent_id in range(self.num_agents)}\n",
    "\n",
    "    def action_space_sample(self, action):\n",
    "        return {agent_id: self.action_space.sample() for agent_id in range(self.num_agents)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and test environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode 1\n",
      "Step 1000 finished\n",
      "Episode 1 finished after 1000 timesteps with rewards: 0.07974984115534882\n",
      "Starting episode 2\n",
      "Step 1000 finished\n",
      "Episode 2 finished after 1000 timesteps with rewards: 0.11110731104797063\n"
     ]
    }
   ],
   "source": [
    "env = MultiAgentSimulationEnv(None)\n",
    "\n",
    "for i_episode in range(2):\n",
    "    observations, infos = env.reset()\n",
    "    total_rewards = {agent_id: 0 for agent_id in observations.keys()}\n",
    "    print(f\"Starting episode {i_episode + 1}\")\n",
    "    \n",
    "    # Max steps per episode\n",
    "    for t in range(T + 1):\n",
    "        # Optional: Render the environment for visualization\n",
    "        # env.render()\n",
    "        \n",
    "        # Choose random actions\n",
    "        actions = {agent_id: env.action_space.sample() for agent_id in observations.keys()}\n",
    "        \n",
    "        observations, rewards, dones, truncated, infos = env.step(actions)\n",
    "        \n",
    "        for agent_id, reward in rewards.items():\n",
    "            total_rewards[agent_id] += reward\n",
    "            \n",
    "        print(f\"Step {t}... \\r\", end=\"\")\n",
    "            \n",
    "        if any(dones.values()):\n",
    "            print(f\"Step {t} finished\")\n",
    "            # The reward is the same for all agents. We just take the first one.\n",
    "            print(f\"Episode {i_episode + 1} finished after {t} timesteps with rewards: {next(iter(rewards.values()))}\")\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Mapping\n",
    "\n",
    "In the following code, ``policy_mapping_fn(agent_id)`` is defined to map each agent to a policy. The agents id is used to map each agent to a policy. The policy is then used to compute the action for each agent.\n",
    "\n",
    "In this case, a shared policy is used for all agents. The policy is defined in the ``policy_graph`` function. The policy is a simple neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dict with all agent_ids\n",
    "agent_ids = {\"agent_\" + str(i): i for i in range(N)}\n",
    "\n",
    "def policy_mapping_fn(agent_id):\n",
    "    \"\"\"Returns the policy that should be used by the agent with the id agent_id.\n",
    "    In this case, all agents share the same policy.\n",
    "    \n",
    "    Later on, multiple policies can be used for different agents.\"\"\"\n",
    "    return \"shared_policy\"\n",
    "\n",
    "\n",
    "def get_shared_policy():\n",
    "    policies = {\n",
    "        \"shared_policy\": PolicySpec(\n",
    "            policy_class=maddpg_tf_policy.MADDPGTFPolicy,   # Can also be set to None. Should be the same.\n",
    "            observation_space=env.observation_space, \n",
    "            action_space=env.action_space, \n",
    "            config={}\n",
    "            # Or maybe config=agent_ids? But then the constructor of MADDPGTFPolicy throws an error\n",
    "        )\n",
    "    }\n",
    "    return policies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an individual policy for each agent (for testing)\n",
    "This doesn't work for some reason. It conflicts with the tensorflow framework. The tensor shapes aren't correct. I'm not sure how to fix this. I think it's because the policy is expecting a batch of observations, but I'm only giving it one observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def policy_mapping_fn(agent_id):\n",
    "#     \"\"\"Returns the policy that should be used by the agent with the id agent_id.\n",
    "#     Here, each agent gets its own policy based on its agent_id.\"\"\"\n",
    "#     return \"policy_for_{}\".format(agent_id)\n",
    "\n",
    "# def get_individual_policies():\n",
    "#     \"\"\"Create a separate policy for each agent.\"\"\"\n",
    "#     policies = {}\n",
    "#     for i in range(N):\n",
    "#         policy_name = \"policy_for_agent_{}\".format(i)\n",
    "#         policies[policy_name] = PolicySpec(\n",
    "#             policy_class=None,\n",
    "#             observation_space=env.observation_space,\n",
    "#             action_space=env.action_space,\n",
    "#             config={\"agent_id\": i}\n",
    "#         )\n",
    "#     return policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations\n",
    "\n",
    "`config` is not an ordinary ``dict``. To set its values, use the `config.update_from_dict` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default MADDPG config settings\n",
    "```python\n",
    "extra_python_environs_for_driver {}\n",
    "extra_python_environs_for_worker {}\n",
    "num_gpus 0\n",
    "num_cpus_per_worker 1\n",
    "num_gpus_per_worker 0\n",
    "_fake_gpus False\n",
    "num_learner_workers 0\n",
    "num_gpus_per_learner_worker 0\n",
    "num_cpus_per_learner_worker 1\n",
    "local_gpu_idx 0\n",
    "custom_resources_per_worker {}\n",
    "placement_strategy PACK\n",
    "eager_tracing True\n",
    "eager_max_retraces 20\n",
    "tf_session_args {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}\n",
    "local_tf_session_args {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}\n",
    "torch_compile_learner False\n",
    "torch_compile_learner_what_to_compile TorchCompileWhatToCompile.FORWARD_TRAIN\n",
    "torch_compile_learner_dynamo_backend inductor\n",
    "torch_compile_learner_dynamo_mode None\n",
    "torch_compile_worker False\n",
    "torch_compile_worker_dynamo_backend onnxrt\n",
    "torch_compile_worker_dynamo_mode None\n",
    "env None\n",
    "env_config {}\n",
    "observation_space None\n",
    "action_space None\n",
    "env_task_fn None\n",
    "render_env False\n",
    "clip_rewards None\n",
    "normalize_actions True\n",
    "clip_actions False\n",
    "disable_env_checking False\n",
    "_is_atari None\n",
    "auto_wrap_old_gym_envs True\n",
    "action_mask_key action_mask\n",
    "env_runner_cls None\n",
    "num_envs_per_worker 1\n",
    "sample_collector <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>\n",
    "sample_async False\n",
    "enable_connectors True\n",
    "update_worker_filter_stats True\n",
    "use_worker_filter_stats True\n",
    "rollout_fragment_length 100\n",
    "batch_mode truncate_episodes\n",
    "remote_worker_envs False\n",
    "remote_env_batch_wait_ms 0\n",
    "validate_workers_after_construction True\n",
    "preprocessor_pref deepmind\n",
    "observation_filter NoFilter\n",
    "compress_observations False\n",
    "enable_tf1_exec_eagerly False\n",
    "sampler_perf_stats_ema_coef None\n",
    "gamma 0.99\n",
    "lr 0.001\n",
    "grad_clip None\n",
    "grad_clip_by global_norm\n",
    "train_batch_size 1024\n",
    "model {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}\n",
    "optimizer {}\n",
    "max_requests_in_flight_per_sampler_worker 2\n",
    "_learner_class None\n",
    "_enable_learner_api False\n",
    "explore True\n",
    "exploration_config {'type': 'StochasticSampling'}\n",
    "algorithm_config_overrides_per_module {}\n",
    "policy_map_capacity 100\n",
    "policy_mapping_fn <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7f311e7b6320>\n",
    "policies_to_train None\n",
    "policy_states_are_swappable False\n",
    "observation_fn None\n",
    "count_steps_by env_steps\n",
    "input_config {}\n",
    "actions_in_input_normalized False\n",
    "postprocess_inputs False\n",
    "shuffle_buffer_size 0\n",
    "output None\n",
    "output_config {}\n",
    "output_compress_columns ['obs', 'new_obs']\n",
    "output_max_file_size 67108864\n",
    "offline_sampling False\n",
    "evaluation_interval None\n",
    "evaluation_duration 10\n",
    "evaluation_duration_unit episodes\n",
    "evaluation_sample_timeout_s 180.0\n",
    "evaluation_parallel_to_training False\n",
    "evaluation_config None\n",
    "off_policy_estimation_methods {}\n",
    "ope_split_batch_by_episode True\n",
    "evaluation_num_workers 0\n",
    "always_attach_evaluation_results False\n",
    "enable_async_evaluation False\n",
    "in_evaluation False\n",
    "sync_filters_on_rollout_workers_timeout_s 60.0\n",
    "keep_per_episode_custom_metrics False\n",
    "metrics_episode_collection_timeout_s 60.0\n",
    "metrics_num_episodes_for_smoothing 100\n",
    "min_time_s_per_iteration 0\n",
    "min_train_timesteps_per_iteration 0\n",
    "min_sample_timesteps_per_iteration 0\n",
    "export_native_model_files False\n",
    "checkpoint_trainable_policies_only False\n",
    "logger_creator None\n",
    "logger_config None\n",
    "log_level WARN\n",
    "log_sys_usage True\n",
    "fake_sampler False\n",
    "seed None\n",
    "ignore_worker_failures False\n",
    "recreate_failed_workers False\n",
    "max_num_worker_restarts 1000\n",
    "delay_between_worker_restarts_s 60.0\n",
    "restart_failed_sub_environments False\n",
    "num_consecutive_worker_failures_tolerance 100\n",
    "worker_health_probe_timeout_s 60\n",
    "worker_restore_timeout_s 1800\n",
    "rl_module_spec None\n",
    "_enable_rl_module_api False\n",
    "_AlgorithmConfig__prior_exploration_config None\n",
    "_tf_policy_handles_more_than_one_loss False\n",
    "_disable_preprocessor_api False\n",
    "_disable_action_flattening False\n",
    "_disable_execution_plan_api True\n",
    "_disable_initialize_loss_from_dummy_batch False\n",
    "simple_optimizer -1\n",
    "policy_map_cache -1\n",
    "worker_cls -1\n",
    "synchronize_filters -1\n",
    "replay_sequence_length None\n",
    "agent_id None\n",
    "use_local_critic False\n",
    "use_state_preprocessor False\n",
    "actor_hiddens [64, 64]\n",
    "actor_hidden_activation relu\n",
    "critic_hiddens [64, 64]\n",
    "critic_hidden_activation relu\n",
    "n_step 1\n",
    "good_policy maddpg\n",
    "adv_policy maddpg\n",
    "replay_buffer_config {'type': 'MultiAgentReplayBuffer', 'prioritized_replay': -1, 'capacity': 1000000, 'replay_mode': 'lockstep'}\n",
    "training_intensity None\n",
    "num_steps_sampled_before_learning_starts 25600\n",
    "critic_lr 0.01\n",
    "actor_lr 0.01\n",
    "target_network_update_freq 0\n",
    "tau 0.01\n",
    "actor_feature_reg 0.001\n",
    "grad_norm_clipping 0.5\n",
    "input sampler\n",
    "policies {'default_policy': (None, None, None, None)}\n",
    "callbacks <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>\n",
    "create_env_on_driver False\n",
    "custom_eval_function None\n",
    "framework torch\n",
    "num_cpus_for_driver 1\n",
    "num_workers 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-12 13:05:43,852\tINFO worker.py:1621 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra_python_environs_for_driver {}\n",
      "extra_python_environs_for_worker {}\n",
      "num_gpus 0\n",
      "num_cpus_per_worker 1\n",
      "num_gpus_per_worker 0\n",
      "_fake_gpus False\n",
      "num_learner_workers 0\n",
      "num_gpus_per_learner_worker 0\n",
      "num_cpus_per_learner_worker 1\n",
      "local_gpu_idx 0\n",
      "custom_resources_per_worker {}\n",
      "placement_strategy PACK\n",
      "eager_tracing False\n",
      "eager_max_retraces 20\n",
      "tf_session_args {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}\n",
      "local_tf_session_args {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}\n",
      "torch_compile_learner False\n",
      "torch_compile_learner_what_to_compile TorchCompileWhatToCompile.FORWARD_TRAIN\n",
      "torch_compile_learner_dynamo_backend inductor\n",
      "torch_compile_learner_dynamo_mode None\n",
      "torch_compile_worker False\n",
      "torch_compile_worker_dynamo_backend onnxrt\n",
      "torch_compile_worker_dynamo_mode None\n",
      "env multi_agent_simulation\n",
      "env_config {}\n",
      "observation_space Box(0.0, 6.283185307179586, (6,), float64)\n",
      "action_space Box(0.0, 6.283185307179586, (), float64)\n",
      "env_task_fn None\n",
      "render_env False\n",
      "clip_rewards None\n",
      "normalize_actions True\n",
      "clip_actions False\n",
      "disable_env_checking True\n",
      "_is_atari None\n",
      "auto_wrap_old_gym_envs True\n",
      "action_mask_key action_mask\n",
      "env_runner_cls None\n",
      "num_envs_per_worker 1\n",
      "sample_collector <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>\n",
      "sample_async False\n",
      "enable_connectors True\n",
      "update_worker_filter_stats True\n",
      "use_worker_filter_stats True\n",
      "rollout_fragment_length 100\n",
      "batch_mode truncate_episodes\n",
      "remote_worker_envs False\n",
      "remote_env_batch_wait_ms 0\n",
      "validate_workers_after_construction True\n",
      "preprocessor_pref deepmind\n",
      "observation_filter NoFilter\n",
      "compress_observations False\n",
      "enable_tf1_exec_eagerly False\n",
      "sampler_perf_stats_ema_coef None\n",
      "gamma 0.99\n",
      "lr 0.001\n",
      "grad_clip None\n",
      "grad_clip_by global_norm\n",
      "train_batch_size 1024\n",
      "model {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}\n",
      "optimizer {}\n",
      "max_requests_in_flight_per_sampler_worker 2\n",
      "_learner_class None\n",
      "_enable_learner_api False\n",
      "explore True\n",
      "exploration_config {'type': 'StochasticSampling'}\n",
      "algorithm_config_overrides_per_module {}\n",
      "policy_map_capacity 100\n",
      "policy_mapping_fn <function policy_mapping_fn at 0x7f2f4a6bb520>\n",
      "policies_to_train ['shared_policy']\n",
      "policy_states_are_swappable False\n",
      "observation_fn None\n",
      "count_steps_by env_steps\n",
      "input_config {}\n",
      "actions_in_input_normalized False\n",
      "postprocess_inputs False\n",
      "shuffle_buffer_size 0\n",
      "output None\n",
      "output_config {}\n",
      "output_compress_columns ['obs', 'new_obs']\n",
      "output_max_file_size 67108864\n",
      "offline_sampling False\n",
      "evaluation_interval None\n",
      "evaluation_duration 10\n",
      "evaluation_duration_unit episodes\n",
      "evaluation_sample_timeout_s 180.0\n",
      "evaluation_parallel_to_training False\n",
      "evaluation_config None\n",
      "off_policy_estimation_methods {}\n",
      "ope_split_batch_by_episode True\n",
      "evaluation_num_workers 0\n",
      "always_attach_evaluation_results False\n",
      "enable_async_evaluation False\n",
      "in_evaluation False\n",
      "sync_filters_on_rollout_workers_timeout_s 60.0\n",
      "keep_per_episode_custom_metrics False\n",
      "metrics_episode_collection_timeout_s 60.0\n",
      "metrics_num_episodes_for_smoothing 100\n",
      "min_time_s_per_iteration 0\n",
      "min_train_timesteps_per_iteration 0\n",
      "min_sample_timesteps_per_iteration 0\n",
      "export_native_model_files False\n",
      "checkpoint_trainable_policies_only False\n",
      "logger_creator None\n",
      "logger_config None\n",
      "log_level WARN\n",
      "log_sys_usage True\n",
      "fake_sampler False\n",
      "seed None\n",
      "ignore_worker_failures False\n",
      "recreate_failed_workers False\n",
      "max_num_worker_restarts 1000\n",
      "delay_between_worker_restarts_s 60.0\n",
      "restart_failed_sub_environments False\n",
      "num_consecutive_worker_failures_tolerance 100\n",
      "worker_health_probe_timeout_s 60\n",
      "worker_restore_timeout_s 1800\n",
      "rl_module_spec None\n",
      "_enable_rl_module_api False\n",
      "_AlgorithmConfig__prior_exploration_config None\n",
      "_tf_policy_handles_more_than_one_loss False\n",
      "_disable_preprocessor_api False\n",
      "_disable_action_flattening False\n",
      "_disable_execution_plan_api True\n",
      "_disable_initialize_loss_from_dummy_batch False\n",
      "simple_optimizer True\n",
      "policy_map_cache -1\n",
      "worker_cls -1\n",
      "synchronize_filters -1\n",
      "replay_sequence_length None\n",
      "agent_id None\n",
      "use_local_critic False\n",
      "use_state_preprocessor False\n",
      "actor_hiddens [64, 64]\n",
      "actor_hidden_activation relu\n",
      "critic_hiddens [64, 64]\n",
      "critic_hidden_activation relu\n",
      "n_step 1\n",
      "good_policy maddpg\n",
      "adv_policy maddpg\n",
      "replay_buffer_config {'type': 'MultiAgentReplayBuffer', 'prioritized_replay': -1, 'capacity': 1000000, 'replay_mode': 'lockstep'}\n",
      "training_intensity None\n",
      "num_steps_sampled_before_learning_starts 25600\n",
      "critic_lr 0.01\n",
      "actor_lr 0.01\n",
      "target_network_update_freq 0\n",
      "tau 0.01\n",
      "actor_feature_reg 0.001\n",
      "grad_norm_clipping 0.5\n",
      "input sampler\n",
      "policies {'shared_policy': (<class 'ray.rllib.algorithms.maddpg.maddpg_tf_policy.MADDPGTFPolicy'>, Box(0.0, 6.283185307179586, (6,), float64), Box(0.0, 6.283185307179586, (), float64), {})}\n",
      "callbacks <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>\n",
      "create_env_on_driver False\n",
      "custom_eval_function None\n",
      "framework tf\n",
      "num_cpus_for_driver 1\n",
      "num_workers 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/gymnasium/spaces/box.py:227: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  logger.warn(\"Casting input x to numpy array.\")\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ray\n",
    "# ray.init(local_mode=True)\n",
    "ray.init(num_gpus=0)    # Has nicer output\n",
    "\n",
    "config = MADDPGConfig()\n",
    "\n",
    "# Register the custom environment\n",
    "register_env(\"multi_agent_simulation\", lambda config: MultiAgentSimulationEnv(config))\n",
    "config.environment(\"multi_agent_simulation\")\n",
    "\n",
    "# Disable automatic environment checking\n",
    "config.environment(disable_env_checking=True)\n",
    "\n",
    "# Test if the environment is valid\n",
    "check_env(env, config)\n",
    "\n",
    "policies = get_shared_policy()\n",
    "# policies = get_individual_policies()      # Uncomment this line to use individual policies\n",
    "\n",
    "config.update_from_dict({\n",
    "    \"simple_optimizer\": True,\n",
    "    \"policies\": policies,\n",
    "    \"policy_mapping_fn\": policy_mapping_fn,\n",
    "    \"policies_to_train\": list(policies.keys()),\n",
    "    \"count_steps_by\": \"env_steps\",\n",
    "    \"framework\": \"tf\",\n",
    "    \"eager_tracing\": False,\n",
    "    \"observation_space\": env.observation_space,\n",
    "    \"action_space\": env.action_space,\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# Print the configuration (contains all the hyperparameters)\n",
    "for key, value in config.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-12 13:05:44,564\tINFO tune.py:657 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2023-08-12 13:05:44,599\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/gymnasium/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "2023-08-12 13:05:44,622\tWARNING algorithm_config.py:2558 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-08-12 13:05:49</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:04.47        </td></tr>\n",
       "<tr><td>Memory:      </td><td>6.2/15.5 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/32 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                               </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                        </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>MADDPG_multi_agent_simulation_2f001_00000</td><td style=\"text-align: right;\">           1</td><td>/mnt/c/GitHub/bachelor_thesis23/notebooks/results/MADDPG/MADDPG_multi_agent_simulation_2f001_00000_0_2023-08-12_13-05-44/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                               </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>MADDPG_multi_agent_simulation_2f001_00000</td><td>ERROR   </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m bash: /home/renlephy/miniconda3/envs/bachelor/lib/libtinfo.so.6: no version information available (required by bash)\n",
      "\u001b[2m\u001b[36m(pid=27681)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=27681)\u001b[0m 2023-08-12 13:05:46,689\tWARNING deprecation.py:50 -- DeprecationWarning: `build_tf_policy` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=27681)\u001b[0m 2023-08-12 13:05:46,692\tWARNING deprecation.py:50 -- DeprecationWarning: `build_policy_class` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m 2023-08-12 13:05:46,697\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/maddpg/` has been deprecated. Use `rllib_contrib/maddpg/` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m 2023-08-12 13:05:46,697\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/simple_q/` has been deprecated. Use `rllib_contrib/simple_q/` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m 2023-08-12 13:05:46,697\tWARNING algorithm_config.py:656 -- Cannot create MADDPGConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m bash: /home/renlephy/miniconda3/envs/bachelor/lib/libtinfo.so.6: no version information available (required by bash)\n",
      "2023-08-12 13:05:49,100\tERROR tune_controller.py:911 -- Trial task failed for trial MADDPG_multi_agent_simulation_2f001_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/_private/worker.py\", line 2522, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::MADDPG.__init__()\u001b[39m (pid=27681, ip=172.20.85.17, actor_id=c5aecae19ce9ee143c6fc7ee01000000, repr=MADDPG)\n",
      "  File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 227, in _setup\n",
      "    self.add_workers(\n",
      "  File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 593, in add_workers\n",
      "    raise result.get()\n",
      "  File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 481, in __fetch_result\n",
      "    result = ray.get(r)\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=27739, ip=172.20.85.17, actor_id=d1cd889a3c07b29f5495d77801000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fb5a5254970>)\n",
      "  File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 525, in __init__\n",
      "    self._update_policy_map(policy_dict=self.policy_dict)\n",
      "  File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1727, in _update_policy_map\n",
      "    self._build_policy_map(\n",
      "  File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1838, in _build_policy_map\n",
      "    new_policy = create_policy_for_framework(\n",
      "  File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 133, in create_policy_for_framework\n",
      "    return policy_class(\n",
      "  File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/algorithms/maddpg/maddpg_tf_policy.py\", line 54, in __init__\n",
      "    raise ValueError(\"Must set `agent_id` in the policy config.\")\n",
      "ValueError: Must set `agent_id` in the policy config.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::MADDPG.__init__()\u001b[39m (pid=27681, ip=172.20.85.17, actor_id=c5aecae19ce9ee143c6fc7ee01000000, repr=MADDPG)\n",
      "  File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/utils/deprecation.py\", line 106, in patched_init\n",
      "    return obj_init(*args, **kwargs)\n",
      "  File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/utils/deprecation.py\", line 106, in patched_init\n",
      "    return obj_init(*args, **kwargs)\n",
      "  File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 517, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 169, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 639, in setup\n",
      "    self.workers = WorkerSet(\n",
      "  File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 179, in __init__\n",
      "    raise e.args[0].args[2]\n",
      "ValueError: Must set `agent_id` in the policy config.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>MADDPG_multi_agent_simulation_2f001_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m 2023-08-12 13:05:49,090\tERROR actor_manager.py:500 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=27739, ip=172.20.85.17, actor_id=d1cd889a3c07b29f5495d77801000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fb5a5254970>)\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m   File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 525, in __init__\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m     self._update_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m   File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1727, in _update_policy_map\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m   File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1838, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m   File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 133, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m     return policy_class(\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m   File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/algorithms/maddpg/maddpg_tf_policy.py\", line 54, in __init__\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m     raise ValueError(\"Must set `agent_id` in the policy config.\")\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m ValueError: Must set `agent_id` in the policy config.\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::MADDPG.__init__()\u001b[39m (pid=27681, ip=172.20.85.17, actor_id=c5aecae19ce9ee143c6fc7ee01000000, repr=MADDPG)\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m   File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 227, in _setup\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m     self.add_workers(\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m   File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 593, in add_workers\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m     raise result.get()\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m   File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 481, in __fetch_result\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m     result = ray.get(r)\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=27739, ip=172.20.85.17, actor_id=d1cd889a3c07b29f5495d77801000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fb5a5254970>)\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m   File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 525, in __init__\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m     self._update_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m   File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1727, in _update_policy_map\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m   File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1838, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m   File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 133, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m     return policy_class(\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m   File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/algorithms/maddpg/maddpg_tf_policy.py\", line 54, in __init__\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m     raise ValueError(\"Must set `agent_id` in the policy config.\")\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m ValueError: Must set `agent_id` in the policy config.\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m \n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m \n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m \u001b[36mray::MADDPG.__init__()\u001b[39m (pid=27681, ip=172.20.85.17, actor_id=c5aecae19ce9ee143c6fc7ee01000000, repr=MADDPG)\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m   File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/utils/deprecation.py\", line 106, in patched_init\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m     return obj_init(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m   File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/utils/deprecation.py\", line 106, in patched_init\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m     return obj_init(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m   File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 517, in __init__\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m   File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 169, in __init__\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m   File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 639, in setup\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m   File \"/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 179, in __init__\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m     raise e.args[0].args[2]\n",
      "\u001b[2m\u001b[36m(MADDPG pid=27681)\u001b[0m ValueError: Must set `agent_id` in the policy config.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27739)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=27739, ip=172.20.85.17, actor_id=d1cd889a3c07b29f5495d77801000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fb5a5254970>)\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [MADDPG_multi_agent_simulation_2f001_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m stop \u001b[39m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtraining_iteration\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m100\u001b[39m,\n\u001b[1;32m      3\u001b[0m }\n\u001b[0;32m----> 5\u001b[0m results \u001b[39m=\u001b[39m tune\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m      6\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mMADDPG\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m      8\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m      9\u001b[0m     checkpoint_at_end\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     10\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m     checkpoint_freq\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m     storage_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mresults\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mMADDPG\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/bachelor/lib/python3.10/site-packages/ray/tune/tune.py:1142\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, chdir_to_trial_dir, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, checkpoint_keep_all_ranks, checkpoint_upload_from_workers, trial_executor, local_dir, _experiment_checkpoint_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m incomplete_trials:\n\u001b[1;32m   1141\u001b[0m     \u001b[39mif\u001b[39;00m raise_on_failed_trial \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m experiment_interrupted_event\u001b[39m.\u001b[39mis_set():\n\u001b[0;32m-> 1142\u001b[0m         \u001b[39mraise\u001b[39;00m TuneError(\u001b[39m\"\u001b[39m\u001b[39mTrials did not complete\u001b[39m\u001b[39m\"\u001b[39m, incomplete_trials)\n\u001b[1;32m   1143\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1144\u001b[0m         logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mTrials did not complete: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, incomplete_trials)\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [MADDPG_multi_agent_simulation_2f001_00000])"
     ]
    }
   ],
   "source": [
    "stop = {\n",
    "    \"training_iteration\": 100,\n",
    "}\n",
    "\n",
    "results = tune.run(\n",
    "    \"MADDPG\",\n",
    "    stop=stop,\n",
    "    config=config,\n",
    "    checkpoint_at_end=True,\n",
    "    verbose=2,\n",
    "    checkpoint_freq=10,\n",
    "    storage_path=\"results\",\n",
    "    name=\"MADDPG\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
