{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 12:11:27.425738: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-10 12:11:27.450360: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-10 12:11:27.867025: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    }
   ],
   "source": [
    "# Basics\n",
    "from Solver import Particle, Perceptron, PerceptronModel, VicsekModel, NeuralNetwork, PerceptronMode, Mode, NeuralSwarmModel\n",
    "\n",
    "import tensorflow   as tf\n",
    "import numpy        as np\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Allow for parallelization in both, tensorflow and the C++ backend (OMP)\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# Simulation Environment\n",
    "from tf_agents.trajectories     import time_step as ts\n",
    "from tf_agents.specs            import array_spec\n",
    "from tf_agents.environments     import py_environment\n",
    "\n",
    "# Evaluation Environment\n",
    "from tf_agents.networks         import network\n",
    "from tf_agents.train.utils      import spec_utils\n",
    "from tf_agents.environments     import tf_py_environment\n",
    "\n",
    "# Environment Testing\n",
    "from tf_agents.environments     import utils\n",
    "\n",
    "# Critic Network\n",
    "from tf_agents.agents.ddpg      import critic_network\n",
    "\n",
    "# DDPG Agent\n",
    "from tf_agents.agents.ddpg      import ddpg_agent\n",
    "from tf_agents.train.utils      import train_utils\n",
    "\n",
    "# Replay Buffer\n",
    "import reverb\n",
    "from tf_agents.replay_buffers   import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers   import reverb_utils\n",
    "\n",
    "# Policies\n",
    "from tf_agents.policies         import py_tf_eager_policy\n",
    "from tf_agents.policies         import random_py_policy\n",
    "\n",
    "# Actors\n",
    "from tf_agents.train            import actor\n",
    "from tf_agents.metrics          import py_metrics\n",
    "from tf_agents.train            import learner\n",
    "\n",
    "import tempfile\n",
    "\n",
    "# Learner\n",
    "from tf_agents.train            import triggers\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot        as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation settings\n",
    "settings = {\n",
    "        #                  N,      L,      v,      noise,  r\n",
    "        \"small\": [         100,    10,     0.03,   0.1,    1]\n",
    "    }\n",
    "    \n",
    "# Choose between RADIUS, FIXED, FIXEDRADIUS (don't use RADIUS)\n",
    "mode = Mode.FIXEDRADIUS\n",
    "# Flags\n",
    "ZDimension = False     # 2D or 3D\n",
    "seed = False           # Random seed\n",
    "# Choose settings\n",
    "chosen_settings = settings[\"small\"]\n",
    "N       = chosen_settings[0]\n",
    "L       = chosen_settings[1]\n",
    "v       = chosen_settings[2]\n",
    "noise   = chosen_settings[3]\n",
    "r       = chosen_settings[4]\n",
    "\n",
    "k_neighbors = 5\n",
    "# Timesteps in an episode\n",
    "T = 1001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "There are a lot of hyperparameters that can be tuned in a neural network, especially in a framework that allows for a lot of flexibility like TensorFlow. In this notebook, some of the hyperparameters are explored that can be tuned in a neural network (or agent- and RL-based framework) and how they affect the performance of the network. \n",
    "\n",
    "**However**, not all parameters that can be interpreted as hyperparameters are essential. TensorFlow has a lot of default values for parameters that are not essential to the network. Consequently, to retain readability but also offer TensorFlow's flexibility, hyperparameters are separated in **two blocks**.\n",
    "\n",
    "1. Essential hyperparameters\n",
    "2. Non-essential hyperparameters (default values)\n",
    "\n",
    "### Essential hyperparameters\n",
    "Essential hyperparameters are parameters that are essential to the network. These parameters are the ones that are most likely to be tuned in order to improve the performance of the network. These parameters are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 12:11:28.839596: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-10 12:11:28.861666: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-10 12:11:28.861700: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-10 12:11:28.864220: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-10 12:11:28.864252: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-10 12:11:28.864265: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-10 12:11:29.256535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-10 12:11:29.256575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-10 12:11:29.256579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-08-10 12:11:29.256594: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-10 12:11:29.256615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9330 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# Critic Network\n",
    "observation_fc_layer_params     = (20,20)                       # @param {type:\"tuple\"}         - NO DEFAULT\n",
    "action_fc_layer_params          = (20,)                         # @param {type:\"tuple\"}         - NO DEFAULT\n",
    "joint_fc_layer_params           = (50,)                         # @param {type:\"tuple\"}         - NO DEFAULT\n",
    "activation_fn                   = \"relu\"                        # @param {type:\"string\"}        - Default: \"relu\"\n",
    "kernel_initializer              = \"glorot_uniform\"              # @param {type:\"string\"}        - Default: \"glorot_uniform\"\n",
    "last_kernel_initializer         = \"glorot_uniform\"              # @param {type:\"string\"}        - Default: \"glorot_uniform\"\n",
    "\n",
    "# DDPG Agent\n",
    "actor_learning_rate             = 0.01                          # @param {type:\"float\"}         - Default: 0.001\n",
    "actor_optimizer                 = tf.keras.optimizers.Adam(\n",
    "    learning_rate=actor_learning_rate)                          #                               - NO DEFAULT\n",
    "critic_learning_rate            = 0.01                          # @param {type:\"float\"}         - Default: 0.001\n",
    "critic_optimizer                = tf.keras.optimizers.Adam(\n",
    "    learning_rate=critic_learning_rate)                         #                               - NO DEFAULT\n",
    "target_update_tau               = 1.0                           # @param {type:\"float\"}         - Default: 1.0\n",
    "target_update_period            = 1                             # @param {type:\"int\"}           - Default: 1\n",
    "td_errors_loss_fn = tf.math.squared_difference                  #                               - NO DEFAULT\n",
    "gamma                           = 1.0                           # @param {type:\"float\"}         - Default: 1.0\n",
    "reward_scale_factor             = 1.0                           # @param {type:\"float\"}         - Default: 1.0\n",
    "\n",
    "# Replay Buffer\n",
    "# Table\n",
    "replay_buffer_capacity          = 10000000                      # @param {type:\"integer\"}       - NO DEFAULT\n",
    "sampler                         = reverb.selectors.Uniform()    #                               - NO DEFAULT\n",
    "remover                         = reverb.selectors.Fifo()       #                               - NO DEFAULT\n",
    "# Rate Limiter\n",
    "samples_per_insert              = 1.0                           # @param {type:\"float\"}         - NO DEFAULT\n",
    "min_size_to_sample              = 1                             # @param {type:\"integer\"}       - NO DEFAULT\n",
    "error_buffer                    = samples_per_insert            # {float | Tuple[float, float]} - NO DEFAULT\n",
    "# Saving and Sampling\n",
    "sequence_length                 = 2                             # @param {type:\"integer\"}       - NO DEFAULT\n",
    "batch_size                      = 1                             # @param {type:\"integer\"}       - NO DEFAULT\n",
    "num_steps                       = 2                             # @param {type:\"integer\"}       - NO DEFAULT\n",
    "num_prefetch                    = 1                             # @param {type:\"integer\"}       - NO DEFAULT\n",
    "\n",
    "# Observer\n",
    "observer_sequence_length        = 2                             # @param {type:\"integer\"}       - NO DEFAULT\n",
    "observer_stride_length          = 1                             # @param {type:\"integer\"}       - Default: 1\n",
    "\n",
    "# Actors\n",
    "initial_collect_steps           = 100                           # @param {type:\"integer\"}       - NO DEFAULT\n",
    "collector_steps_per_run         = 1                             # @param {type:\"integer\"}       - NO DEFAULT\n",
    "eval_episodes_per_run           = 1                             # @param {type:\"integer\"}       - NO DEFAULT\n",
    "\n",
    "# Learning Trigger\n",
    "policy_save_interval            = 10                            # @param {type:\"integer\"}       - NO DEFAULT\n",
    "\n",
    "# Training\n",
    "num_iterations                  = 1000                          # @param {type:\"integer\"}       - NO DEFAULT\n",
    "eval_interval                   = 10                            # @param {type:\"integer\"}       - NO DEFAULT\n",
    "log_interval                    = 10                            # @param {type:\"integer\"}       - NO DEFAULT\n",
    "iterations_per_call             = 1                             # @param {type:\"integer\"}       - Default: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-essential hyperparameters (default values)\n",
    "These hyperparameters are not required to be set, but can be tuned to improve performance or test different configurations.\n",
    "### (to be done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom environment\n",
    "class SimulationEnvironment(py_environment.PyEnvironment):\n",
    "    \"\"\"Interface for a swarm simulation environment.\n",
    "    \n",
    "    Can be converted into a TensorFlow environment.\n",
    "    \n",
    "    Provides uniform access to the simulation and hosts the reward function.\n",
    "    \"\"\"\n",
    "    minimum = 0.0\n",
    "    maximum = 2*np.pi\n",
    "    \n",
    "    def __init__(self):\n",
    "        # The action is the angle of the particle\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(N, ), dtype=np.float32, minimum=self.minimum, maximum=self.maximum, name='action')\n",
    "        \n",
    "        \n",
    "        # k_neighbors + 1 because the particle itself is also included\n",
    "        # [ ] Change the observation to a relative angle (this reduces the dimensionality of the observation space by 1)\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(N, k_neighbors + 1), dtype=np.float32, minimum=self.minimum, maximum=self.maximum, name='observation')\n",
    "        \n",
    "        # Flags and variables\n",
    "        self._episode_ended = False\n",
    "        self.index = 0\n",
    "        self.simulation = NeuralSwarmModel(N, L, v, noise, r, mode, k_neighbors, ZDimension, seed=seed)\n",
    "        self._state = self.simulation.mean_direction2D()\n",
    "        \n",
    "        observation = self.simulation.get_all_angles()\n",
    "        self._current_time_step = ts.restart(np.array(observation, dtype=np.float32))\n",
    "        \n",
    "\n",
    "    def observation_spec(self):\n",
    "        \"\"\"Return observation_spec.\"\"\"\n",
    "        # DONE\n",
    "        return self._observation_spec\n",
    "\n",
    "    def action_spec(self):\n",
    "        \"\"\"Return action_spec.\"\"\"\n",
    "        # DONE\n",
    "        return self._action_spec\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Return initial_time_step and reset the simulation.\n",
    "        \n",
    "        Note that this is a hard reset and not a reset for the current epoch.\"\"\"\n",
    "        \n",
    "        # DONE\n",
    "        self._current_time_step = self._reset()\n",
    "        return self._current_time_step\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Apply action and return new time_step.\"\"\"\n",
    "        # DONE\n",
    "        if self._current_time_step is None:\n",
    "            return self.reset()\n",
    "        self._current_time_step = self._step(action)\n",
    "        return self._current_time_step\n",
    "\n",
    "    def current_time_step(self):\n",
    "        # DONE\n",
    "        return self._current_time_step\n",
    "\n",
    "    # def time_step_spec(self):\n",
    "        \"\"\"Return time_step_spec.\"\"\"\n",
    "        # DONE\n",
    "        # return ts.time_step_spec(self.observation_spec())\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Return initial_time_step and reset the simulation.\n",
    "        \n",
    "        Note that this is a hard reset.\"\"\"\n",
    "        # Reset simulation\n",
    "        self.simulation = NeuralSwarmModel(N, L, v, noise, r, mode, k_neighbors, ZDimension, seed=False)\n",
    "        self._state = self.simulation.mean_direction2D()\n",
    "        self._episode_ended = False\n",
    "        self.index = 0\n",
    "        observation = self.simulation.get_all_angles()\n",
    "        \n",
    "        logger.info(f'Reset with observation = {observation}')\n",
    "        return ts.restart(np.array(observation, dtype=np.float32))\n",
    "    \n",
    "    def _step(self, action):\n",
    "        \"\"\"Apply actions and return new time_step.\n",
    "        This method hosts the reward function.\"\"\"\n",
    "\n",
    "        # logger.info(f'Step with action = {action}')\n",
    "        # Check action boundaries\n",
    "        action = np.clip(action, self.minimum, self.maximum)\n",
    "        \n",
    "        oldState = self._state\n",
    "        \n",
    "        self.simulation.update_angles(action)\n",
    "        \n",
    "        self._state = self.simulation.mean_direction2D()\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = self._state - oldState\n",
    "        \n",
    "        # Create the observation\n",
    "        observation = self.simulation.get_all_angles()\n",
    "        observation = np.array(observation, dtype=np.float32)\n",
    "        observation = np.where(observation < 0.0, 0.0, observation)  # Make sure observation is positive\n",
    "        \n",
    "        self.index += 1\n",
    "\n",
    "        # Check validity of observation\n",
    "        if np.isnan(observation).any():\n",
    "            raise ValueError(f'The observation contains nan. Observation = {observation}.')\n",
    "        elif np.any(observation < self.minimum) or np.any(observation > self.maximum):\n",
    "            raise ValueError(f'The observation is not in the correct range. Observation = {observation}.')\n",
    "\n",
    "        # Check if we've completed a predefined number of steps\n",
    "        if self.index >= T:\n",
    "            self._episode_ended = True\n",
    "            self.index = 0\n",
    "\n",
    "            logger.info(f'Episode ended with reward = {reward}')\n",
    "            return ts.termination(observation, reward)\n",
    "\n",
    "        logger.info(f'Step ended with reward = {reward}')\n",
    "        return ts.transition(observation, reward=reward, discount=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Environment Flowchart Functions](illustrations/environment_flowchart_functions_transparent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies\n",
    "\n",
    "With this option, the training can be scaled to multiple GPUs or TPUs, or multiple machines.\n",
    "\n",
    "For this to work, two steps are necessary:\n",
    "\n",
    "- The strategy has to be initialised.\n",
    "- The model must be defined in a strategy scope.\n",
    "\n",
    "In this project this option is not used, but it is included for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom tf_agents.train.utils import strategy_utils\\n\\n# Distribution strategy\\n# For now, don't use GPU or TPU\\n\\nstrategy = strategy_utils.get_strategy(tpu=False, use_gpu=False)\\n\\n# All variables and Agents need to be created under strategy.scope()\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from tf_agents.train.utils import strategy_utils\n",
    "\n",
    "# Distribution strategy\n",
    "# For now, don't use GPU or TPU\n",
    "\n",
    "strategy = strategy_utils.get_strategy(tpu=False, use_gpu=False)\n",
    "\n",
    "# All variables and Agents need to be created under strategy.scope()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Wrapping\n",
    "\n",
    "To ensure compatibility with the TensorFlow Agents (TF-Agents) framework, the code involves a necessary process of environment wrapping.\n",
    "\n",
    "There are two distinct environments established, each tailored for different aspects of the training process:\n",
    "\n",
    "## Training Environment (`collect_env`)\n",
    "\n",
    "- **Purpose**: Primarily employed for training the agent.\n",
    "- **Data Collection**: Retrieves interactions from the environment and stores them in a replay buffer, which serves as a repository of experience to train the agent.\n",
    "- **Agent Updates**: Utilizes the critic network to compute the loss and refine the agent.\n",
    "- **Characteristics**:\n",
    "  - **Exploration**: Incorporates various strategies for data acquisition. It often integrates components such as noise to encourage the agent to explore novel actions.\n",
    "  - **Action Handling**: May use action clipping to ensure the agent's actions are within permissible bounds.\n",
    "\n",
    "## Evaluation Environment (`eval_env`)\n",
    "\n",
    "- **Purpose**: Specifically designed for evaluating the agent's performance.\n",
    "- **Performance Metrics**: Monitors how well the agent performs, ensuring an unbiased assessment by eliminating training-related components (like noise or exploration).\n",
    "- **Characteristics**:\n",
    "  - Operates without the aforementioned noise or exploration components to provide a clear view of the agent's capabilities in a given task.\n",
    "\n",
    "Both environments undergo a wrapping process to make them seamlessly integrate with TensorFlow operations, thereby streamlining training and evaluation processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two networks and two environments: one for training and one for evaluation.\n",
    "collect_env = SimulationEnvironment()\n",
    "eval_env = SimulationEnvironment()\n",
    "\n",
    "# Wrap the environment in a TF environment.\n",
    "tf_collect_env = tf_py_environment.TFPyEnvironment(collect_env)\n",
    "tf_eval_env = tf_py_environment.TFPyEnvironment(eval_env)\n",
    "\n",
    "# For the network to work with the environment, the specs have to be known.\n",
    "observation_spec, action_spec, time_step_spec = (spec_utils.get_tensor_specs(tf_collect_env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the python environments\n",
      "Testing the collect environment... Done.\n",
      "Testing the eval environment... Done.\n"
     ]
    }
   ],
   "source": [
    "# Test the python environments\n",
    "\n",
    "print(\"Testing the python environments\")\n",
    "print(\"Testing the collect environment... \", end=\"\")\n",
    "utils.validate_py_environment(collect_env, episodes=1)\n",
    "print(\"Done.\")\n",
    "print(\"Testing the eval environment... \", end=\"\")\n",
    "utils.validate_py_environment(eval_env, episodes=1)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YES! IT WORKS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Networks\n",
    "\n",
    "## Overview\n",
    "\n",
    "DDPG (Deep Deterministic Policy Gradients) is a model-free, online, off-policy reinforcement learning method. The method uses a policy network (the Actor) to select actions and a value network (the Critic) to evaluate them. The given code provides the implementation of these two networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a network that can learn to predict the action given an observation.\n",
    "# This is a simple (on demand fully connected) network that takes in an observation and outputs an action.\n",
    "\n",
    "singleObservationSpec = array_spec.BoundedArraySpec(\n",
    "            shape=(k_neighbors + 1,), dtype=np.float32, minimum=0.0, maximum=2*np.pi, name='observation')\n",
    "singleActionSpec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.float32, minimum=0.0, maximum=2*np.pi, name='action')\n",
    "\n",
    "class ActorNet(network.Network):\n",
    "\n",
    "    def __init__(self, input_tensor_spec, output_tensor_spec, k=5, name='ActorNet'):\n",
    "        super(ActorNet, self).__init__(input_tensor_spec=input_tensor_spec, state_spec=(), name=name)\n",
    "\n",
    "        self._output_tensor_spec = output_tensor_spec\n",
    "        self._k = k_neighbors + 1\n",
    "\n",
    "        self._flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "        self._neurons = []\n",
    "        for _ in range(self._output_tensor_spec.shape.num_elements()):\n",
    "            self._neurons.append(tf.keras.layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "        logger.info(\"Number of output neurons: %d\", output_tensor_spec.shape.num_elements())\n",
    "        logger.info(\"Number of input neurons: %d\", input_tensor_spec.shape.num_elements())\n",
    "\n",
    "    def call(self, observations, step_type=(), network_state=()):\n",
    "        del step_type  # unused.\n",
    "\n",
    "        logger.info(f\"ActorNet - call, observations\\n {observations}\")\n",
    "\n",
    "        flat_observations = self._flatten(observations)\n",
    "\n",
    "        outputs = []\n",
    "        for i, neuron in enumerate(self._neurons):\n",
    "            start_idx = i * (self._k + 1)\n",
    "            end_idx = start_idx + (self._k + 1)\n",
    "            local_input = tf.keras.layers.Lambda(lambda x: x[:, start_idx:end_idx])(flat_observations)\n",
    "            outputs.append(neuron(local_input))\n",
    "\n",
    "        concatenated_outputs = tf.keras.layers.Concatenate()(outputs)\n",
    "\n",
    "        logger.info(f\"ActorNet - call, transformed outputs\\n {concatenated_outputs}\")\n",
    "\n",
    "        newShape = [-1] + self._output_tensor_spec.shape.as_list()\n",
    "        logger.info(f\"ActorNet - call, newShape: {newShape}\")\n",
    "\n",
    "        actions = tf.reshape(concatenated_outputs, newShape)\n",
    "\n",
    "        logger.info(f\"ActorNet - call, actions\\n {actions}\")\n",
    "\n",
    "        return actions, network_state\n",
    "\n",
    "\n",
    "# Create the Actor Network\n",
    "# CRITICAL: The neural network handels all particles at once, but its architecture is designed for a single particle.\n",
    "# So we DON'T use the observation_spec and action_spec from the environment, but the specs for a single particle.\n",
    "actor_net = ActorNet(\n",
    "    input_tensor_spec=observation_spec,\n",
    "    output_tensor_spec=action_spec)\n",
    "\n",
    "\n",
    "# Critic Network\n",
    "critic_net = critic_network.CriticNetwork(\n",
    "      (observation_spec, action_spec),\n",
    "      observation_fc_layer_params=observation_fc_layer_params,\n",
    "      action_fc_layer_params=action_fc_layer_params,\n",
    "      joint_fc_layer_params=joint_fc_layer_params,\n",
    "      activation_fn=activation_fn,\n",
    "      kernel_initializer=kernel_initializer,\n",
    "      last_kernel_initializer=last_kernel_initializer)\n",
    "\n",
    "# DONT USE AN ACTOR DISTRIBUTION NETWORK FOR DDPG AGENTS\n",
    "# They need a deterministic action output. The actor distribution network is for stochastic policies. (like PPO or SAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Explanation\n",
    "\n",
    "### 1. Dependencies and Tools\n",
    "\n",
    "```python\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "```\n",
    "\n",
    "- We import the ``critic_network`` module, a predefined architecture for the Critic in DDPG from TensorFlow Agents.\n",
    "\n",
    "### 2. Actor Network\n",
    "\n",
    "![Actor Network](illustrations/actor_network_transparent.png)\n",
    "\n",
    "The Actor network predicts the best possible action for a given state. Unlike other methods, in DDPG, this prediction is deterministic.\n",
    "\n",
    "### Structure\n",
    "```python\n",
    "class ActorNet(network.Network):\n",
    "```\n",
    "\n",
    "- **Purpose**: Predict the optimal action for a given state.\n",
    "- **Nature**: Deterministic. Always outputs the same action for the same state.\n",
    "- **Architecture**:\n",
    "    - Single fully connected layer (as of now).\n",
    "    - Uses a linear activation function.\n",
    "\n",
    "### Inner Workings\n",
    "\n",
    "- The network gets an observation (state) and processes it through its internal layers.\n",
    "- The output is reshaped to match the expected action's structure.\n",
    "- If necessary, the action values are adjusted (scaled/shifted) to fit within the valid action range.\n",
    "\n",
    "### 3. Critic Network\n",
    "![Critic Network](illustrations/critic_network_transparent.png)\n",
    "\n",
    "The Critic Network, in the context of DDPG, is used to estimate the Q-value of a state-action pair.\n",
    "\n",
    "### Creation\n",
    "```\n",
    "critic_net = critic_network.CriticNetwork(...)\n",
    "```\n",
    "\n",
    "- **Input**: The network takes two inputs â€“ an observation and an action.\n",
    "- **Output**: Outputs a Q-value, estimating the expected return of that action in that state.\n",
    "\n",
    "### Inner Workings\n",
    "\n",
    "- Both the observation and the action are separately processed through their own neural network layers.\n",
    "- The outputs from these separate branches are then combined.\n",
    "- The combined output goes through one or more additional layers to produce the final Q-value estimate.\n",
    "\n",
    "### Architectural Details\n",
    "- **Initialization**: The network weights are initialized using the 'glorot_uniform' method, suitable for most deep learning scenarios.\n",
    "- **Layers and Parameters**:\n",
    "    - Separate layers for processing observations and actions.\n",
    "    - Joint layers to process combined data.\n",
    "    - Activation functions and other layer parameters can be adjusted for fine-tuning.\n",
    "\n",
    "## Key Takeaway\n",
    "It's essential to ensure that the Actor network in DDPG outputs deterministic actions. It should not be confused with other methods where the policy network might output a distribution over actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters in the above code block\n",
    "(all of these are optional; default values suffice)\n",
    "- `observation_fc_layer_params`\n",
    "- `action_fc_layer_params`\n",
    "- `critic_joint_fc_layer_params`\n",
    "- `activation_fn`\n",
    "- `kernel_initializer`\n",
    "- `last_kernel_initializer`\n",
    "\n",
    "### Hyperparameters Explanation\n",
    "\n",
    "- `observation_fc_layer_params`: Specifies the architecture (e.g., number of layers and their respective sizes) for the fully connected layers that process the observation (or state) input in the critic network.\n",
    "\n",
    "- `action_fc_layer_params`: Dictates the architecture for the fully connected layers handling the action input in the critic network.\n",
    "\n",
    "- `joint_fc_layer_params`: After processing the observation and action separately, their outputs are concatenated. This parameter sets the architecture for the subsequent fully connected layers that process this combined data.\n",
    "\n",
    "- `kernel_initializer`: Determines the method for initializing the weights in the network layers. Proper initialization ensures effective training and convergence. The initializer affects all layers except the last one. ``'glorot_uniform'`` initializes weights randomly based on the number of input and output neurons. \n",
    "\n",
    "- `last_kernel_initializer`: Like the `kernel_initializer`, but specifically for the final layer of the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(time_step_spec)\n",
    "# print(action_spec)\n",
    "# print(observation_spec)\n",
    "# print(singleActionSpec)\n",
    "# print(singleObservationSpec)\n",
    "\n",
    "# singleTimeStepSpec = ts.time_step_spec(singleObservationSpec)\n",
    "\n",
    "# print(singleTimeStepSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 12:11:30.969520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the agent\n",
    "\n",
    "train_step = train_utils.create_train_step()\n",
    "\n",
    "tf_agent = ddpg_agent.DdpgAgent(\n",
    "    time_step_spec,\n",
    "    action_spec,\n",
    "    actor_network=actor_net,\n",
    "    critic_network=critic_net,\n",
    "    actor_optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=actor_learning_rate),\n",
    "    critic_optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=critic_learning_rate),\n",
    "    target_update_tau=target_update_tau,\n",
    "    target_update_period=target_update_period,\n",
    "    td_errors_loss_fn=tf.math.squared_difference,\n",
    "    gamma=gamma,\n",
    "    reward_scale_factor=reward_scale_factor,\n",
    "    train_step_counter=train_step)\n",
    "\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of the right Agent\n",
    "\n",
    "For this project, the DDPG algorithm was chosen. The main reason for this choice is that the DDPG algorithm is a good fit for continuous action spaces.\n",
    "\n",
    "Other algorithms (for continuous action spaces) might be suitable as well. As a reference, the following figure provides an overview of the most common algorithms in reinforcement learning.\n",
    "\n",
    "![RL Algorithms](illustrations/agents_overview_transparent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation:\n",
    "\n",
    "1. **Imports**:\n",
    "\n",
    "```python\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.train.utils import train_utils\n",
    "```\n",
    "- These lines import necessary modules from TensorFlow's reinforcement learning library, TF-Agents.\n",
    "\n",
    "1. **Training Step Initialization**:\n",
    "```\n",
    "train_step = train_utils.create_train_step()\n",
    "```\n",
    "  - A training step counter is created. This will keep track of how many training iterations (or steps) the agent has undergone. It's especially useful for logging, debugging, and scheduling (e.g., learning rate decay based on steps).\n",
    "\n",
    "1. **Initializing the DDPG Agent**:\n",
    "\n",
    "```\n",
    "tf_agent = ddpg_agent.DdpgAgent(...)\n",
    "```\n",
    "  - This section initializes the DDPG (Deep Deterministic Policy Gradients) agent. The DDPG agent is an RL agent suitable for continuous action spaces.\n",
    "  - Parameters:\n",
    "    - `time_step_spec` & `action_spec`: Specifications ensuring compatibility between the agent's components and the environment.\n",
    "    - `actor_network` & `critic_network`: Pre-defined actor and critic networks.\n",
    "    - `actor_optimizer` & `critic_optimizer`: Optimization algorithms (Adam) for training the actor and critic networks. \n",
    "    - `target_update_tau` & `target_update_period`: Parameters controlling the soft update of target networks.\n",
    "    - `td_errors_loss_fn`: The temporal difference (TD) error loss function. This function measures the difference between the estimated future rewards and the observed reward, using the squared difference.\n",
    "    - `gamma`: Discount factor for future rewards.\n",
    "    - `reward_scale_factor`: A factor to scale the magnitude of rewards.\n",
    "    - `train_step_counter`: Earlier initialized training step counter.\n",
    "\n",
    "1. **Agent Initialization**:\n",
    "\n",
    "`tf_agent.initialize()`\n",
    "  - This line initializes the agent, preparing it for the training process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters in the above code block\n",
    "- `actor_optimizer`\n",
    "  - `actor_learning_rate`\n",
    "- `critic_optimizer`\n",
    "  - `critic_learning_rate`\n",
    "- `target_update_tau`\n",
    "- `target_update_period`\n",
    "- `td_errors_loss_fn`\n",
    "- `gamma`\n",
    "- `reward_scale_factor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Loss in Reinforcement Learning\n",
    "In supervised learning, \"loss\" directly measures the difference between the predicted value and the actual value (e.g., how far off a predicted house price is from the actual price). In reinforcement learning, things are more indirect. The agent receives \"rewards\" from the environment based on actions it takes. Over time, it learns to maximize cumulative rewards.\n",
    "\n",
    "However, to \"learn,\" we typically use neural networks (or other function approximators) that need a scalar \"loss\" to perform optimization (i.e., adjust weights). The \"loss\" in RL is derived from the difference between expected rewards (Q-values from our networks) and the actual received rewards plus expected future rewards. In DDPG and similar algorithms, this is represented as the Temporal Difference (TD) error.\n",
    "\n",
    "This \"loss\" is then backpropagated through the network to adjust the weights, allowing the agent to make better action choices in the future.\n",
    "\n",
    "Overall, while the principles between supervised learning and RL are similar, in RL, the challenge and novelty lie in the fact that the \"correct answer\" (or immediate reward) might lead to sub-optimal long-term outcomes. Thus, the agent must learn to balance immediate and future rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/platform/tfrecord_checkpointer.cc:162]  Initializing TFRecordCheckpointer in /tmp/tmpl2jwq0hc.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:567] Loading latest checkpoint from /tmp/tmpl2jwq0hc\n",
      "[reverb/cc/platform/default/server.cc:71] Started replay server on port 42057\n"
     ]
    }
   ],
   "source": [
    "# Use Reverb, a framework for experience replay developed by DeepMind, to store and sample experience tuples for training.\n",
    "# Using a samples_per_insert somewhere between 2 and 1000. This is a trade-off between the number of samples that can be drawn from the replay buffer and the number of times the replay buffer needs to be updated.\n",
    "rate_limiter=reverb.rate_limiters.SampleToInsertRatio(samples_per_insert=samples_per_insert, min_size_to_sample=min_size_to_sample, error_buffer=error_buffer)\n",
    "\n",
    "table_name = 'uniform_table'\n",
    "\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=replay_buffer_capacity,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=rate_limiter)\n",
    "\n",
    "reverb_server = reverb.Server([table])\n",
    "\n",
    "# Since the agent needs N steps of experience to make an update, the dataset will need to sample batches of N steps + 1 to allow the agent to learn from a complete transition.\n",
    "\n",
    "reverb_replay = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    tf_agent.collect_data_spec,\n",
    "    sequence_length=sequence_length,\n",
    "    table_name=table_name,\n",
    "    local_server=reverb_server)\n",
    "\n",
    "# A dataset is created from the replay buffer to be fed to the agent for training. \n",
    "dataset = reverb_replay.as_dataset(\n",
    "      sample_batch_size=batch_size, num_steps=num_steps).prefetch(num_prefetch)\n",
    "\n",
    "experience_dataset_fn = lambda: dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer with Reverb\n",
    "\n",
    "Experience replay is a critical component in many reinforcement learning algorithms, primarily Deep Q-Networks (DQN) and its variants (like DDPG). It helps break the correlation between subsequent state-action-reward transitions, providing a more stable and smoother learning experience. Additionally, it enables the reuse of past experiences, increasing the data efficiency.\n",
    "\n",
    "In the code provided, DeepMind's **Reverb** is employed for experience replay. Reverb is an efficient and easy-to-use system that can be used as a storage for experience. \n",
    "\n",
    "\n",
    "**Reverb Rate Limiter**: `rate_limiter=reverb.rate_limiters.SampleToInsertRatio(...)`\n",
    "- This part of the code controls the rate at which items can be added to the buffer compared to the rate at which items are sampled. It's a mechanism to ensure the replay buffer remains diverse and doesn't get flooded with too many similar experiences in quick succession.\n",
    "- The parameter `samples_per_insert` determines how many samples, on average, will be drawn from the replay buffer for each item that's inserted.\n",
    "\n",
    "**Table Configuration**: \n",
    "- `table_name` simply gives a name to the Reverb table being created.\n",
    "- The `reverb.Table` instance represents where the experiences will be stored.\n",
    "  - `max_size` sets the capacity of the replay buffer.\n",
    "  - The `sampler` defines a strategy for sampling experiences from the buffer. Here, a `Uniform` sampler is used, meaning that all experiences have an equal chance of being sampled.\n",
    "  - The `remover` uses a FIFO (First-In-First-Out) strategy, meaning the oldest experiences will be removed first when the buffer reaches its capacity.\n",
    "  - `rate_limiter` ensures that the sampling and inserting rates adhere to the specified rules.\n",
    "\n",
    "**Reverb Server**: `reverb_server = reverb.Server([table])`\n",
    "  - Reverb functions through a client-server architecture. Here, a server instance is started with the defined table.\n",
    "\n",
    "**Reverb Replay Buffer**: `reverb_replay = reverb_replay_buffer.ReverbReplayBuffer(...)`\n",
    "- This integrates Reverb with the TF-Agents framework. It provides an interface for the agent to interact with the replay buffer.\n",
    "- `sequence_length` determines how many consecutive items will be returned when sampling from the replay buffer. \n",
    "\n",
    "**Dataset Creation**: `dataset = reverb_replay.as_dataset(...)`\n",
    "- This line translates the replay buffer into a TensorFlow dataset. This is crucial because it enables efficient batching and shuffling, and provides an easy-to-useformat to feed to the learning algorithm.\n",
    "- `sample_batch_size` determines how many items (based on `sequence_length`) will be returned in each ``batch``.\n",
    "- `num_steps` determines how many batches will be returned per sampling.\n",
    "- `prefetch(50)` improves training throughput by preparing 50 batches in advance to minimize latency.\n",
    "- The lambda function `experience_dataset_fn` is a simple utility to fetch this dataset, which can be used later for training iterations.\n",
    "\n",
    "\n",
    "In the context of reinforcement learning, using such a buffer allows the agent to learn from a more diverse set of experiences, rather than just the most recent ones, making training more robust and data-efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters in the above code block\n",
    "\n",
    "- `samples_per_insert`\n",
    "- `min_size_to_sample`\n",
    "- `error_buffer`\n",
    "- `max_size` = `replay_buffer_capacity`\n",
    "- `sampler` = `reverb.selectors.Uniform()`\n",
    "- `remover` = `reverb.selectors.Fifo()`\n",
    "- `rate_limiter` = `rate_limiter`\n",
    "- `sequence_length`\n",
    "- `sample_batch_size` = `batch_size`\n",
    "- `num_steps`\n",
    "- `Prefetch Value`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference between ``sequence_length`` and ``num_steps``\n",
    "\n",
    "Understanding the nuances between ``sequence_length`` and ``num_steps`` is essential when working with the Reverb replay system in the TF-Agents framework. These parameters control how data is stored and sampled, respectively.\n",
    "\n",
    "### 1. Data Storage with ``sequence_length``:\n",
    "  - When you set ``sequence_length=5``, the replay buffer will store experiences in chunks of 5 steps.\n",
    "  - Each stored sequence in the buffer will contain these 5 steps.\n",
    "\n",
    "### 2. Data Sampling with ``num_steps``:\n",
    "  - If you try to sample with ``num_steps=3`` from a buffer storing sequences of 5 steps, you can potentially retrieve any 3-step sub-sequence from within the 5-step stored sequences.\n",
    "  - This means you might get steps 1-3, 2-4, or 3-5, depending on the sampler strategy used.\n",
    "  - With a uniform sampler, each possible 3-step sub-sequence has an equal chance of being sampled.\n",
    "\n",
    "**Example Scenario**:\n",
    "\n",
    "Imagine storing experiences in 5-step sequences. If you then try to sample using ``num_steps=6``, you'll likely encounter an error since the buffer does not contain sequences of that length. Conversely, when sampling with ``num_steps=3``, you can extract any contiguous 3-step sequence from the stored 5-step experiences.\n",
    "\n",
    "It's crucial to set these parameters with clarity on their roles:\n",
    "\n",
    "- ``sequence_length`` dictates how experiences are organized in storage.\n",
    "- ``num_steps`` influences the way you sample from these stored experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policies\n",
    "# Create policies from the agent\n",
    "\n",
    "tf_eval_policy = tf_agent.policy\n",
    "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "  tf_eval_policy, use_tf_function=True)\n",
    "\n",
    "tf_collect_policy = tf_agent.collect_policy\n",
    "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "  tf_collect_policy, use_tf_function=True)\n",
    "\n",
    "# Random policy to sample from the environment\n",
    "random_policy = random_py_policy.RandomPyPolicy(\n",
    "  collect_env.time_step_spec(), collect_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policies Summary\n",
    "\n",
    "In the above code block, three distinct policies are created:\n",
    "\n",
    "### 1. **Evaluation Policy (`eval_policy`):**\n",
    "   - Extracted from the DDPG agent's internal evaluation policy (`tf_eval_policy`).\n",
    "   - This policy is wrapped using `PyTFEagerPolicy`, which facilitates policy execution in TensorFlow's eager mode. Eager mode allows for more intuitive imperative execution, improving debugging and making the code more readable.\n",
    "\n",
    "### 2. **Collection Policy (`collect_policy`):**\n",
    "   - Derived from the DDPG agent's internal collection policy (`tf_collect_policy`).\n",
    "   - Like the evaluation policy, this is also wrapped using `PyTFEagerPolicy`.\n",
    "   - Notably, the exploration-exploitation trade-off inherent in this policy is already predefined within the DDPG agent's implementation. The wrapping into an eager policy does not modify these exploration aspects.\n",
    "\n",
    "### 3. **Random Policy (`random_policy`):**\n",
    "   - An entirely random policy that samples actions based on the environment's action specification.\n",
    "   - Primarily used to initialize data collection, ensuring a diverse set of experiences at the start of training.\n",
    "\n",
    "It's essential to recognize that the `PyTFEagerPolicy` wrapper serves primarily to enable eager execution and does not dictate or alter the exploration strategy of the underlying policy. That behavior is ingrained in the DDPG agent's implementation itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/trajectory_writer.cc:655] The number of pending items is alarmingly high, did you forget to call Flush? 1 items are waiting to be sent and 49 items have been sent to the server but haven't been confirmed yet. It is important to call Flush regularly as large numbers of pending items can result in OOM crashes on both client and server.\n",
      "[reverb/cc/trajectory_writer.cc:655] The number of pending items is alarmingly high, did you forget to call Flush? 0 items are waiting to be sent and 59 items have been sent to the server but haven't been confirmed yet. It is important to call Flush regularly as large numbers of pending items can result in OOM crashes on both client and server.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial collector step 100 of 100                    \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/trajectory_writer.cc:655] The number of pending items is alarmingly high, did you forget to call Flush? 1 items are waiting to be sent and 69 items have been sent to the server but haven't been confirmed yet. It is important to call Flush regularly as large numbers of pending items can result in OOM crashes on both client and server.\n",
      "[reverb/cc/trajectory_writer.cc:655] The number of pending items is alarmingly high, did you forget to call Flush? 1 items are waiting to be sent and 79 items have been sent to the server but haven't been confirmed yet. It is important to call Flush regularly as large numbers of pending items can result in OOM crashes on both client and server.\n",
      "[reverb/cc/trajectory_writer.cc:655] The number of pending items is alarmingly high, did you forget to call Flush? 0 items are waiting to be sent and 89 items have been sent to the server but haven't been confirmed yet. It is important to call Flush regularly as large numbers of pending items can result in OOM crashes on both client and server.\n"
     ]
    }
   ],
   "source": [
    "tempdir = tempfile.gettempdir()\n",
    "\n",
    "# Custom observer that counts and logs the number of times it is called.\n",
    "class ProgressObserver:\n",
    "    def __init__(self, total_steps, log_every=1):\n",
    "        self.total_steps = total_steps\n",
    "        self.log_every = log_every\n",
    "        self.current_step = 0\n",
    "\n",
    "    def __call__(self, trajectory):\n",
    "        self.current_step += 1\n",
    "        if self.current_step % self.log_every == 0:\n",
    "            print(f\"Initial collector step {self.current_step} of {self.total_steps}                    \\r\", end=\"\")\n",
    "\n",
    "progress_observer = ProgressObserver(total_steps=initial_collect_steps)\n",
    "\n",
    "# As the Actors run data collection steps, they pass trajectories of (state, action, reward) to the observer, which caches and writes them to the Reverb replay system.\n",
    "\n",
    "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
    "  reverb_replay.py_client,\n",
    "  table_name,\n",
    "  sequence_length=observer_sequence_length,\n",
    "  stride_length=observer_stride_length)\n",
    "\n",
    "# We create an Actor with the random policy and collect experiences to seed the replay buffer with.\n",
    "initial_collect_actor = actor.Actor(\n",
    "  collect_env,\n",
    "  random_policy,\n",
    "  train_step,\n",
    "  steps_per_run=initial_collect_steps,\n",
    "  observers=[rb_observer, progress_observer])\n",
    "\n",
    "initial_collect_actor.run()\n",
    "\n",
    "# Instantiate an Actor with the collect policy to gather more experiences during training.\n",
    "env_step_metric = py_metrics.EnvironmentSteps()\n",
    "\n",
    "collect_actor = actor.Actor(\n",
    "  collect_env,\n",
    "  collect_policy,\n",
    "  train_step,\n",
    "  steps_per_run=collector_steps_per_run,\n",
    "  metrics=actor.collect_metrics(10),\n",
    "  summary_dir=os.path.join(tempdir, learner.TRAIN_DIR),\n",
    "  observers=[rb_observer, env_step_metric])\n",
    "\n",
    "# Create an Actor which will be used to evaluate the policy during training.\n",
    "# actor.eval_metrics(num_eval_episodes) to log metrics later.\n",
    "eval_actor = actor.Actor(\n",
    "  eval_env,\n",
    "  eval_policy,\n",
    "  train_step,\n",
    "  episodes_per_run=eval_episodes_per_run,\n",
    "  metrics=actor.eval_metrics(eval_episodes_per_run),\n",
    "  summary_dir=os.path.join(tempdir, 'eval'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actors & Observer Summary\n",
    "\n",
    "The provided code demonstrates the creation of actors and an observer in the context of the DDPG agent and Reverb replay system.\n",
    "\n",
    "### 1. **Temporary Directory (`tempdir`):**\n",
    "   - A directory path is acquired to temporarily store relevant information, such as metrics summaries.\n",
    "\n",
    "### 2. **Reverb Observer (`rb_observer`):**\n",
    "   - An instance of `ReverbAddTrajectoryObserver` is created.\n",
    "   - As actors interact with their environments, they generate trajectories, which are sequences of (state, action, reward).\n",
    "   - The observer's role is to cache these trajectories and write them into the Reverb replay system for future training.\n",
    "   - It's set up to observe trajectories with a specified `sequence_length` and `stride_length`.\n",
    "\n",
    "### 3. **Initial Collection Actor (`initial_collect_actor`):**\n",
    "   - This actor uses the `random_policy`.\n",
    "   - Its purpose is to interact with the environment and generate initial experiences. These are important for seeding the replay buffer, ensuring a diverse starting point for training.\n",
    "   - After creation, the actor immediately starts running, thereby populating the replay buffer.\n",
    "   - The generated trajectories are observed by the `rb_observer`.\n",
    "\n",
    "### 4. **Training Collection Actor (`collect_actor`):**\n",
    "   - Unlike the initial actor, this one employs the `collect_policy`.\n",
    "   - It's designed to gather more refined experiences during training, as the agent learns and improves.\n",
    "   - In addition to the `rb_observer`, this actor also updates an `env_step_metric`, which counts the environment steps taken. This can be useful for monitoring and diagnostics.\n",
    "   - The summary of metrics related to data collection is stored in a directory specified by `os.path.join(tempdir, learner.TRAIN_DIR)`.\n",
    "\n",
    "### 5. **Evaluation Actor (`eval_actor`):**\n",
    "   - This actor uses the `eval_policy` and interacts with the evaluation environment (`eval_env`).\n",
    "   - Its main purpose is to periodically assess the performance of the agent's policy during training.\n",
    "   - Metrics related to evaluation (like average return over episodes) are logged and can be analyzed to monitor the agent's progress.\n",
    "   - The summary of these evaluation metrics is stored in a directory specified by `os.path.join(tempdir, 'eval')`.\n",
    "\n",
    "In essence, the actors are agents that interact with environments and generate experiences. The observer's role is to monitor these experiences and store them appropriately for training. Together, they ensure a seamless flow of information, enabling the agent to learn effectively from its interactions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters in the above code block\n",
    "- ``observer_sequence_length``\n",
    "- ``observer_stride_length``\n",
    "- ``initial_collect_steps``\n",
    "- ``collector_steps_per_run``\n",
    "- ``eval_episodes_per_run``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = reverb.Client(\"localhost:40263\")\n",
    "\n",
    "# generator = client.sample(table_name)\n",
    "\n",
    "# print(\"Generator: \", generator)\n",
    "\n",
    "# sampled_item = next(generator)\n",
    "\n",
    "# print(\"sampled_item: \", sampled_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.ou_noise_policy.OUNoisePolicy object at 0x7f0190102a10>\". Calling saved_model.distribution() will raise the following assertion error: Distributions are not implemented yet.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (21920) so Table uniform_table is accessed directly without gRPC.\n"
     ]
    }
   ],
   "source": [
    "# Learners\n",
    "# The Learner component contains the agent and performs gradient step updates to the policy variables using experience data from the replay buffer.\n",
    "# After one or more training steps, the Learner can push a new set of variable values to the variable container.\n",
    "\n",
    "saved_model_dir = os.path.join(tempdir, learner.POLICY_SAVED_MODEL_DIR)\n",
    "\n",
    "# Triggers to save the agent's policy checkpoints.\n",
    "learning_triggers = [\n",
    "    triggers.PolicySavedModelTrigger(\n",
    "        saved_model_dir,\n",
    "        tf_agent,\n",
    "        train_step,\n",
    "        interval=policy_save_interval),\n",
    "    triggers.StepPerSecondLogTrigger(train_step, interval=1000)\n",
    "]\n",
    "\n",
    "# A strategy can be used here. The predefined strategy would be passed to the learner.\n",
    "agent_learner = learner.Learner(\n",
    "  tempdir,\n",
    "  train_step,\n",
    "  tf_agent,\n",
    "  experience_dataset_fn,\n",
    "  triggers=learning_triggers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner & Triggers Summary\n",
    "\n",
    "This segment focuses on the creation and configuration of the `Learner` component within the DDPG training process.\n",
    "\n",
    "### 1. **Learner Introduction:**\n",
    "   - The `Learner` component is central to the training process. It encapsulates the agent and is responsible for executing gradient updates on the policy's parameters using experiences sourced from the replay buffer.\n",
    "   - Post-training (after one or multiple steps), the `Learner` has the capability to update the set of variables within a designated container.\n",
    "\n",
    "### 2. **Model Save Directory (`saved_model_dir`):**\n",
    "   - A specific directory path is designated to save the policy's model. This is set as a sub-directory (`POLICY_SAVED_MODEL_DIR`) within the temporary directory (`tempdir`).\n",
    "\n",
    "### 3. **Triggers (`learning_triggers`):**\n",
    "   - Triggers are mechanisms to automate certain actions during the training process.\n",
    "   - Two triggers are set up:\n",
    "     - `PolicySavedModelTrigger`: This saves the agent's policy checkpoints at specified intervals (`policy_save_interval`). These checkpoints capture the policy's state, allowing for later resumption or analysis.\n",
    "     - `StepPerSecondLogTrigger`: Designed to log training information, specifically how many training steps are executed per second. The logging occurs every 1000 steps.\n",
    "\n",
    "### 4. **Instantiating the Learner (`agent_learner`):**\n",
    "   - The `Learner` object is initialized with various configurations:\n",
    "     - The temporary directory (`tempdir`), which serves as a location to save intermediate data.\n",
    "     - The current training step (`train_step`), which keeps track of the progress.\n",
    "     - The agent itself (`tf_agent`), which contains the policy and value network.\n",
    "     - A function (`experience_dataset_fn`) that supplies experiences from the replay buffer.\n",
    "     - The predefined triggers (`learning_triggers`) to automate specific tasks during training.\n",
    "\n",
    "Together, the `Learner` and associated triggers ensure that the agent's policy is continually updated and refined using the stored experiences, while simultaneously maintaining checkpoints and logs for monitoring and potential resumption of training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters in the above code block\n",
    "- ``policy_save_interval``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0: AverageReturn = 0.841057, AverageEpisodeLength = 1001.000000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We instantiated the eval Actor with actor.eval_metrics above, which creates most commonly used metrics during policy evaluation:\n",
    "\n",
    "- Average return. The return is the sum of rewards obtained while running a policy in an environment for an episode, and we usually average this over a few episodes.\n",
    "- Average episode length.\n",
    "\n",
    "We run the Actor to generate these metrics.\n",
    "\"\"\"\n",
    "\n",
    "def get_eval_metrics():\n",
    "  eval_actor.run()\n",
    "  results = {}\n",
    "  for metric in eval_actor.metrics:\n",
    "    results[metric.name] = metric.result()\n",
    "  return results\n",
    "\n",
    "metrics = get_eval_metrics()\n",
    "\n",
    "def log_eval_metrics(step, metrics):\n",
    "  eval_results = (', ').join(\n",
    "      '{} = {:.6f}'.format(name, result) for name, result in metrics.items())\n",
    "  print('step = {0}: {1}'.format(step, eval_results))\n",
    "\n",
    "log_eval_metrics(0, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics Summary\n",
    "\n",
    "In the above segment, the focus is on generating and logging key evaluation metrics during the policy's performance assessment.\n",
    "\n",
    "### 1. **Introduction to Eval Actor Metrics:**\n",
    "   - The `eval_actor` was previously instantiated using `actor.eval_metrics`. This procedure automatically includes some commonly used metrics to evaluate the efficacy of the policy:\n",
    "     - **Average Return**: It signifies the average sum of rewards accumulated while the policy operates within an environment over multiple episodes.\n",
    "     - **Average Episode Length**: This measures the average duration of an episode, indicating how long the agent persists in the environment before reaching a terminal state.\n",
    "\n",
    "### 2. **Metric Generation (`get_eval_metrics` function):**\n",
    "   - The `eval_actor` runs to generate the aforementioned metrics.\n",
    "   - A ``dictionary`` named `results` is populated with metric names as keys and their corresponding outcomes as values.\n",
    "\n",
    "### 3. **Logging Metrics (`log_eval_metrics` function):**\n",
    "   - Designed for a clear and structured display of the evaluation metrics.\n",
    "   - The metrics are formatted as a string and then printed alongside the current training step, offering insights into the policy's performance at different stages of training.\n",
    "\n",
    "### 4. **Executing Metric Collection and Logging:**\n",
    "   - The `get_eval_metrics` function is invoked to obtain the evaluation metrics.\n",
    "   - Using the `log_eval_metrics` function, these metrics are subsequently printed for the training step `0`.\n",
    "\n",
    "Overall, these functions enable consistent monitoring of the policy's performance, ensuring that potential issues or areas of improvement are swiftly identified.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 12:11:49.194770: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f000c032670 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-10 12:11:49.194791: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Ti, Compute Capability 8.9\n",
      "2023-08-10 12:11:49.197872: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-08-10 12:11:49.322524: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-08-10 12:11:49.386876: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 10: AverageReturn = 0.839999, AverageEpisodeLength = 1000.000000\n",
      "step = 10: loss = 2.7446329593658447\n",
      "step = 20: AverageReturn = 0.837968, AverageEpisodeLength = 1000.000000\n",
      "step = 20: loss = 0.9231399297714233\n",
      "step = 30: AverageReturn = 0.834711, AverageEpisodeLength = 1000.000000\n",
      "step = 30: loss = 0.0015735565684735775\n",
      "step = 40: AverageReturn = 0.829382, AverageEpisodeLength = 1000.000000\n",
      "step = 40: loss = 0.7206490635871887\n",
      "step = 50: AverageReturn = 0.820400, AverageEpisodeLength = 1000.000000\n",
      "step = 50: loss = 0.00053497648332268\n",
      "step = 60: AverageReturn = 0.806608, AverageEpisodeLength = 1000.000000\n",
      "step = 60: loss = 0.06836921721696854\n",
      "step = 70: AverageReturn = 0.785148, AverageEpisodeLength = 1000.000000\n",
      "step = 70: loss = 0.2563805878162384\n",
      "step = 80: AverageReturn = 0.751917, AverageEpisodeLength = 1000.000000\n",
      "step = 80: loss = 0.004329761024564505\n",
      "step = 90: AverageReturn = 0.698215, AverageEpisodeLength = 1000.000000\n",
      "step = 90: loss = 0.10924828797578812\n",
      "step = 100: AverageReturn = 0.628135, AverageEpisodeLength = 1000.000000\n",
      "step = 100: loss = 0.2426069974899292\n",
      "step = 110: AverageReturn = 0.568631, AverageEpisodeLength = 1000.000000\n",
      "step = 110: loss = 1.3677589893341064\n",
      "step = 120: AverageReturn = 0.538317, AverageEpisodeLength = 1000.000000\n",
      "step = 120: loss = 0.40406733751296997\n",
      "step = 130: AverageReturn = 0.469706, AverageEpisodeLength = 1000.000000\n",
      "step = 130: loss = 18.295570373535156\n",
      "step = 140: AverageReturn = 0.354964, AverageEpisodeLength = 1000.000000\n",
      "step = 140: loss = 172.97132873535156\n",
      "step = 150: AverageReturn = 0.359368, AverageEpisodeLength = 1000.000000\n",
      "step = 150: loss = 136.4590301513672\n",
      "step = 160: AverageReturn = 0.374337, AverageEpisodeLength = 1000.000000\n",
      "step = 160: loss = 159556.46875\n",
      "step = 170: AverageReturn = 0.325450, AverageEpisodeLength = 1000.000000\n",
      "step = 170: loss = 3301505.5\n",
      "step = 180: AverageReturn = 0.235053, AverageEpisodeLength = 1000.000000\n",
      "step = 180: loss = 108137448.0\n",
      "step = 190: AverageReturn = 0.322969, AverageEpisodeLength = 1000.000000\n",
      "step = 190: loss = 358235008.0\n",
      "step = 200: AverageReturn = 0.434828, AverageEpisodeLength = 1000.000000\n",
      "step = 200: loss = 1769950080.0\n",
      "step = 210: AverageReturn = 0.441669, AverageEpisodeLength = 1000.000000\n",
      "step = 210: loss = 1321973632.0\n",
      "step = 220: AverageReturn = 0.408600, AverageEpisodeLength = 1000.000000\n",
      "step = 220: loss = 14099826688.0\n",
      "step = 230: AverageReturn = 0.436668, AverageEpisodeLength = 1000.000000\n",
      "step = 230: loss = 44787871744.0\n",
      "step = 240: AverageReturn = 0.525183, AverageEpisodeLength = 1000.000000\n",
      "step = 240: loss = 153180782592.0\n",
      "step = 250: AverageReturn = 0.585565, AverageEpisodeLength = 1000.000000\n",
      "step = 250: loss = 688270082048.0\n",
      "step = 260: AverageReturn = 0.642795, AverageEpisodeLength = 1000.000000\n",
      "step = 260: loss = 3704389369856.0\n",
      "step = 270: AverageReturn = 0.690123, AverageEpisodeLength = 1000.000000\n",
      "step = 270: loss = 597929426944.0\n",
      "step = 280: AverageReturn = 0.725159, AverageEpisodeLength = 1000.000000\n",
      "step = 280: loss = 1890087469056.0\n",
      "step = 290: AverageReturn = 0.756940, AverageEpisodeLength = 1000.000000\n",
      "step = 290: loss = 15934916919296.0\n",
      "step = 300: AverageReturn = 0.783084, AverageEpisodeLength = 1000.000000\n",
      "step = 300: loss = 30954863198208.0\n",
      "step = 310: AverageReturn = 0.802276, AverageEpisodeLength = 1000.000000\n",
      "step = 310: loss = 55985771118592.0\n",
      "step = 320: AverageReturn = 0.815822, AverageEpisodeLength = 1000.000000\n",
      "step = 320: loss = 120199424507904.0\n",
      "step = 330: AverageReturn = 0.825362, AverageEpisodeLength = 1000.000000\n",
      "step = 330: loss = 34130802769920.0\n",
      "step = 340: AverageReturn = 0.832219, AverageEpisodeLength = 1000.000000\n",
      "step = 340: loss = 321436526837760.0\n",
      "step = 350: AverageReturn = 0.836780, AverageEpisodeLength = 1000.000000\n",
      "step = 350: loss = 632651434164224.0\n",
      "step = 360: AverageReturn = 0.839588, AverageEpisodeLength = 1000.000000\n",
      "step = 360: loss = 373208498831360.0\n",
      "step = 370: AverageReturn = 0.840833, AverageEpisodeLength = 1000.000000\n",
      "step = 370: loss = 3258267417444352.0\n",
      "step = 380: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 380: loss = 1470446746730496.0\n",
      "step = 390: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 390: loss = 4051848524726272.0\n",
      "step = 400: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 400: loss = 3342856798339072.0\n",
      "step = 410: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 410: loss = 8728247571316736.0\n",
      "step = 420: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 420: loss = 605629110550528.0\n",
      "step = 430: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 430: loss = 1.553129430581248e+16\n",
      "step = 440: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 440: loss = 1.909581114507264e+16\n",
      "step = 450: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 450: loss = 7056994728935424.0\n",
      "step = 460: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 460: loss = 5.268525142350234e+16\n",
      "step = 470: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 470: loss = 7.024668120711168e+16\n",
      "step = 480: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 480: loss = 1.0174469360320512e+16\n",
      "step = 490: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 490: loss = 1.3312237390056653e+17\n",
      "step = 500: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 500: loss = 1.2093633620606976e+16\n",
      "step = 510: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 510: loss = 1.684157694392402e+17\n",
      "step = 520: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 520: loss = 2.430660290753331e+16\n",
      "step = 530: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 530: loss = 2.872863938796585e+17\n",
      "step = 540: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 540: loss = 3.805756347593523e+16\n",
      "step = 550: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 550: loss = 4.293598404023419e+17\n",
      "step = 560: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 560: loss = 8.329650983927808e+16\n",
      "step = 570: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 570: loss = 6.502361013315174e+16\n",
      "step = 580: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 580: loss = 8.065582307076997e+17\n",
      "step = 590: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 590: loss = 9.965275891472794e+17\n",
      "step = 600: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 600: loss = 4.527299256909824e+16\n",
      "step = 610: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 610: loss = 1.5690604735994266e+17\n",
      "step = 620: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 620: loss = 8.570121048870093e+16\n",
      "step = 630: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 630: loss = 1.0585317127172915e+17\n",
      "step = 640: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 640: loss = 2.415984937258713e+18\n",
      "step = 650: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 650: loss = 2.9645694963852247e+18\n",
      "step = 660: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 660: loss = 1.0987935092441088e+18\n",
      "step = 670: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 670: loss = 4.1169019376374907e+18\n",
      "step = 680: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 680: loss = 4.801618955544297e+18\n",
      "step = 690: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 690: loss = 1.8037827727880356e+18\n",
      "step = 700: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 700: loss = 6.474082794019488e+18\n",
      "step = 710: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 710: loss = 4.278029044496204e+18\n",
      "step = 720: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 720: loss = 3.0484028599566336e+18\n",
      "step = 730: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 730: loss = 1.401224490385408e+18\n",
      "step = 740: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 740: loss = 2.2624197216200294e+18\n",
      "step = 750: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 750: loss = 1.4278846237019996e+19\n",
      "step = 760: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 760: loss = 1.2121609920881623e+19\n",
      "step = 770: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 770: loss = 3.0783104008658616e+18\n",
      "step = 780: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 780: loss = 2.2115363575830675e+19\n",
      "step = 790: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 790: loss = 2.5415360809623618e+19\n",
      "step = 800: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 800: loss = 2.928785615581348e+19\n",
      "step = 810: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 810: loss = 3.3424082560064946e+19\n",
      "step = 820: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 820: loss = 3.865066605827955e+19\n",
      "step = 830: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 830: loss = 1.9356152340066337e+19\n",
      "step = 840: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 840: loss = 4.671102747183101e+19\n",
      "step = 850: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 850: loss = 5.653386204424228e+19\n",
      "step = 860: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 860: loss = 6.414071889180125e+19\n",
      "step = 870: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 870: loss = 4.031943443817379e+19\n",
      "step = 880: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 880: loss = 7.949784848559977e+19\n",
      "step = 890: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 890: loss = 1.692543449675124e+19\n",
      "step = 900: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 900: loss = 1.0508589419343538e+20\n",
      "step = 910: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 910: loss = 1.1864104297656484e+20\n",
      "step = 920: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 920: loss = 1.3231885327688899e+20\n",
      "step = 930: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 930: loss = 2.8208261081663013e+19\n",
      "step = 940: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 940: loss = 7.452020101698342e+19\n",
      "step = 950: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 950: loss = 1.7652711960501813e+20\n",
      "step = 960: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 960: loss = 1.9686699400333938e+20\n",
      "step = 970: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 970: loss = 1.1957433483450017e+20\n",
      "step = 980: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 980: loss = 2.411630981163883e+20\n",
      "step = 990: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 990: loss = 2.7060986668776423e+20\n",
      "step = 1000: AverageReturn = 0.841057, AverageEpisodeLength = 1000.000000\n",
      "step = 1000: loss = 2.9137392387598844e+20\n",
      "Training completed. Closing the observer... Closed.\n",
      "Stopping the reverb server... stopped.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/platform/default/server.cc:84] Shutting down replay server\n"
     ]
    }
   ],
   "source": [
    "# The training loop involves both collecting data from the environment and optimizing the agent's networks.\n",
    "# Along the way, we will occasionally evaluate the agent's policy to see how we are doing.\n",
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = get_eval_metrics()[\"AverageReturn\"]\n",
    "returns = [avg_return]\n",
    "losses = [0]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "  # Training.\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  collect_actor.run()\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  loss_info = agent_learner.run(iterations=iterations_per_call)\n",
    "\n",
    "  # Evaluating.\n",
    "  step = agent_learner.train_step_numpy\n",
    "\n",
    "  if eval_interval and step % eval_interval == 0:\n",
    "    metrics = get_eval_metrics()\n",
    "    log_eval_metrics(step, metrics)\n",
    "    returns.append(metrics[\"AverageReturn\"])\n",
    "    losses.append(loss_info.loss.numpy())\n",
    "    \n",
    "\n",
    "  if log_interval and step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, loss_info.loss.numpy()))\n",
    "\n",
    "time.sleep(1)  # Give the last metrics log a second to flush.\n",
    "print(\"Training completed. Closing the observer... \", end=\"\")\n",
    "rb_observer.close()\n",
    "print(\"Closed.\")\n",
    "time.sleep(1)  # Give the observer a second to flush.\n",
    "print(\"Stopping the reverb server... \", end=\"\")\n",
    "reverb_server.stop()\n",
    "print(\"stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop Summary\n",
    "\n",
    "This segment highlights the core procedure: executing the training loop, which involves collecting data, training the agent, and periodically evaluating its performance.\n",
    "\n",
    "### 1. **Initial Setup:**\n",
    "   - The agent's `train_step_counter` is reset to zero, ensuring that training begins afresh.\n",
    "   - The agent's policy is evaluated before the training begins to provide a baseline. The initial average return value is obtained and stored.\n",
    "\n",
    "### 2. **Main Training Loop (`for _ in range(num_iterations)`):**\n",
    "   - For each iteration in the training loop:\n",
    "     - **Data Collection**: The `collect_actor` runs, utilizing the `collect_policy` to gather experiences from the environment. These experiences are stored in the replay buffer.\n",
    "     - **Network Training**: A batch of experiences is sampled from the replay buffer, and the agent's networks (Actor and Critic) are updated. The resultant loss information from this update is captured.\n",
    "\n",
    "### 3. **Periodic Evaluation and Logging:**\n",
    "   - The current training step is fetched.\n",
    "   - If the step aligns with the defined `eval_interval`, the following actions take place:\n",
    "     - The agent's policy is evaluated using `get_eval_metrics`.\n",
    "     - These metrics are logged, and the average return value is added to the `returns` list.\n",
    "     - The loss value from the most recent network update is also recorded in the `losses` list.\n",
    "   - If the step matches the `log_interval`, the loss is printed, providing periodic insight into the agent's training progression.\n",
    "\n",
    "### 4. **Closing Procedures:**\n",
    "   - The `rb_observer` is closed, ensuring that no further experiences are added to the replay buffer.\n",
    "   - The `reverb_server` (associated with the replay buffer) is also stopped, marking the end of the training process.\n",
    "\n",
    "In essence, this segment captures the cyclical process of collecting experiences, training the agent on these experiences, and periodic evaluations to assess the agent's proficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters in the above code block\n",
    "- ``num_iterations``\n",
    "- ``iterations_per_call``\n",
    "- ``eval_interval``\n",
    "- ``log_interval``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.20475269854068756, 0.8713567703962326)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.grid()\n",
    "plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.4568696193799422e+19, 3.0594262006978786e+20)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps[1:], losses[1:])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.grid()\n",
    "plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight 0:\n",
      "[[ 2.1713408e+01]\n",
      " [ 6.1703324e-03]\n",
      " [-7.5619864e-01]\n",
      " [ 7.8544420e-01]\n",
      " [-4.7157168e-02]\n",
      " [ 5.0450617e-01]\n",
      " [ 2.0839445e+01]]\n",
      "-----\n",
      "Weight 1:\n",
      "[24.198214]\n",
      "-----\n",
      "Weight 2:\n",
      "[[ 0.3459    ]\n",
      " [-0.06432837]\n",
      " [ 0.83355814]\n",
      " [ 0.34981853]\n",
      " [-0.77497566]\n",
      " [20.273073  ]\n",
      " [ 0.5782848 ]]\n",
      "-----\n",
      "Weight 3:\n",
      "[24.47441]\n",
      "-----\n",
      "Weight 4:\n",
      "[[ 0.5639302 ]\n",
      " [-0.7486839 ]\n",
      " [-0.8632497 ]\n",
      " [-0.5393698 ]\n",
      " [18.350588  ]\n",
      " [-0.80489594]\n",
      " [ 0.802617  ]]\n",
      "-----\n",
      "Weight 5:\n",
      "[23.228231]\n",
      "-----\n",
      "Weight 6:\n",
      "[[ 4.1257852e-01]\n",
      " [ 1.4941871e-02]\n",
      " [ 7.3333484e-01]\n",
      " [ 1.7865797e+01]\n",
      " [-7.0385814e-02]\n",
      " [ 2.6369661e-01]\n",
      " [-5.6624532e-02]]\n",
      "-----\n",
      "Weight 7:\n",
      "[23.430912]\n",
      "-----\n",
      "Weight 8:\n",
      "[[ 0.4623472 ]\n",
      " [ 0.26793736]\n",
      " [19.018215  ]\n",
      " [ 0.36379784]\n",
      " [ 0.11377376]\n",
      " [ 0.14798957]\n",
      " [ 0.15267104]]\n",
      "-----\n",
      "Weight 9:\n",
      "[22.683662]\n",
      "-----\n",
      "Weight 10:\n",
      "[[ 0.22759968]\n",
      " [21.773869  ]\n",
      " [-0.65458286]\n",
      " [ 0.730297  ]\n",
      " [ 0.20568758]\n",
      " [-0.411292  ]\n",
      " [ 0.22095138]]\n",
      "-----\n",
      "Weight 11:\n",
      "[24.354982]\n",
      "-----\n",
      "Weight 12:\n",
      "[[19.611876  ]\n",
      " [ 0.37414712]\n",
      " [ 0.4337446 ]\n",
      " [ 0.18159348]\n",
      " [ 0.3679716 ]\n",
      " [ 0.6466494 ]\n",
      " [22.45739   ]]\n",
      "-----\n",
      "Weight 13:\n",
      "[24.488192]\n",
      "-----\n",
      "Weight 14:\n",
      "[[ 0.33848304]\n",
      " [-0.57241   ]\n",
      " [-0.5771569 ]\n",
      " [-0.58183277]\n",
      " [-0.6362994 ]\n",
      " [18.66265   ]\n",
      " [-0.27019686]]\n",
      "-----\n",
      "Weight 15:\n",
      "[24.530312]\n",
      "-----\n",
      "Weight 16:\n",
      "[[-0.07408231]\n",
      " [ 0.5116002 ]\n",
      " [ 0.6312236 ]\n",
      " [-0.85996056]\n",
      " [21.413067  ]\n",
      " [ 0.76211447]\n",
      " [ 0.24384218]]\n",
      "-----\n",
      "Weight 17:\n",
      "[24.272764]\n",
      "-----\n",
      "Weight 18:\n",
      "[[ 0.5913703 ]\n",
      " [ 0.7109545 ]\n",
      " [ 0.04752183]\n",
      " [18.165833  ]\n",
      " [ 0.8325004 ]\n",
      " [ 0.10273302]\n",
      " [ 0.5044361 ]]\n",
      "-----\n",
      "Weight 19:\n",
      "[22.84987]\n",
      "-----\n",
      "Weight 20:\n",
      "[[-0.06392598]\n",
      " [ 0.404828  ]\n",
      " [21.01737   ]\n",
      " [ 0.17291838]\n",
      " [-0.13511348]\n",
      " [-0.72852254]\n",
      " [-0.58415973]]\n",
      "-----\n",
      "Weight 21:\n",
      "[23.809134]\n",
      "-----\n",
      "Weight 22:\n",
      "[[-0.6245866 ]\n",
      " [17.96387   ]\n",
      " [-0.40059194]\n",
      " [-0.73476493]\n",
      " [-0.33264506]\n",
      " [ 0.05739492]\n",
      " [ 0.14190346]]\n",
      "-----\n",
      "Weight 23:\n",
      "[23.511314]\n",
      "-----\n",
      "Weight 24:\n",
      "[[19.931892  ]\n",
      " [-0.5872903 ]\n",
      " [ 0.1904152 ]\n",
      " [ 0.12018651]\n",
      " [-0.20866704]\n",
      " [ 0.8490884 ]\n",
      " [20.663866  ]]\n",
      "-----\n",
      "Weight 25:\n",
      "[22.904987]\n",
      "-----\n",
      "Weight 26:\n",
      "[[ 3.0531853e-01]\n",
      " [-6.1461788e-01]\n",
      " [ 1.9675785e-01]\n",
      " [-8.1015038e-01]\n",
      " [ 1.8536448e-02]\n",
      " [ 1.9670631e+01]\n",
      " [-3.7170175e-01]]\n",
      "-----\n",
      "Weight 27:\n",
      "[23.181997]\n",
      "-----\n",
      "Weight 28:\n",
      "[[ 0.4273904 ]\n",
      " [ 0.73969847]\n",
      " [-0.47662342]\n",
      " [-0.31642509]\n",
      " [20.402819  ]\n",
      " [ 0.5488376 ]\n",
      " [-0.5926742 ]]\n",
      "-----\n",
      "Weight 29:\n",
      "[24.398464]\n",
      "-----\n",
      "Weight 30:\n",
      "[[-1.3725489e-01]\n",
      " [-4.9015674e-01]\n",
      " [-8.5109240e-01]\n",
      " [ 2.1097370e+01]\n",
      " [-1.5222490e-02]\n",
      " [ 7.2008175e-01]\n",
      " [-1.2832165e-01]]\n",
      "-----\n",
      "Weight 31:\n",
      "[23.549238]\n",
      "-----\n",
      "Weight 32:\n",
      "[[ 0.23737448]\n",
      " [-0.7463842 ]\n",
      " [20.929174  ]\n",
      " [-0.4487235 ]\n",
      " [ 0.8212634 ]\n",
      " [-0.76757735]\n",
      " [ 0.4338029 ]]\n",
      "-----\n",
      "Weight 33:\n",
      "[24.345333]\n",
      "-----\n",
      "Weight 34:\n",
      "[[ 0.5185748 ]\n",
      " [20.31895   ]\n",
      " [-0.26259875]\n",
      " [ 0.07041305]\n",
      " [ 0.75288385]\n",
      " [ 0.05036312]\n",
      " [ 0.612632  ]]\n",
      "-----\n",
      "Weight 35:\n",
      "[24.56588]\n",
      "-----\n",
      "Weight 36:\n",
      "[[20.772383 ]\n",
      " [ 0.314421 ]\n",
      " [-0.3923426]\n",
      " [ 0.8582906]\n",
      " [-0.580829 ]\n",
      " [ 0.6643135]\n",
      " [20.374355 ]]\n",
      "-----\n",
      "Weight 37:\n",
      "[23.785667]\n",
      "-----\n",
      "Weight 38:\n",
      "[[ 0.29876477]\n",
      " [-0.0855065 ]\n",
      " [ 0.6259522 ]\n",
      " [ 0.22464997]\n",
      " [ 0.47436374]\n",
      " [17.436934  ]\n",
      " [-0.5114783 ]]\n",
      "-----\n",
      "Weight 39:\n",
      "[23.64209]\n",
      "-----\n",
      "Weight 40:\n",
      "[[ 0.6260658 ]\n",
      " [ 0.84014505]\n",
      " [-0.0990538 ]\n",
      " [ 0.37490314]\n",
      " [21.584942  ]\n",
      " [-0.14020973]\n",
      " [-0.11275542]]\n",
      "-----\n",
      "Weight 41:\n",
      "[24.332869]\n",
      "-----\n",
      "Weight 42:\n",
      "[[ 0.13810843]\n",
      " [ 0.6650159 ]\n",
      " [ 0.8576332 ]\n",
      " [20.801558  ]\n",
      " [-0.739899  ]\n",
      " [ 0.02895731]\n",
      " [ 0.2591024 ]]\n",
      "-----\n",
      "Weight 43:\n",
      "[24.171072]\n",
      "-----\n",
      "Weight 44:\n",
      "[[-0.53105724]\n",
      " [-0.26048005]\n",
      " [19.546537  ]\n",
      " [-0.39709982]\n",
      " [ 0.6693447 ]\n",
      " [ 0.4619543 ]\n",
      " [-0.07602113]]\n",
      "-----\n",
      "Weight 45:\n",
      "[23.380104]\n",
      "-----\n",
      "Weight 46:\n",
      "[[ 0.12175572]\n",
      " [18.723694  ]\n",
      " [-0.7669348 ]\n",
      " [-0.26653397]\n",
      " [-0.02821916]\n",
      " [-0.46894065]\n",
      " [-0.06576276]]\n",
      "-----\n",
      "Weight 47:\n",
      "[22.71077]\n",
      "-----\n",
      "Weight 48:\n",
      "[[20.176155  ]\n",
      " [-0.3354259 ]\n",
      " [ 0.14302677]\n",
      " [ 0.03030312]\n",
      " [ 0.09340113]\n",
      " [-0.45965022]\n",
      " [21.366493  ]]\n",
      "-----\n",
      "Weight 49:\n",
      "[23.862185]\n",
      "-----\n",
      "Weight 50:\n",
      "[[-0.25647527]\n",
      " [-0.6192475 ]\n",
      " [ 0.23775262]\n",
      " [-0.79686856]\n",
      " [-0.2783981 ]\n",
      " [20.257393  ]\n",
      " [-0.08033484]]\n",
      "-----\n",
      "Weight 51:\n",
      "[23.732597]\n",
      "-----\n",
      "Weight 52:\n",
      "[[ 4.5043558e-01]\n",
      " [ 2.7051586e-01]\n",
      " [-7.0933986e-01]\n",
      " [ 4.2160052e-01]\n",
      " [ 1.8019169e+01]\n",
      " [ 1.7341775e-01]\n",
      " [ 3.1630397e-03]]\n",
      "-----\n",
      "Weight 53:\n",
      "[23.358105]\n",
      "-----\n",
      "Weight 54:\n",
      "[[ 0.37483007]\n",
      " [ 0.27544802]\n",
      " [-0.28370953]\n",
      " [17.67896   ]\n",
      " [ 0.2901314 ]\n",
      " [ 0.05736148]\n",
      " [-0.15137458]]\n",
      "-----\n",
      "Weight 55:\n",
      "[24.172037]\n",
      "-----\n",
      "Weight 56:\n",
      "[[-0.07907885]\n",
      " [ 0.12246394]\n",
      " [18.822517  ]\n",
      " [-0.6615546 ]\n",
      " [ 0.6402429 ]\n",
      " [ 0.709745  ]\n",
      " [-0.49194625]]\n",
      "-----\n",
      "Weight 57:\n",
      "[24.246874]\n",
      "-----\n",
      "Weight 58:\n",
      "[[-0.29845524]\n",
      " [20.272726  ]\n",
      " [-0.8169315 ]\n",
      " [ 0.46583563]\n",
      " [-0.22199988]\n",
      " [ 0.06739849]\n",
      " [-0.6621779 ]]\n",
      "-----\n",
      "Weight 59:\n",
      "[24.420742]\n",
      "-----\n",
      "Weight 60:\n",
      "[[ 1.7159269e+01]\n",
      " [ 7.6532680e-01]\n",
      " [ 6.7723399e-01]\n",
      " [ 1.8939477e-01]\n",
      " [-5.8059514e-01]\n",
      " [-1.8766522e-03]\n",
      " [ 1.9256090e+01]]\n",
      "-----\n",
      "Weight 61:\n",
      "[23.624128]\n",
      "-----\n",
      "Weight 62:\n",
      "[[ 0.76178616]\n",
      " [ 0.6929502 ]\n",
      " [-0.5148984 ]\n",
      " [ 0.22615165]\n",
      " [-0.3049739 ]\n",
      " [16.363916  ]\n",
      " [-0.04335225]]\n",
      "-----\n",
      "Weight 63:\n",
      "[22.67309]\n",
      "-----\n",
      "Weight 64:\n",
      "[[ 0.02149236]\n",
      " [-0.5312989 ]\n",
      " [ 0.61694926]\n",
      " [ 0.7021629 ]\n",
      " [18.287537  ]\n",
      " [ 0.70109195]\n",
      " [-0.6369145 ]]\n",
      "-----\n",
      "Weight 65:\n",
      "[21.507633]\n",
      "-----\n",
      "Weight 66:\n",
      "[[-3.0882818e-01]\n",
      " [ 2.2002321e-01]\n",
      " [ 1.8333179e-01]\n",
      " [ 1.6417675e+01]\n",
      " [-6.0293460e-01]\n",
      " [ 1.5478730e-02]\n",
      " [-7.3456198e-01]]\n",
      "-----\n",
      "Weight 67:\n",
      "[22.81668]\n",
      "-----\n",
      "Weight 68:\n",
      "[[-0.48256934]\n",
      " [ 0.70351034]\n",
      " [20.32971   ]\n",
      " [-0.34811968]\n",
      " [ 0.65159243]\n",
      " [ 0.0965274 ]\n",
      " [-0.34402066]]\n",
      "-----\n",
      "Weight 69:\n",
      "[22.31423]\n",
      "-----\n",
      "Weight 70:\n",
      "[[ 0.5453543 ]\n",
      " [17.007277  ]\n",
      " [-0.50329363]\n",
      " [-0.69140077]\n",
      " [-0.0208376 ]\n",
      " [ 0.40422553]\n",
      " [ 0.5963846 ]]\n",
      "-----\n",
      "Weight 71:\n",
      "[21.956537]\n",
      "-----\n",
      "Weight 72:\n",
      "[[18.195436  ]\n",
      " [-0.65319514]\n",
      " [-0.05962712]\n",
      " [ 0.36123592]\n",
      " [-0.27286744]\n",
      " [ 0.80128855]\n",
      " [19.6278    ]]\n",
      "-----\n",
      "Weight 73:\n",
      "[22.210333]\n",
      "-----\n",
      "Weight 74:\n",
      "[[-0.24714005]\n",
      " [-0.49209285]\n",
      " [ 0.29451567]\n",
      " [-0.16626233]\n",
      " [ 0.76168984]\n",
      " [16.193867  ]\n",
      " [-0.52048236]]\n",
      "-----\n",
      "Weight 75:\n",
      "[21.542336]\n",
      "-----\n",
      "Weight 76:\n",
      "[[-0.43753886]\n",
      " [ 0.44487458]\n",
      " [-0.7081464 ]\n",
      " [-0.33357424]\n",
      " [20.061604  ]\n",
      " [-0.52061015]\n",
      " [-0.8055513 ]]\n",
      "-----\n",
      "Weight 77:\n",
      "[24.455011]\n",
      "-----\n",
      "Weight 78:\n",
      "[[-0.6169996 ]\n",
      " [-0.85058796]\n",
      " [ 0.54705924]\n",
      " [16.427715  ]\n",
      " [ 0.38391417]\n",
      " [ 0.06794918]\n",
      " [-0.6642024 ]]\n",
      "-----\n",
      "Weight 79:\n",
      "[22.098562]\n",
      "-----\n",
      "Weight 80:\n",
      "[[-0.8294415 ]\n",
      " [ 0.52352315]\n",
      " [21.58444   ]\n",
      " [ 0.78354174]\n",
      " [-0.18179494]\n",
      " [ 0.20346719]\n",
      " [-0.6755256 ]]\n",
      "-----\n",
      "Weight 81:\n",
      "[24.224432]\n",
      "-----\n",
      "Weight 82:\n",
      "[[ 0.4691705 ]\n",
      " [18.903486  ]\n",
      " [ 0.84237975]\n",
      " [ 0.8361891 ]\n",
      " [ 0.39524108]\n",
      " [-0.3536679 ]\n",
      " [-0.8209763 ]]\n",
      "-----\n",
      "Weight 83:\n",
      "[22.047964]\n",
      "-----\n",
      "Weight 84:\n",
      "[[18.21615   ]\n",
      " [-0.13918912]\n",
      " [ 0.71572167]\n",
      " [-0.4342526 ]\n",
      " [-0.02992958]\n",
      " [ 0.08983672]\n",
      " [19.953276  ]]\n",
      "-----\n",
      "Weight 85:\n",
      "[23.388496]\n",
      "-----\n",
      "Weight 86:\n",
      "[[ 0.18708307]\n",
      " [-0.82460725]\n",
      " [ 0.11585814]\n",
      " [ 0.35335213]\n",
      " [-0.55233216]\n",
      " [21.777477  ]\n",
      " [ 0.2605496 ]]\n",
      "-----\n",
      "Weight 87:\n",
      "[23.370806]\n",
      "-----\n",
      "Weight 88:\n",
      "[[ 0.19763726]\n",
      " [-0.0929392 ]\n",
      " [-0.34893793]\n",
      " [ 0.1292634 ]\n",
      " [18.03487   ]\n",
      " [-0.56633747]\n",
      " [ 0.83909017]]\n",
      "-----\n",
      "Weight 89:\n",
      "[21.938393]\n",
      "-----\n",
      "Weight 90:\n",
      "[[ 0.7486308 ]\n",
      " [-0.37580135]\n",
      " [-0.39177087]\n",
      " [18.701242  ]\n",
      " [-0.8030595 ]\n",
      " [-0.33115268]\n",
      " [ 0.09029508]]\n",
      "-----\n",
      "Weight 91:\n",
      "[22.071941]\n",
      "-----\n",
      "Weight 92:\n",
      "[[-0.8619211 ]\n",
      " [-0.6565314 ]\n",
      " [20.078842  ]\n",
      " [-0.65843534]\n",
      " [-0.53995574]\n",
      " [-0.8002665 ]\n",
      " [-0.42456636]]\n",
      "-----\n",
      "Weight 93:\n",
      "[23.072569]\n",
      "-----\n",
      "Weight 94:\n",
      "[[ 0.58522016]\n",
      " [22.2252    ]\n",
      " [ 0.62223214]\n",
      " [ 0.3771482 ]\n",
      " [-0.25603235]\n",
      " [ 0.14216799]\n",
      " [-0.81626844]]\n",
      "-----\n",
      "Weight 95:\n",
      "[24.373905]\n",
      "-----\n",
      "Weight 96:\n",
      "[[20.549362  ]\n",
      " [-0.28969258]\n",
      " [-0.5862955 ]\n",
      " [ 0.62395746]\n",
      " [-0.44049025]\n",
      " [-0.47581652]\n",
      " [18.422516  ]]\n",
      "-----\n",
      "Weight 97:\n",
      "[23.501093]\n",
      "-----\n",
      "Weight 98:\n",
      "[[ 0.37535065]\n",
      " [ 0.01884592]\n",
      " [-0.24533421]\n",
      " [-0.71204054]\n",
      " [ 0.08720142]\n",
      " [18.112091  ]\n",
      " [ 0.27461547]]\n",
      "-----\n",
      "Weight 99:\n",
      "[21.455479]\n",
      "-----\n",
      "Weight 100:\n",
      "[[-0.5040138 ]\n",
      " [-0.7566513 ]\n",
      " [ 0.31011254]\n",
      " [-0.75595564]\n",
      " [21.711662  ]\n",
      " [-0.4188075 ]\n",
      " [ 0.2338211 ]]\n",
      "-----\n",
      "Weight 101:\n",
      "[24.275375]\n",
      "-----\n",
      "Weight 102:\n",
      "[[ 0.76672953]\n",
      " [-0.64889735]\n",
      " [ 0.1092304 ]\n",
      " [21.714304  ]\n",
      " [ 0.78599674]\n",
      " [ 0.29062873]\n",
      " [ 0.3823281 ]]\n",
      "-----\n",
      "Weight 103:\n",
      "[23.421322]\n",
      "-----\n",
      "Weight 104:\n",
      "[[-0.81403416]\n",
      " [-0.10368407]\n",
      " [19.136488  ]\n",
      " [ 0.09471822]\n",
      " [-0.6642173 ]\n",
      " [ 0.36252075]\n",
      " [-0.49953222]]\n",
      "-----\n",
      "Weight 105:\n",
      "[24.393185]\n",
      "-----\n",
      "Weight 106:\n",
      "[[-0.855659  ]\n",
      " [20.658594  ]\n",
      " [ 0.7188186 ]\n",
      " [-0.14906245]\n",
      " [ 0.03602105]\n",
      " [-0.02126688]\n",
      " [ 0.25817126]]\n",
      "-----\n",
      "Weight 107:\n",
      "[23.71008]\n",
      "-----\n",
      "Weight 108:\n",
      "[[ 1.7494329e+01]\n",
      " [-9.2307329e-03]\n",
      " [ 3.9795262e-01]\n",
      " [ 2.0925015e-01]\n",
      " [-6.2779856e-01]\n",
      " [ 1.2865204e-01]\n",
      " [ 2.1435345e+01]]\n",
      "-----\n",
      "Weight 109:\n",
      "[23.043064]\n",
      "-----\n",
      "Weight 110:\n",
      "[[ 0.43206745]\n",
      " [ 0.26199287]\n",
      " [-0.20284337]\n",
      " [-0.6431724 ]\n",
      " [ 0.36658984]\n",
      " [21.076982  ]\n",
      " [ 0.4849531 ]]\n",
      "-----\n",
      "Weight 111:\n",
      "[24.594841]\n",
      "-----\n",
      "Weight 112:\n",
      "[[ 0.2428332 ]\n",
      " [-0.8659845 ]\n",
      " [ 0.68318444]\n",
      " [ 0.75997597]\n",
      " [18.396332  ]\n",
      " [ 0.47084826]\n",
      " [ 0.34883982]]\n",
      "-----\n",
      "Weight 113:\n",
      "[24.16689]\n",
      "-----\n",
      "Weight 114:\n",
      "[[-0.5575433 ]\n",
      " [-0.15924442]\n",
      " [ 0.20262104]\n",
      " [18.365267  ]\n",
      " [ 0.18940502]\n",
      " [ 0.4672938 ]\n",
      " [-0.47013965]]\n",
      "-----\n",
      "Weight 115:\n",
      "[23.374477]\n",
      "-----\n",
      "Weight 116:\n",
      "[[ 0.20153469]\n",
      " [ 0.6551102 ]\n",
      " [20.519423  ]\n",
      " [ 0.2487989 ]\n",
      " [-0.38382998]\n",
      " [ 0.05154377]\n",
      " [ 0.5193259 ]]\n",
      "-----\n",
      "Weight 117:\n",
      "[23.55]\n",
      "-----\n",
      "Weight 118:\n",
      "[[ 0.66460055]\n",
      " [20.573223  ]\n",
      " [-0.10391718]\n",
      " [-0.1513502 ]\n",
      " [-0.58491874]\n",
      " [-0.18465257]\n",
      " [ 0.34985143]]\n",
      "-----\n",
      "Weight 119:\n",
      "[24.513115]\n",
      "-----\n",
      "Weight 120:\n",
      "[[21.243021  ]\n",
      " [-0.8588177 ]\n",
      " [-0.5277978 ]\n",
      " [-0.08374357]\n",
      " [-0.26121098]\n",
      " [ 0.5623228 ]\n",
      " [21.148895  ]]\n",
      "-----\n",
      "Weight 121:\n",
      "[23.985617]\n",
      "-----\n",
      "Weight 122:\n",
      "[[-3.7568820e-01]\n",
      " [-6.7265105e-01]\n",
      " [ 3.1632107e-01]\n",
      " [ 1.5196699e-01]\n",
      " [ 8.7834477e-02]\n",
      " [ 1.9244967e+01]\n",
      " [-1.4235318e-02]]\n",
      "-----\n",
      "Weight 123:\n",
      "[22.442568]\n",
      "-----\n",
      "Weight 124:\n",
      "[[-0.05882972]\n",
      " [ 0.64553875]\n",
      " [-0.81675595]\n",
      " [ 0.6095124 ]\n",
      " [21.568514  ]\n",
      " [-0.5780065 ]\n",
      " [ 0.30746013]]\n",
      "-----\n",
      "Weight 125:\n",
      "[24.072863]\n",
      "-----\n",
      "Weight 126:\n",
      "[[ 0.47295994]\n",
      " [-0.0769757 ]\n",
      " [-0.625594  ]\n",
      " [17.403862  ]\n",
      " [-0.28986955]\n",
      " [ 0.72870547]\n",
      " [ 0.24494189]]\n",
      "-----\n",
      "Weight 127:\n",
      "[22.004517]\n",
      "-----\n",
      "Weight 128:\n",
      "[[ 0.8537198 ]\n",
      " [-0.42543873]\n",
      " [21.327274  ]\n",
      " [-0.10301423]\n",
      " [ 0.4209053 ]\n",
      " [ 0.54521674]\n",
      " [ 0.4169963 ]]\n",
      "-----\n",
      "Weight 129:\n",
      "[24.416676]\n",
      "-----\n",
      "Weight 130:\n",
      "[[ 5.8638984e-01]\n",
      " [ 1.9839584e+01]\n",
      " [ 7.1916074e-01]\n",
      " [-4.2820263e-01]\n",
      " [-1.2763321e-02]\n",
      " [ 5.4376048e-01]\n",
      " [ 6.2007016e-01]]\n",
      "-----\n",
      "Weight 131:\n",
      "[24.15468]\n",
      "-----\n",
      "Weight 132:\n",
      "[[16.94159   ]\n",
      " [ 0.8571966 ]\n",
      " [-0.11552835]\n",
      " [-0.408621  ]\n",
      " [ 0.5169588 ]\n",
      " [-0.83495665]\n",
      " [17.104864  ]]\n",
      "-----\n",
      "Weight 133:\n",
      "[22.958212]\n",
      "-----\n",
      "Weight 134:\n",
      "[[ 0.6792801 ]\n",
      " [-0.10264486]\n",
      " [-0.21101344]\n",
      " [ 0.37586826]\n",
      " [-0.6106164 ]\n",
      " [18.801277  ]\n",
      " [ 0.37621242]]\n",
      "-----\n",
      "Weight 135:\n",
      "[23.063929]\n",
      "-----\n",
      "Weight 136:\n",
      "[[ 0.7373963 ]\n",
      " [ 0.72134894]\n",
      " [-0.21236795]\n",
      " [-0.21206981]\n",
      " [20.621088  ]\n",
      " [ 0.21815854]\n",
      " [-0.19149727]]\n",
      "-----\n",
      "Weight 137:\n",
      "[24.01667]\n",
      "-----\n",
      "Weight 138:\n",
      "[[ 0.6127772 ]\n",
      " [ 0.29401475]\n",
      " [-0.6507668 ]\n",
      " [19.511972  ]\n",
      " [ 0.6320662 ]\n",
      " [ 0.12114781]\n",
      " [-0.20085979]]\n",
      "-----\n",
      "Weight 139:\n",
      "[23.906109]\n",
      "-----\n",
      "Weight 140:\n",
      "[[-2.3265159e-01]\n",
      " [ 7.9169303e-01]\n",
      " [ 2.1119841e+01]\n",
      " [-6.0138154e-01]\n",
      " [-1.1965334e-02]\n",
      " [-5.8656287e-01]\n",
      " [ 7.4869257e-01]]\n",
      "-----\n",
      "Weight 141:\n",
      "[24.22754]\n",
      "-----\n",
      "Weight 142:\n",
      "[[-0.65197754]\n",
      " [20.080597  ]\n",
      " [-0.596125  ]\n",
      " [-0.7266806 ]\n",
      " [-0.6132748 ]\n",
      " [-0.05968183]\n",
      " [ 0.05083144]]\n",
      "-----\n",
      "Weight 143:\n",
      "[24.425575]\n",
      "-----\n",
      "Weight 144:\n",
      "[[19.781473 ]\n",
      " [-0.4450645]\n",
      " [ 0.5737062]\n",
      " [ 0.7994024]\n",
      " [ 0.7653348]\n",
      " [ 0.3465566]\n",
      " [19.48363  ]]\n",
      "-----\n",
      "Weight 145:\n",
      "[24.4231]\n",
      "-----\n",
      "Weight 146:\n",
      "[[ 0.7484793 ]\n",
      " [-0.7360759 ]\n",
      " [-0.20345539]\n",
      " [ 0.6852779 ]\n",
      " [-0.66814715]\n",
      " [20.660618  ]\n",
      " [ 0.06335694]]\n",
      "-----\n",
      "Weight 147:\n",
      "[24.100822]\n",
      "-----\n",
      "Weight 148:\n",
      "[[-0.6762402 ]\n",
      " [ 0.47814745]\n",
      " [ 0.5208611 ]\n",
      " [-0.23801583]\n",
      " [19.799568  ]\n",
      " [ 0.16761643]\n",
      " [-0.6867449 ]]\n",
      "-----\n",
      "Weight 149:\n",
      "[24.043718]\n",
      "-----\n",
      "Weight 150:\n",
      "[[ 0.03207612]\n",
      " [ 0.6059465 ]\n",
      " [-0.81587285]\n",
      " [19.508415  ]\n",
      " [ 0.61422783]\n",
      " [ 0.5873948 ]\n",
      " [-0.7631028 ]]\n",
      "-----\n",
      "Weight 151:\n",
      "[23.504091]\n",
      "-----\n",
      "Weight 152:\n",
      "[[-0.36865747]\n",
      " [ 0.29211408]\n",
      " [18.458027  ]\n",
      " [ 0.6780935 ]\n",
      " [ 0.70176977]\n",
      " [ 0.26950926]\n",
      " [ 0.5673738 ]]\n",
      "-----\n",
      "Weight 153:\n",
      "[24.300383]\n",
      "-----\n",
      "Weight 154:\n",
      "[[-0.7799742 ]\n",
      " [18.3143    ]\n",
      " [-0.64132273]\n",
      " [ 0.8658677 ]\n",
      " [-0.38786206]\n",
      " [ 0.74590415]\n",
      " [ 0.4142217 ]]\n",
      "-----\n",
      "Weight 155:\n",
      "[21.760334]\n",
      "-----\n",
      "Weight 156:\n",
      "[[17.805109  ]\n",
      " [ 0.02556533]\n",
      " [-0.14144117]\n",
      " [-0.76696   ]\n",
      " [ 0.04366338]\n",
      " [ 0.460432  ]\n",
      " [17.776192  ]]\n",
      "-----\n",
      "Weight 157:\n",
      "[21.773794]\n",
      "-----\n",
      "Weight 158:\n",
      "[[-0.5060959 ]\n",
      " [-0.41958758]\n",
      " [-0.31363827]\n",
      " [ 0.41512567]\n",
      " [-0.6897811 ]\n",
      " [19.240484  ]\n",
      " [-0.6330455 ]]\n",
      "-----\n",
      "Weight 159:\n",
      "[23.549158]\n",
      "-----\n",
      "Weight 160:\n",
      "[[-0.3418017 ]\n",
      " [-0.36923045]\n",
      " [-0.56297505]\n",
      " [-0.0266524 ]\n",
      " [19.92002   ]\n",
      " [-0.44243464]\n",
      " [ 0.05595762]]\n",
      "-----\n",
      "Weight 161:\n",
      "[23.476461]\n",
      "-----\n",
      "Weight 162:\n",
      "[[-0.8158584 ]\n",
      " [-0.32659614]\n",
      " [-0.2919706 ]\n",
      " [18.990166  ]\n",
      " [ 0.44779557]\n",
      " [ 0.4332307 ]\n",
      " [-0.45282412]]\n",
      "-----\n",
      "Weight 163:\n",
      "[23.138493]\n",
      "-----\n",
      "Weight 164:\n",
      "[[ 0.76093894]\n",
      " [ 0.4300838 ]\n",
      " [19.924398  ]\n",
      " [ 0.14241904]\n",
      " [-0.6076175 ]\n",
      " [-0.21665978]\n",
      " [ 0.490395  ]]\n",
      "-----\n",
      "Weight 165:\n",
      "[23.767721]\n",
      "-----\n",
      "Weight 166:\n",
      "[[ 1.3979954e-01]\n",
      " [ 2.0099075e+01]\n",
      " [ 7.4869949e-01]\n",
      " [ 4.9708027e-01]\n",
      " [ 5.8593661e-01]\n",
      " [-2.8519386e-01]\n",
      " [ 8.5090995e-03]]\n",
      "-----\n",
      "Weight 167:\n",
      "[23.82143]\n",
      "-----\n",
      "Weight 168:\n",
      "[[17.148975  ]\n",
      " [-0.8375657 ]\n",
      " [-0.7259577 ]\n",
      " [-0.35432327]\n",
      " [ 0.03769064]\n",
      " [ 0.08328086]\n",
      " [18.50308   ]]\n",
      "-----\n",
      "Weight 169:\n",
      "[22.102589]\n",
      "-----\n",
      "Weight 170:\n",
      "[[ 0.3111055 ]\n",
      " [-0.80634856]\n",
      " [ 0.17951679]\n",
      " [-0.40530992]\n",
      " [-0.12074828]]\n",
      "-----\n",
      "Weight 171:\n",
      "[23.907154]\n",
      "-----\n",
      "Weight 172:\n",
      "[]\n",
      "-----\n",
      "Weight 173:\n",
      "[24.102789]\n",
      "-----\n",
      "Weight 174:\n",
      "[]\n",
      "-----\n",
      "Weight 175:\n",
      "[24.399492]\n",
      "-----\n",
      "Weight 176:\n",
      "[]\n",
      "-----\n",
      "Weight 177:\n",
      "[23.17058]\n",
      "-----\n",
      "Weight 178:\n",
      "[]\n",
      "-----\n",
      "Weight 179:\n",
      "[22.42873]\n",
      "-----\n",
      "Weight 180:\n",
      "[]\n",
      "-----\n",
      "Weight 181:\n",
      "[24.379642]\n",
      "-----\n",
      "Weight 182:\n",
      "[]\n",
      "-----\n",
      "Weight 183:\n",
      "[22.689974]\n",
      "-----\n",
      "Weight 184:\n",
      "[]\n",
      "-----\n",
      "Weight 185:\n",
      "[24.610125]\n",
      "-----\n",
      "Weight 186:\n",
      "[]\n",
      "-----\n",
      "Weight 187:\n",
      "[23.441446]\n",
      "-----\n",
      "Weight 188:\n",
      "[]\n",
      "-----\n",
      "Weight 189:\n",
      "[23.74349]\n",
      "-----\n",
      "Weight 190:\n",
      "[]\n",
      "-----\n",
      "Weight 191:\n",
      "[24.134487]\n",
      "-----\n",
      "Weight 192:\n",
      "[]\n",
      "-----\n",
      "Weight 193:\n",
      "[24.366814]\n",
      "-----\n",
      "Weight 194:\n",
      "[]\n",
      "-----\n",
      "Weight 195:\n",
      "[24.27569]\n",
      "-----\n",
      "Weight 196:\n",
      "[]\n",
      "-----\n",
      "Weight 197:\n",
      "[24.107622]\n",
      "-----\n",
      "Weight 198:\n",
      "[]\n",
      "-----\n",
      "Weight 199:\n",
      "[23.922102]\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "weights = actor_net.get_weights()\n",
    "for i, w in enumerate(weights):\n",
    "    print(f\"Weight {i}:\")\n",
    "    print(w)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
