{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 10:18:13.484333: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-23 10:18:13.508316: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-23 10:18:13.859442: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/renlephy/miniconda3/envs/bachelor/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "2023-08-23 10:18:14,872\tWARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "2023-08-23 10:18:14,917\tWARNING deprecation.py:50 -- DeprecationWarning: `build_tf_policy` has been deprecated. This will raise an error in the future!\n",
      "2023-08-23 10:18:14,922\tWARNING deprecation.py:50 -- DeprecationWarning: `build_policy_class` has been deprecated. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "# Basics\n",
    "from Solver import Particle, Perceptron, PerceptronModel, VicsekModel, NeuralNetwork, PerceptronMode, Mode, NeuralSwarmModel\n",
    "\n",
    "import numpy        as np\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import tensorflow as tf\n",
    "\n",
    "# Logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv, make_multi_agent\n",
    "from ray.rllib.utils import check_env\n",
    "from ray.rllib.algorithms.maddpg import MADDPGConfig\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "# For custom models\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation settings\n",
    "settings = {\n",
    "        #                  N,      L,      v,      noise,  r\n",
    "        \"testing\": [       10,     10,     0.03,   0.1,    1],\n",
    "        \"small\": [         100,    10,     0.03,   0.1,    1],\n",
    "        \"medium\": [        1000,   10,     0.03,   0.1,    1],\n",
    "    }\n",
    "    \n",
    "# Choose between RADIUS, FIXED, FIXEDRADIUS (don't use RADIUS)\n",
    "mode = Mode.FIXEDRADIUS\n",
    "# Flags\n",
    "ZDimension = False     # 2D or 3D\n",
    "seed = False           # Random seed\n",
    "# Choose settings\n",
    "chosen_settings = settings[\"testing\"]\n",
    "N       = chosen_settings[0]\n",
    "L       = chosen_settings[1]\n",
    "v       = chosen_settings[2]\n",
    "noise   = chosen_settings[3]\n",
    "r       = chosen_settings[4]\n",
    "\n",
    "k_neighbors = 5\n",
    "# Timesteps in an episode\n",
    "T = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom multi agent environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentSimulationEnv(MultiAgentEnv):\n",
    "    minimum = 0.0\n",
    "    maximum = 2 * np.pi\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_agents = N\n",
    "        self._spaces_in_preferred_format = True\n",
    "        self._agent_ids = list(range(self.num_agents))\n",
    "        \n",
    "        # We asume the same action space for all agents\n",
    "        self.action_space = Box(low=self.minimum, high=self.maximum, shape=(), dtype=np.float32)\n",
    "        \n",
    "        # We assume the same observation space for all agents\n",
    "        self.observation_space = Box(low=self.minimum, high=self.maximum, shape=(k_neighbors + 1,), dtype=np.float32)\n",
    "        \n",
    "        self.simulation = NeuralSwarmModel(N, L, v, noise, r, mode, k_neighbors, ZDimension, seed=seed)\n",
    "        self.new_angles = np.zeros(shape=(N,), dtype=np.float32)\n",
    "        self.index = 0\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # Reset the state of the environment to an initial state\n",
    "        observations = {}\n",
    "        infos = {}\n",
    "        self.simulation = NeuralSwarmModel(N, L, v, noise, r, mode, k_neighbors, ZDimension, seed=False)\n",
    "        self.index = 0\n",
    "        self.new_angles = np.zeros(shape=(N,), dtype=np.float32)\n",
    "        for agent_id in range(self.num_agents):\n",
    "            observations[agent_id] = self.simulation.get_angles(agent_id)\n",
    "        return observations, infos\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        # Actions for all agents are provided in a dictionary\n",
    "        \n",
    "        # Rewards for all agents are provided in a dictionary {agent_id: reward}\n",
    "        rewards = {}\n",
    "        # Observations for all agents are provided in a dictionary {agent_id: observation}\n",
    "        new_obs = {}\n",
    "        # Dones for all agents are provided in a dictionary {agent_id: done (boolean))}\n",
    "        dones = {}\n",
    "        # Truncated for all agents are provided in a dictionary {agent_id: truncated (boolean))}\n",
    "        # Truncated is used to indicate that the episode was ended early\n",
    "        truncated = {}\n",
    "        # Infos for all agents are provided in a dictionary {agent_id: info}\n",
    "        # Infos can be used to provide extra information about an agent's state or action\n",
    "        infos = {}\n",
    "        \n",
    "        # Collect all actions and set dones\n",
    "        for agent_id, action in action_dict.items():\n",
    "            action = np.clip(action, self.minimum, self.maximum)\n",
    "            self.new_angles[agent_id] = action\n",
    "            dones[agent_id] = True if self.index >= T else False\n",
    "            \n",
    "        # Update the simulation\n",
    "        self.simulation.update_angles(self.new_angles)\n",
    "        self.simulation.update()\n",
    "        self.index += 1\n",
    "        reward = self.simulation.mean_direction2D()\n",
    "        \n",
    "        # Collect observations and rewards\n",
    "        for agent_id in range(self.num_agents):\n",
    "            new_obs[agent_id] = self.simulation.get_angles(agent_id)\n",
    "            rewards[agent_id] = reward\n",
    "            \n",
    "        # Ends the episode if all agents are done\n",
    "        dones['__all__'] = all(dones.values())\n",
    "        # Truncated is used to indicate that the episode was ended early (opposite of dones)\n",
    "        truncated['__all__'] = not dones['__all__']\n",
    "        \n",
    "        return new_obs, rewards, dones, truncated, infos\n",
    "\n",
    "    def render(self, mode='rgb_array'):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_xlim(0, L)\n",
    "        ax.set_ylim(0, L)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(\"Simulation\")\n",
    "        ax.set_xlabel(\"x\")\n",
    "        ax.set_ylabel(\"y\")\n",
    "        ax.set_facecolor('black')\n",
    "        for agent_id in range(self.num_agents):\n",
    "            x, y, angle = self.simulation.particles[agent_id].x, self.simulation.particles[agent_id].y, self.simulation.particles[agent_id].angle\n",
    "            ax.plot(x, y, 'o', color='white', markersize=5)\n",
    "            ax.arrow(x, y, 0.5 * np.cos(angle), 0.5 * np.sin(angle), color='white', width=0.1)\n",
    "\n",
    "        if mode == 'human':\n",
    "            plt.show()\n",
    "        elif mode == 'rgb_array':\n",
    "            fig.canvas.draw()\n",
    "            image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "            image_from_plot = image_from_plot.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "            plt.close(fig)\n",
    "            return image_from_plot\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        # Optional: Clean up. Called at the end of an episode.\n",
    "        pass\n",
    "    \n",
    "    # Optional methods\n",
    "    def observation_space_contains(self, observation):\n",
    "        # Check if the observation is a valid observation\n",
    "        # Obervation is a dictionary {agent_id: observation}\n",
    "        observations = observation.values()\n",
    "        return all([self.observation_space.contains(obs) for obs in observations])\n",
    "    \n",
    "    def action_space_contains(self, action):\n",
    "        # Check if the action is a valid action\n",
    "        # Action is a dictionary {agent_id: action}\n",
    "        actions = action.values()\n",
    "        return all([self.action_space.contains(act) for act in actions])\n",
    "    \n",
    "    def observation_space_sample(self):\n",
    "        return {agent_id: self.observation_space.sample() for agent_id in range(self.num_agents)}\n",
    "\n",
    "    def action_space_sample(self, action):\n",
    "        return {agent_id: self.action_space.sample() for agent_id in range(self.num_agents)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tests on the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode 1\n",
      "Step 100 finished\n",
      "Episode 1 finished after 100 timesteps with rewards: 0.14410832714022614\n",
      "Rendering video...  Done\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = MultiAgentSimulationEnv(None)\n",
    "\n",
    "for i_episode in range(1):\n",
    "    observations, infos = env.reset()\n",
    "    total_rewards = {agent_id: 0 for agent_id in observations.keys()}\n",
    "    print(f\"Starting episode {i_episode + 1}\")\n",
    "    \n",
    "    frames = []  # List to store frames of the episode\n",
    "    # Max steps per episode\n",
    "    for t in range(T + 1):\n",
    "        # Render the environment\n",
    "        frames.append(env.render(mode='rgb_array'))\n",
    "        \n",
    "        # Choose random actions\n",
    "        actions = {agent_id: env.action_space.sample() for agent_id in observations.keys()}\n",
    "        \n",
    "        observations, rewards, dones, truncated, infos = env.step(actions)\n",
    "        \n",
    "        for agent_id, reward in rewards.items():\n",
    "            total_rewards[agent_id] += reward\n",
    "            \n",
    "        print(f\"Step {t}... \\r\", end=\"\")\n",
    "            \n",
    "        if any(dones.values()):\n",
    "            print(f\"Step {t} finished\")\n",
    "            # The reward is the same for all agents. We just take the first one.\n",
    "            print(f\"Episode {i_episode + 1} finished after {t} timesteps with rewards: {next(iter(rewards.values()))}\")\n",
    "            break\n",
    "\n",
    "print(\"Rendering video...  \", end=\"\")\n",
    "    \n",
    "fig = plt.figure()\n",
    "\n",
    "def update(num, frames):\n",
    "    plt.clf()\n",
    "    # Remove ticks, labels and axes\n",
    "    plt.axis('off')\n",
    "    plt.imshow(frames[num])\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update, frames=range(len(frames)), fargs=[frames])\n",
    "# Create directory if it doesn't exist\n",
    "path = \"test_videos\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "ani.save(f\"{path}/random_policy_env_test.avi\", writer='ffmpeg', fps=50)\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up policy management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative policy mapping function to map ALL agents to the shared policy\n",
    "def policy_mapping_fn_shared(agent_id, episode, **kwargs):\n",
    "    return \"shared_policy\"\n",
    "\n",
    "# Function to get the shared policy\n",
    "def get_shared_policy():\n",
    "    policies = {\n",
    "        \"shared_policy\": PolicySpec(\n",
    "            policy_class=None,\n",
    "            observation_space=env.observation_space, \n",
    "            action_space=env.action_space, \n",
    "            config={\n",
    "                \"model\": {\n",
    "                    \"custom_model\": \"SharedArchitectureModel\",\n",
    "                    # Additional custom model config parameters if needed\n",
    "                    # For example:\n",
    "                    # \"custom_model_config\": {\"shared_layers\": [64, 64]}\n",
    "                    # or\n",
    "                    # \"custom_model_config\": {\"critic_layers\": [64, 64]} \n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    }\n",
    "    return policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom actor network with tensorflow\n",
    "class CustomActorCriticModel(TFModelV2):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name, critic_layers=(64, 64)):\n",
    "        super(CustomActorCriticModel, self).__init__(obs_space, action_space, num_outputs, model_config, name)\n",
    "        \n",
    "        # Actor network\n",
    "        self.actor = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(1, activation=\"linear\")\n",
    "        ])\n",
    "\n",
    "        # Critic network\n",
    "        critic_input = tf.keras.Input(shape=obs_space.shape)\n",
    "        critic_model = critic_input\n",
    "        for units in critic_layers:\n",
    "            critic_model = tf.keras.layers.Dense(units, activation='relu')(critic_model)\n",
    "        self.critic_value = tf.keras.layers.Dense(1, activation=None)(critic_model)\n",
    "        self.critic = tf.keras.Model(inputs=critic_input, outputs=self.critic_value)\n",
    "\n",
    "        # Build the networks to set the weights\n",
    "        self.actor.build(input_shape=obs_space.shape)\n",
    "        self.critic.build(input_shape=obs_space.shape)\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        actor_output = self.actor(input_dict[\"obs_flat\"])\n",
    "        self._last_critic_value = self.critic(input_dict[\"obs_flat\"])\n",
    "        return actor_output, state\n",
    "\n",
    "    def value_function(self):\n",
    "        return tf.squeeze(self._last_critic_value, axis=-1)\n",
    "    \n",
    "class SharedArchitectureModel(TFModelV2):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name, shared_layers=None):\n",
    "        super(SharedArchitectureModel, self).__init__(obs_space, action_space, num_outputs, model_config, name)\n",
    "        \n",
    "        # Shared layers\n",
    "        if shared_layers:\n",
    "            self.shared = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(units, activation=None) for units in shared_layers\n",
    "            ])\n",
    "        else:\n",
    "            self.shared = None\n",
    "\n",
    "        # Actor head\n",
    "        self.actor_head = tf.keras.layers.Dense(num_outputs, activation=None)\n",
    "        \n",
    "        # Critic head\n",
    "        self.critic_head = tf.keras.layers.Dense(1, activation=None)\n",
    "\n",
    "        # Build the networks to set the weights\n",
    "        dummy_obs = tf.keras.Input(shape=obs_space.shape)\n",
    "        if self.shared:\n",
    "            shared_features = self.shared(dummy_obs)\n",
    "        else:\n",
    "            shared_features = dummy_obs\n",
    "        self.actor_head(shared_features)\n",
    "        self.critic_head(shared_features)\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        if self.shared:\n",
    "            shared_features = self.shared(input_dict[\"obs_flat\"])\n",
    "        else:\n",
    "            shared_features = input_dict[\"obs_flat\"]\n",
    "        actor_output = self.actor_head(shared_features)\n",
    "        self._last_critic_value = self.critic_head(shared_features)\n",
    "        return actor_output, state\n",
    "\n",
    "    def value_function(self):\n",
    "        return tf.squeeze(self._last_critic_value, axis=-1)\n",
    "\n",
    "\n",
    "ModelCatalog.register_custom_model(\"SharedArchitectureModel\", SharedArchitectureModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the algorithm and all the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = MADDPGConfig()\n",
    "config = PPOConfig()\n",
    "\n",
    "config.update_from_dict({\n",
    "    # This will evaluate the model every x training iterations\n",
    "    \"evaluation_interval\": 2,\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": True,\n",
    "        \"record_env\": \"./evaluation_videos/\",\n",
    "    },\n",
    "    \"render_env\": True,\n",
    "    \"record_env\": \"./evaluation_videos/\",\n",
    "\n",
    "    # Force the local worker to have an environment for evaluation\n",
    "    \"create_env_on_driver\": True,\n",
    "    \n",
    "})\n",
    "\n",
    "\n",
    "def simple_env_creator(config):\n",
    "    env = MultiAgentSimulationEnv(config)\n",
    "    \n",
    "    return env\n",
    "\n",
    "# Register the custom environment\n",
    "register_env(\"multi_agent_simulation\", simple_env_creator)\n",
    "config.environment(\"multi_agent_simulation\")\n",
    "\n",
    "# Disable automatic environment checking\n",
    "config.environment(disable_env_checking=True)\n",
    "\n",
    "# Test if the environment is valid\n",
    "check_env(simple_env_creator(config), config)\n",
    "\n",
    "policies = get_shared_policy()\n",
    "# policies = get_individual_policies()      # Uncomment this line to use individual policies\n",
    "# policies = {\"policy_%d\" % i: gen_policy(i) for i in range(N)}\n",
    "\n",
    "\n",
    "config.update_from_dict({\n",
    "    \"simple_optimizer\": True,\n",
    "    # \"policies\": policies,\n",
    "    # \"policy_mapping_fn\": policy_mapping_fn,\n",
    "    # \"policy_mapping_fn\": policy_mapping_fn_individual,\n",
    "    # \"policies_to_train\": list(policies.keys()),\n",
    "    # \"count_steps_by\": \"env_steps\",\n",
    "    \"framework\": \"tf\",\n",
    "    \"use_local_critic\": False,\n",
    "    \"use_state_preprocessor\": True,\n",
    "    # \"eager_tracing\": False,\n",
    "    # \"observation_space\": env.observation_space,\n",
    "    # \"action_space\": env.action_space,\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policies,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn_shared,\n",
    "    },\n",
    "    \"reuse_actors\": True,\n",
    "    \"num_gpus\": 1,\n",
    "    # \"num_cpus_per_worker\": 2,\n",
    "    \n",
    "    # Custom model\n",
    "    \"model\": {\n",
    "        \"custom_model\": \"SharedArchitectureModel\"\n",
    "    },\n",
    "    \"train_batch_size\": 1000,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "extra_python_environs_for_driver {}\n",
    "extra_python_environs_for_worker {}\n",
    "num_gpus 1\n",
    "num_cpus_per_worker 1\n",
    "num_gpus_per_worker 0\n",
    "_fake_gpus False\n",
    "num_learner_workers 0\n",
    "num_gpus_per_learner_worker 0\n",
    "num_cpus_per_learner_worker 1\n",
    "local_gpu_idx 0\n",
    "custom_resources_per_worker {}\n",
    "placement_strategy PACK\n",
    "eager_tracing True\n",
    "eager_max_retraces 20\n",
    "tf_session_args {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}\n",
    "local_tf_session_args {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}\n",
    "torch_compile_learner False\n",
    "torch_compile_learner_what_to_compile TorchCompileWhatToCompile.FORWARD_TRAIN\n",
    "torch_compile_learner_dynamo_backend inductor\n",
    "torch_compile_learner_dynamo_mode None\n",
    "torch_compile_worker False\n",
    "torch_compile_worker_dynamo_backend onnxrt\n",
    "torch_compile_worker_dynamo_mode None\n",
    "env multi_agent_simulation\n",
    "env_config {}\n",
    "observation_space None\n",
    "action_space None\n",
    "env_task_fn None\n",
    "render_env False\n",
    "clip_rewards None\n",
    "normalize_actions True\n",
    "clip_actions False\n",
    "disable_env_checking True\n",
    "_is_atari None\n",
    "auto_wrap_old_gym_envs True\n",
    "action_mask_key action_mask\n",
    "env_runner_cls None\n",
    "num_envs_per_worker 1\n",
    "sample_collector <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>\n",
    "sample_async False\n",
    "enable_connectors True\n",
    "update_worker_filter_stats True\n",
    "use_worker_filter_stats True\n",
    "rollout_fragment_length auto\n",
    "batch_mode truncate_episodes\n",
    "remote_worker_envs False\n",
    "remote_env_batch_wait_ms 0\n",
    "validate_workers_after_construction True\n",
    "preprocessor_pref deepmind\n",
    "observation_filter NoFilter\n",
    "compress_observations False\n",
    "enable_tf1_exec_eagerly False\n",
    "sampler_perf_stats_ema_coef None\n",
    "gamma 0.99\n",
    "lr 5e-05\n",
    "grad_clip None\n",
    "grad_clip_by global_norm\n",
    "train_batch_size 4000\n",
    "model {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}\n",
    "optimizer {}\n",
    "max_requests_in_flight_per_sampler_worker 2\n",
    "_learner_class None\n",
    "_enable_learner_api True\n",
    "explore True\n",
    "exploration_config {}\n",
    "algorithm_config_overrides_per_module {}\n",
    "policy_map_capacity 100\n",
    "policy_mapping_fn <function policy_mapping_fn_shared at 0x7f40210bb760>\n",
    "policies_to_train None\n",
    "policy_states_are_swappable False\n",
    "observation_fn None\n",
    "count_steps_by env_steps\n",
    "input_config {}\n",
    "actions_in_input_normalized False\n",
    "postprocess_inputs False\n",
    "shuffle_buffer_size 0\n",
    "output None\n",
    "output_config {}\n",
    "output_compress_columns ['obs', 'new_obs']\n",
    "output_max_file_size 67108864\n",
    "offline_sampling False\n",
    "evaluation_interval 2\n",
    "evaluation_duration 10\n",
    "evaluation_duration_unit episodes\n",
    "evaluation_sample_timeout_s 180.0\n",
    "evaluation_parallel_to_training False\n",
    "evaluation_config None\n",
    "off_policy_estimation_methods {}\n",
    "ope_split_batch_by_episode True\n",
    "evaluation_num_workers 0\n",
    "always_attach_evaluation_results False\n",
    "enable_async_evaluation False\n",
    "in_evaluation False\n",
    "sync_filters_on_rollout_workers_timeout_s 60.0\n",
    "keep_per_episode_custom_metrics False\n",
    "metrics_episode_collection_timeout_s 60.0\n",
    "metrics_num_episodes_for_smoothing 100\n",
    "min_time_s_per_iteration None\n",
    "min_train_timesteps_per_iteration 0\n",
    "min_sample_timesteps_per_iteration 0\n",
    "export_native_model_files False\n",
    "checkpoint_trainable_policies_only False\n",
    "logger_creator None\n",
    "logger_config None\n",
    "log_level WARN\n",
    "log_sys_usage True\n",
    "fake_sampler False\n",
    "seed None\n",
    "ignore_worker_failures False\n",
    "recreate_failed_workers False\n",
    "max_num_worker_restarts 1000\n",
    "delay_between_worker_restarts_s 60.0\n",
    "restart_failed_sub_environments False\n",
    "num_consecutive_worker_failures_tolerance 100\n",
    "worker_health_probe_timeout_s 60\n",
    "worker_restore_timeout_s 1800\n",
    "rl_module_spec None\n",
    "_enable_rl_module_api True\n",
    "_AlgorithmConfig__prior_exploration_config {'type': 'StochasticSampling'}\n",
    "_tf_policy_handles_more_than_one_loss False\n",
    "_disable_preprocessor_api False\n",
    "_disable_action_flattening False\n",
    "_disable_execution_plan_api True\n",
    "_disable_initialize_loss_from_dummy_batch False\n",
    "simple_optimizer True\n",
    "policy_map_cache -1\n",
    "worker_cls -1\n",
    "synchronize_filters -1\n",
    "replay_sequence_length None\n",
    "lr_schedule None\n",
    "use_critic True\n",
    "use_gae True\n",
    "use_kl_loss True\n",
    "kl_coeff 0.2\n",
    "kl_target 0.01\n",
    "sgd_minibatch_size 128\n",
    "num_sgd_iter 30\n",
    "shuffle_sequences True\n",
    "vf_loss_coeff 1.0\n",
    "entropy_coeff 0.0\n",
    "entropy_coeff_schedule None\n",
    "clip_param 0.3\n",
    "vf_clip_param 10.0\n",
    "vf_share_layers -1\n",
    "use_local_critic False\n",
    "use_state_preprocessor True\n",
    "reuse_actors True\n",
    "lambda 1.0\n",
    "input sampler\n",
    "policies {'shared_policy': (None, Box(0.0, 6.2831855, (6,), float32), Box(0.0, 6.2831855, (), float32), {})}\n",
    "callbacks <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>\n",
    "create_env_on_driver True\n",
    "custom_eval_function None\n",
    "framework tf\n",
    "num_cpus_for_driver 1\n",
    "num_workers 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the configuration (contains all the hyperparameters)\n",
    "for key, value in config.items():\n",
    "    # print(key, value)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = config.build() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iterations = 6\n",
    "video_dir = \"./evaluation_videos\"\n",
    "ckeckpoint_dir = \"./checkpoints\"\n",
    "\n",
    "if not os.path.exists(video_dir):\n",
    "    os.makedirs(video_dir)\n",
    "\n",
    "for i in range(training_iterations):\n",
    "    \n",
    "    iteration = algo.iteration\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo.save()\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")\n",
    "        \n",
    "    # Get the number of iterations from the algo object\n",
    "\n",
    "    # Save frames as a video\n",
    "    if iteration % 1 == 0:\n",
    "        # Manual rendering after each training iteration\n",
    "        print(f\"Rendering episode {iteration}... \", end=\"\")\n",
    "        tempEnv = MultiAgentSimulationEnv(config)\n",
    "        dones = {\"__all__\": False}\n",
    "        observations, infos = tempEnv.reset()\n",
    "        worker = algo.workers.local_worker()\n",
    "        policy = worker.get_policy(\"shared_policy\")\n",
    "\n",
    "        frames = []  # List to store frames of the episode\n",
    "            \n",
    "        while not dones[\"__all__\"]:\n",
    "            # convert the observation dictionary to a list of observation arrays\n",
    "            observation_list = [np.reshape(observation, (1, -1)) for observation in observations.values()]\n",
    "\n",
    "            # concatenate along the first dimension to make a batch\n",
    "            observation_batch = np.concatenate(observation_list, axis=0)\n",
    "            \n",
    "            # Assuming your policy is called \"shared_policy\"\n",
    "            actions, _, _ = policy.compute_actions(observation_batch)\n",
    "            \n",
    "            # map back the batch actions to individual agent IDs\n",
    "            action_dict = {agent_id: action for agent_id, action in zip(observations.keys(), actions)}\n",
    "\n",
    "            observations, rewards, dones, truncated, infos = tempEnv.step(action_dict)\n",
    "            frame = tempEnv.render(mode='rgb_array')\n",
    "            frames.append(frame)\n",
    "        \n",
    "        # Get id of the run from the ckeckpoint dir\n",
    "        checkpoint_id = checkpoint_dir.split(\"/\")[-2].split(\"-\")[-1]\n",
    "        \n",
    "        # Create folder for the video\n",
    "        if not os.path.exists(os.path.join(video_dir, checkpoint_id)):\n",
    "            os.makedirs(os.path.join(video_dir, checkpoint_id))\n",
    "        video_filename = os.path.join(video_dir, f\"{checkpoint_id}/episode_{iteration}.avi\")\n",
    "        \n",
    "        fig = plt.figure()\n",
    "\n",
    "        def update(num, frames):\n",
    "            plt.clf()\n",
    "            # Remove ticks, labels and axes\n",
    "            plt.axis('off')\n",
    "            plt.imshow(frames[num])\n",
    "\n",
    "        ani = animation.FuncAnimation(fig, update, frames=range(len(frames)), fargs=[frames])\n",
    "\n",
    "        ani.save(video_filename, writer='ffmpeg', fps=50)\n",
    "        \n",
    "        print(\"Done.\")\n",
    "    \n",
    "    # Print progress (based on iterations + training_iterations)\n",
    "    print(f\"Training iteration: {iteration}/{iteration + training_iterations}\")\n",
    "    \n",
    "    result = algo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the weights of the policy\n",
    "print(algo.get_weights(\"shared_policy\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
